Jaccard index			
				From Wikipedia, the free encyclopedia				
								
					Jump to:					navigation, 					search
				
				


This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (March 2011) (Learn how and when to remove this template message)









Intersection and union of two sets A and B













Intersection over Union as a similarity measure for object detection on images - an important task in computer vision.


The Jaccard index, also known as Intersection over Union and the Jaccard similarity coefficient (originally coined coefficient de communauté by Paul Jaccard), is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:

  
    
      
        J
        (
        A
        ,
        B
        )
        =
        
          
            
              
                |
              
              A
              ∩
              B
              
                |
              
            
            
              
                |
              
              A
              ∪
              B
              
                |
              
            
          
        
        =
        
          
            
              
                |
              
              A
              ∩
              B
              
                |
              
            
            
              
                |
              
              A
              
                |
              
              +
              
                |
              
              B
              
                |
              
              −
              
                |
              
              A
              ∩
              B
              
                |
              
            
          
        
        .
      
    
    {\displaystyle J(A,B)={{|A\cap B|} \over {|A\cup B|}}={{|A\cap B|} \over {|A|+|B|-|A\cap B|}}.}
  

(If A and B are both empty, we define J(A,B) = 1.)

  
    
      
        0
        ≤
        J
        (
        A
        ,
        B
        )
        ≤
        1.
      
    
    {\displaystyle 0\leq J(A,B)\leq 1.}
  

The Jaccard distance, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:

  
    
      
        
          d
          
            J
          
        
        (
        A
        ,
        B
        )
        =
        1
        −
        J
        (
        A
        ,
        B
        )
        =
        
          
            
              
                |
              
              A
              ∪
              B
              
                |
              
              −
              
                |
              
              A
              ∩
              B
              
                |
              
            
            
              
                |
              
              A
              ∪
              B
              
                |
              
            
          
        
        .
      
    
    {\displaystyle d_{J}(A,B)=1-J(A,B)={{|A\cup B|-|A\cap B|} \over |A\cup B|}.}
  

An alternate interpretation of the Jaccard distance is as the ratio of the size of the symmetric difference 
  
    
      
        A
        △
        B
        =
        (
        A
        ∪
        B
        )
        −
        (
        A
        ∩
        B
        )
      
    
    {\displaystyle A\triangle B=(A\cup B)-(A\cap B)}
  
 to the union.
This distance is a metric on the collection of all finite sets.[1][2][3]
There is also a version of the Jaccard distance for measures, including probability measures. If 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 is a measure on a measurable space 
  
    
      
        X
      
    
    {\displaystyle X}
  
, then we define the Jaccard coefficient by 
  
    
      
        
          J
          
            μ
          
        
        (
        A
        ,
        B
        )
        =
        
          
            
              μ
              (
              A
              ∩
              B
              )
            
            
              μ
              (
              A
              ∪
              B
              )
            
          
        
      
    
    {\displaystyle J_{\mu }(A,B)={{\mu (A\cap B)} \over {\mu (A\cup B)}}}
  
, and the Jaccard distance by 
  
    
      
        
          d
          
            μ
          
        
        (
        A
        ,
        B
        )
        =
        1
        −
        
          J
          
            μ
          
        
        (
        A
        ,
        B
        )
        =
        
          
            
              μ
              (
              A
              △
              B
              )
            
            
              μ
              (
              A
              ∪
              B
              )
            
          
        
      
    
    {\displaystyle d_{\mu }(A,B)=1-J_{\mu }(A,B)={{\mu (A\triangle B)} \over {\mu (A\cup B)}}}
  
. Care must be taken if 
  
    
      
        μ
        (
        A
        ∪
        B
        )
        =
        0
      
    
    {\displaystyle \mu (A\cup B)=0}
  
 or 
  
    
      
        ∞
      
    
    {\displaystyle \infty }
  
, since these formulas are not well defined in that case.
The MinHash min-wise independent permutations locality sensitive hashing scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a hash function.



Contents

1 Similarity of asymmetric binary attributes
1.1 Difference with the simple matching coefficient (SMC)
2 Generalized Jaccard similarity and distance3 Tanimoto similarity and distance
3.1 Tanimoto's definitions of similarity and distance3.2 Other definitions of Tanimoto distance
4 See also5 Notes6 References7 External links


Similarity of asymmetric binary attributes[edit]
Given two objects, A and B, each with n binary attributes, the Jaccard coefficient is a useful measure of the overlap that A and B share with their attributes. Each attribute of A and B can either be 0 or 1. The total number of each combination of attributes for both A and B are specified as follows:

  
    
      
        
          M
          
            11
          
        
      
    
    {\displaystyle M_{11}}
  
 represents the total number of attributes where A and B both have a value of 1.
  
    
      
        
          M
          
            01
          
        
      
    
    {\displaystyle M_{01}}
  
 represents the total number of attributes where the attribute of A is 0 and the attribute of B is 1.
  
    
      
        
          M
          
            10
          
        
      
    
    {\displaystyle M_{10}}
  
 represents the total number of attributes where the attribute of A is 1 and the attribute of B is 0.
  
    
      
        
          M
          
            00
          
        
      
    
    {\displaystyle M_{00}}
  
 represents the total number of attributes where A and B both have a value of 0.
A01B0
  
    
      
        
          M
          
            00
          
        
      
    
    {\displaystyle M_{00}}
  

  
    
      
        
          M
          
            10
          
        
      
    
    {\displaystyle M_{10}}
  
1
  
    
      
        
          M
          
            01
          
        
      
    
    {\displaystyle M_{01}}
  

  
    
      
        
          M
          
            11
          
        
      
    
    {\displaystyle M_{11}}
  

Each attribute must fall into one of these four categories, meaning that

  
    
      
        
          M
          
            11
          
        
        +
        
          M
          
            01
          
        
        +
        
          M
          
            10
          
        
        +
        
          M
          
            00
          
        
        =
        n
        .
      
    
    {\displaystyle M_{11}+M_{01}+M_{10}+M_{00}=n.}
  

The Jaccard similarity coefficient, J, is given as

  
    
      
        J
        =
        
          
            
              M
              
                11
              
            
            
              
                M
                
                  01
                
              
              +
              
                M
                
                  10
                
              
              +
              
                M
                
                  11
                
              
            
          
        
        .
      
    
    {\displaystyle J={M_{11} \over M_{01}+M_{10}+M_{11}}.}
  

The Jaccard distance, dJ, is given as

  
    
      
        
          d
          
            J
          
        
        =
        
          
            
              
                M
                
                  01
                
              
              +
              
                M
                
                  10
                
              
            
            
              
                M
                
                  01
                
              
              +
              
                M
                
                  10
                
              
              +
              
                M
                
                  11
                
              
            
          
        
        =
        1
        −
        J
        .
      
    
    {\displaystyle d_{J}={M_{01}+M_{10} \over M_{01}+M_{10}+M_{11}}=1-J.}
  

Difference with the simple matching coefficient (SMC)[edit]
When used for binary attributes, the Jaccard index is very similar to the simple matching coefficient. The main difference is that the SMC has the term 
  
    
      
        
          M
          
            00
          
        
      
    
    {\displaystyle M_{00}}
  
 in its numerator and denominator, whereas the Jaccard index does not. Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets.
In market basket analysis, for example, the basket of two consumers who we wish to compare might only contain a small fraction of all the available products in the store, so the SMC will usually return very high values of similarities even when the baskets bear very little resemblance, thus making the Jaccard index a more appropriate measure of similarity in that context. For example, consider a supermarket with 1000 products and two customers. The basket of the first customer contains salt and pepper and the basket of the second contains salt and sugar. In this scenario, the similarity between the two baskets as measured by the Jaccard index would be 1/3, but the similarity becomes 0.998 using the SMC.
In other contexts, where 0 and 1 carry equivalent information (symmetry), the SMC is a better measure of similarity. For example, vectors of demographic variables stored in dummy variables, such as gender, would be better compared with the SMC than with the Jaccard index since the impact of gender on similarity should be equal, independently of whether male is defined as a 0 and female as a 1 or the other way around. However, when we have symmetric dummy variables, one could replicate the behaviour of the SMC by splitting the dummies into two binary attributes (in this case, male and female), thus transforming them into asymmetric attributes, allowing the use of the Jaccard index without introducing any bias. By using this trick, the Jaccard index can be considered as making the SMC a fully redundant metric. The SMC remains, however, more computationally efficient in the case of symmetric dummy variables since it does not require adding extra dimensions.
Generalized Jaccard similarity and distance[edit]
If 
  
    
      
        
          x
        
        =
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {x} =(x_{1},x_{2},\ldots ,x_{n})}
  
 and 
  
    
      
        
          y
        
        =
        (
        
          y
          
            1
          
        
        ,
        
          y
          
            2
          
        
        ,
        …
        ,
        
          y
          
            n
          
        
        )
      
    
    {\displaystyle \mathbf {y} =(y_{1},y_{2},\ldots ,y_{n})}
  
 are two vectors with all real 
  
    
      
        
          x
          
            i
          
        
        ,
        
          y
          
            i
          
        
        ≥
        0
      
    
    {\displaystyle x_{i},y_{i}\geq 0}
  
, then their Jaccard similarity coefficient is defined as

  
    
      
        J
        (
        
          x
        
        ,
        
          y
        
        )
        =
        
          
            
              
                ∑
                
                  i
                
              
              min
              (
              
                x
                
                  i
                
              
              ,
              
                y
                
                  i
                
              
              )
            
            
              
                ∑
                
                  i
                
              
              max
              (
              
                x
                
                  i
                
              
              ,
              
                y
                
                  i
                
              
              )
            
          
        
        ,
      
    
    {\displaystyle J(\mathbf {x} ,\mathbf {y} )={\frac {\sum _{i}\min(x_{i},y_{i})}{\sum _{i}\max(x_{i},y_{i})}},}
  

and Jaccard distance

  
    
      
        
          d
          
            J
          
        
        (
        
          x
        
        ,
        
          y
        
        )
        =
        1
        −
        J
        (
        
          x
        
        ,
        
          y
        
        )
        .
      
    
    {\displaystyle d_{J}(\mathbf {x} ,\mathbf {y} )=1-J(\mathbf {x} ,\mathbf {y} ).}
  

With even more generality, if 
  
    
      
        f
      
    
    {\displaystyle f}
  
 and 
  
    
      
        g
      
    
    {\displaystyle g}
  
 are two non-negative measurable functions on a measurable space 
  
    
      
        X
      
    
    {\displaystyle X}
  
 with measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
, then we can define

  
    
      
        J
        (
        f
        ,
        g
        )
        =
        
          
            
              ∫
              min
              (
              f
              ,
              g
              )
              d
              μ
            
            
              ∫
              max
              (
              f
              ,
              g
              )
              d
              μ
            
          
        
        ,
      
    
    {\displaystyle J(f,g)={\frac {\int \min(f,g)d\mu }{\int \max(f,g)d\mu }},}
  

where 
  
    
      
        max
      
    
    {\displaystyle \max }
  
 and 
  
    
      
        min
      
    
    {\displaystyle \min }
  
 are pointwise operators. Then Jaccard distance is

  
    
      
        
          d
          
            J
          
        
        (
        f
        ,
        g
        )
        =
        1
        −
        J
        (
        f
        ,
        g
        )
        .
      
    
    {\displaystyle d_{J}(f,g)=1-J(f,g).}
  

Then, for example, for two measurable sets 
  
    
      
        A
        ,
        B
        ⊆
        X
      
    
    {\displaystyle A,B\subseteq X}
  
, we have 
  
    
      
        
          J
          
            μ
          
        
        (
        A
        ,
        B
        )
        =
        J
        (
        
          χ
          
            A
          
        
        ,
        
          χ
          
            B
          
        
        )
        ,
      
    
    {\displaystyle J_{\mu }(A,B)=J(\chi _{A},\chi _{B}),}
  
 where 
  
    
      
        
          χ
          
            A
          
        
      
    
    {\displaystyle \chi _{A}}
  
 and 
  
    
      
        
          χ
          
            B
          
        
      
    
    {\displaystyle \chi _{B}}
  
 are the characteristic functions of the corresponding set.
Tanimoto similarity and distance[edit]
Various forms of functions described as Tanimoto similarity and Tanimoto distance occur in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources[4] cite an IBM Technical Report[5] as the seminal reference. The report is available from several libraries.
In "A Computer Program for Classifying Plants", published in October 1960,[6] a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is the most authoritative source for the meaning of the terms "Tanimoto similarity" and "Tanimoto Distance". The similarity ratio is equivalent to Jaccard similarity, but the distance function is not the same as Jaccard distance.
Tanimoto's definitions of similarity and distance[edit]
In that paper, a "similarity ratio" is given over bitmaps, where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (i.e. nonzero) in either sample.
Presented in mathematical terms, if samples X and Y are bitmaps, 
  
    
      
        
          X
          
            i
          
        
      
    
    {\displaystyle X_{i}}
  
 is the ith bit of X, and 
  
    
      
        ∧
        ,
        ∨
      
    
    {\displaystyle \land ,\lor }
  
 are bitwise and, or operators respectively, then the similarity ratio 
  
    
      
        
          T
          
            s
          
        
      
    
    {\displaystyle T_{s}}
  
 is

  
    
      
        
          T
          
            s
          
        
        (
        X
        ,
        Y
        )
        =
        
          
            
              
                ∑
                
                  i
                
              
              (
              
                X
                
                  i
                
              
              ∧
              
                Y
                
                  i
                
              
              )
            
            
              
                ∑
                
                  i
                
              
              (
              
                X
                
                  i
                
              
              ∨
              
                Y
                
                  i
                
              
              )
            
          
        
      
    
    {\displaystyle T_{s}(X,Y)={\frac {\sum _{i}(X_{i}\land Y_{i})}{\sum _{i}(X_{i}\lor Y_{i})}}}
  

If each sample is modelled instead as a set of attributes, this value is equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.
Tanimoto goes on to define a "distance coefficient" based on this ratio, defined for bitmaps with non-zero similarity:

  
    
      
        
          T
          
            d
          
        
        (
        X
        ,
        Y
        )
        =
        −
        
          log
          
            2
          
        
        ⁡
        (
        
          T
          
            s
          
        
        (
        X
        ,
        Y
        )
        )
      
    
    {\displaystyle T_{d}(X,Y)=-\log _{2}(T_{s}(X,Y))}
  

This coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is easy to construct an example which disproves the property of triangle inequality.
Other definitions of Tanimoto distance[edit]
Tanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance 
  
    
      
        1
        −
        
          T
          
            s
          
        
      
    
    {\displaystyle 1-T_{s}}
  
. This function is a proper distance metric. "Tanimoto Distance" is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.
If Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as

  
    
      
        f
        (
        A
        ,
        B
        )
        =
        
          
            
              A
              ⋅
              B
            
            
              |
              A
              
                |
                
                  2
                
              
              +
              |
              B
              
                |
                
                  2
                
              
              −
              A
              ⋅
              B
            
          
        
      
    
    {\displaystyle f(A,B)={\frac {A\cdot B}{\vert A\vert ^{2}+\vert B\vert ^{2}-A\cdot B}}}
  

where the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then 
  
    
      
        A
        ⋅
        B
        =
        
          ∑
          
            i
          
        
        
          A
          
            i
          
        
        
          B
          
            i
          
        
        =
        
          ∑
          
            i
          
        
        (
        
          A
          
            i
          
        
        ∧
        
          B
          
            i
          
        
        )
      
    
    {\displaystyle A\cdot B=\sum _{i}A_{i}B_{i}=\sum _{i}(A_{i}\land B_{i})}
  
 and 
  
    
      
        
          
            |
            A
            |
          
          
            2
          
        
        =
        
          ∑
          
            i
          
        
        
          A
          
            i
          
          
            2
          
        
        =
        
          ∑
          
            i
          
        
        
          A
          
            i
          
        
      
    
    {\displaystyle {\vert A\vert }^{2}=\sum _{i}A_{i}^{2}=\sum _{i}A_{i}}
  
.
This is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of 
  
    
      
        
          T
          
            s
          
        
      
    
    {\displaystyle T_{s}}
  
 do not necessarily extend to 
  
    
      
        f
      
    
    {\displaystyle f}
  
. In particular, the difference function 
  
    
      
        1
        −
        f
      
    
    {\displaystyle 1-f}
  
 does not preserve triangle inequality, and is not therefore a proper distance metric, whereas 
  
    
      
        1
        −
        
          T
          
            s
          
        
      
    
    {\displaystyle 1-T_{s}}
  
 is.
There is a real danger that the combination of "Tanimoto Distance" being defined using this formula, along with the statement "Tanimoto Distance is a proper distance metric" will lead to the false conclusion that the function 
  
    
      
        1
        −
        f
      
    
    {\displaystyle 1-f}
  
 is in fact a distance metric over vectors or multisets in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.
Lipkus[2] uses a definition of Tanimoto similarity which is equivalent to 
  
    
      
        f
      
    
    {\displaystyle f}
  
, and refers to Tanimoto distance as the function 
  
    
      
        1
        −
        f
      
    
    {\displaystyle 1-f}
  
. It is, however, made clear within the paper that the context is restricted by the use of a (positive) weighting vector 
  
    
      
        W
      
    
    {\displaystyle W}
  
 such that, for any vector A being considered, 
  
    
      
        
          A
          
            i
          
        
        ∈
        {
        0
        ,
        
          W
          
            i
          
        
        }
      
    
    {\displaystyle A_{i}\in \{0,W_{i}\}}
  
. Under these circumstances, the function is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a metric space under this function.
See also[edit]
Simple matching coefficientMost frequent k charactersHamming distanceSørensen–Dice coefficient, which is equivalent: 
  
    
      
        J
        =
        S
        
          /
        
        (
        2
        −
        S
        )
      
    
    {\displaystyle J=S/(2-S)}
  
 and 
  
    
      
        S
        =
        2
        J
        
          /
        
        (
        1
        +
        J
        )
      
    
    {\displaystyle S=2J/(1+J)}
  
 (
  
    
      
        J
      
    
    {\displaystyle J}
  
: Jaccard index, 
  
    
      
        S
      
    
    {\displaystyle S}
  
: Sørensen–Dice coefficient)Tversky indexCorrelationMutual information, a normalized metricated variant of which is an entropic Jaccard distance.
Notes[edit]


^ Sven Kosub, "A note on the triangle inequality for the Jaccard distance" arXiv:1612.02696^ a b Lipkus, Alan H (1999), "A proof of the triangle inequality for the Tanimoto distance", J Math Chem, 26 (1-3): 263–265 ^ Levandowsky, Michael; Winter, David (1971), "Distance between sets", Nature, 234 (5): 34–35, doi:10.1038/234034a0 ^ For example Qian, Huihuan; Wu, Xinyu; Xu, Yangsheng (2011). Intelligent Surveillance Systems. Springer. p. 161. ISBN 978-94-007-1137-2. ^ Tanimoto, T. (17 Nov 1958). "An Elementary Mathematical theory of Classification and Prediction". Internal IBM Technical Report. 1957 (8?). ^ Rogers, David J.; Tanimoto, Taffee T. (1960). "A Computer Program for Classifying Plants". Science. 132 (3434): 1115–1118. doi:10.1126/science.132.3434.1115. 


References[edit]
Tan, Pang-Ning; Steinbach, Michael; Kumar, Vipin (2005), Introduction to Data Mining, ISBN 0-321-32136-7 .Jaccard, Paul (1901), "Étude comparative de la distribution florale dans une portion des Alpes et des Jura", Bulletin de la Société Vaudoise des Sciences Naturelles, 37: 547–579 .Jaccard, Paul (1912), "The distribution of the flora in the alpine zone", New Phytologist, 11: 37–50, doi:10.1111/j.1469-8137.1912.tb05611.x .
External links[edit]
Introduction to Data Mining lecture notes from Tan, Steinbach, KumarSimMetrics a sourceforge implementation of Jaccard index and many other similarity metricsA web-based calculator for finding the Jaccard CoefficientTutorial on how to calculate different similaritiesIntersection over Union (IoU) for object detectionKaggle Dstl Satellite Imagery Feature Detection - Evaluation






					
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Jaccard_index&oldid=825866635"					
				Categories: Index numbersMeasure theoryClustering criteriaString similarity measuresSimilarity and distance measuresHidden categories: Articles lacking in-text citations from March 2011All articles lacking in-text citations				
							
		
		
			Navigation menu
			
									
						Personal tools
						Not logged inTalkContributionsCreate accountLog in
					
									
										
						Namespaces
						ArticleTalk
					
										
												
						
							Variants
						
						
							
						
					
									
				
										
						Views
						ReadEditView history
					
										
						
						More
						
							
						
					
										
						
							Search
						
						
							
															
						
					
									
			
			
				
						
			Navigation
			
								Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store
							
		
			
			Interaction
			
								HelpAbout WikipediaCommunity portalRecent changesContact page
							
		
			
			Tools
			
								What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page
							
		
			
			Print/export
			
								Create a bookDownload as PDFPrintable version
							
		
			
			Languages
			
								AzərbaycancaCatalàDeutschEspañolفارسیFrançais한국어ItalianoPolskiРусскийУкраїнська
				Edit links			
		
				
		
				
						 This page was last edited on 15 February 2018, at 21:53.Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.
						Privacy policyAbout WikipediaDisclaimersContact WikipediaDevelopersCookie statementMobile view