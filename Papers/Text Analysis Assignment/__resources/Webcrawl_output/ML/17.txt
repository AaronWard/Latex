<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" version="2.0">

<channel>
	<title>Machine Learning Mastery</title>
	
	<link>https://machinelearningmastery.com</link>
	<description>Making developers awesome at machine learning</description>
	<lastBuildDate>Tue, 13 Mar 2018 18:00:02 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.4</generator>

<image>
	<url>https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-32x32.png</url>
	<title>Machine Learning Mastery</title>
	<link>https://machinelearningmastery.com</link>
	<width>32</width>
	<height>32</height>
</image> 
	<atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/MachineLearningMastery" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="machinelearningmastery" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><item>
		<title>A Gentle Introduction to Sparse Matrices for Machine Learning</title>
		<link>https://machinelearningmastery.com/sparse-matrices-for-machine-learning/</link>
		<comments>https://machinelearningmastery.com/sparse-matrices-for-machine-learning/#respond</comments>
		<pubDate>Tue, 13 Mar 2018 18:00:02 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4955</guid>
		<description><![CDATA[<p>Matrices that contain mostly zero values are called sparse, distinct from matrices where most of the values are non-zero, called dense. Large sparse matrices are common in general and especially in applied machine learning, such as in data that contains counts, data encodings that map categories to counts, and even in whole subfields of machine [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/sparse-matrices-for-machine-learning/">A Gentle Introduction to Sparse Matrices for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Matrices that contain mostly zero values are called sparse, distinct from matrices where most of the values are non-zero, called dense.</p>
<p>Large sparse matrices are common in general and especially in applied machine learning, such as in data that contains counts, data encodings that map categories to counts, and even in whole subfields of machine learning such as natural language processing.</p>
<p>It is computationally expensive to represent and work with sparse matrices as though they are dense, and much improvement in performance can be achieved by using representations and operations that specifically handle the matrix sparsity.</p>
<p>In this tutorial, you will discover sparse matrices, the issues they present, and how to work with them directly in Python.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>That sparse matrices contain mostly zero values and are distinct from dense matrices.</li>
<li>The myriad of areas where you are likely to encounter sparse matrices in data, data preparation, and sub-fields of machine learning.</li>
<li>That there are many efficient ways to store and work with sparse matrices and SciPy provides implementations that you can use directly.</li>
</ul>
<p>Let’s get started.</p>
<div id="attachment_4963" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4963" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/A-Gentle-Introduction-to-Sparse-Matrices-for-Machine-Learning.jpg" alt="A Gentle Introduction to Sparse Matrices for Machine Learning" width="640" height="480" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/A-Gentle-Introduction-to-Sparse-Matrices-for-Machine-Learning.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/A-Gentle-Introduction-to-Sparse-Matrices-for-Machine-Learning-300x225.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">A Gentle Introduction to Sparse Matrices for Machine Learning<br />Photo by <a href="https://www.flickr.com/photos/carolannie/26617750821/">CAJC: in the Rockies</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 5 parts; they are:</p>
<ul>
<li>Sparse Matrix</li>
<li>Problems with Sparsity</li>
<li>Sparse Matrices in Machine Learning</li>
<li>Working with Sparse Matrices</li>
<li>Sparse Matrices in Python</li>
</ul>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Sparse Matrix</h2>
<p>A sparse matrix is a matrix that is comprised of mostly zero values.</p>
<p>Sparse matrices are distinct from matrices with mostly non-zero values, which are referred to as dense matrices.</p>
<blockquote><p>A matrix is sparse if many of its coefficients are zero. The interest in sparsity arises because its exploitation can lead to enormous computational savings and because many large matrix problems that occur in practice are sparse.</p></blockquote>
<p>&#8212; Page 1, <a href="http://amzn.to/2DcsQVU">Direct Methods for Sparse Matrices</a>, Second Edition, 2017.</p>
<p>The sparsity of a matrix can be quantified with a score, which is the number of zero values in the matrix divided by the total number of elements in the matrix.</p><pre class="crayon-plain-tag">sparsity = count zero elements / total elements</pre><p>Below is an example of a small 3 x 6 sparse matrix.</p><pre class="crayon-plain-tag">1, 0, 0, 1, 0, 0
A = (0, 0, 2, 0, 0, 1)
     0, 0, 0, 2, 0, 0</pre><p>The example has 13 zero values of the 18 elements in the matrix, giving this matrix a sparsity score of 0.722 or about 72%.</p>
<h2>Problems with Sparsity</h2>
<p>Sparse matrices can cause problems with regards to space and time complexity.</p>
<h3>Space Complexity</h3>
<p>Very large matrices require a lot of memory, and some very large matrices that we wish to work with are sparse.</p>
<blockquote><p>In practice, most large matrices are sparse &#8212; almost all entries are zeros.</p></blockquote>
<p>&#8212; Page 465, <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</p>
<p>An example of a very large matrix that is too large to be stored in memory is a link matrix that shows the links from one website to another.</p>
<p>An example of a smaller sparse matrix might be a word or term occurrence matrix for words in one book against all known words in English.</p>
<p>In both cases, the matrix contained is sparse with many more zero values than data values. The problem with representing these sparse matrices as dense matrices is that memory is required and must be allocated for each 32-bit or even 64-bit zero value in the matrix.</p>
<p>This is clearly a waste of memory resources as those zero values do not contain any information.</p>
<h3>Time Complexity</h3>
<p>Assuming a very large sparse matrix can be fit into memory, we will want to perform operations on this matrix.</p>
<p>Simply, if the matrix contains mostly zero-values, i.e. no data, then performing operations across this matrix may take a long time where the bulk of the computation performed will involve adding or multiplying zero values together.</p>
<blockquote><p>It is wasteful to use general methods of linear algebra on such problems, because most of the O(N^3) arithmetic operations devoted to solving the set of equations or inverting the matrix involve zero operands.</p></blockquote>
<p>&#8212; Page 75, <a href="http://amzn.to/2CF5atj">Numerical Recipes: The Art of Scientific Computing</a>, Third Edition, 2007.</p>
<p>This is a problem of increased time complexity of matrix operations that increases with the size of the matrix.</p>
<p>This problem is compounded when we consider that even trivial machine learning methods may require many operations on each row, column, or even across the entire matrix, resulting in vastly longer execution times.</p>
<h2>Sparse Matrices in Machine Learning</h2>
<p>Sparse matrices turn up a lot in applied machine learning.</p>
<p>In this section, we will look at some common examples to motivate you to be aware of the issues of sparsity.</p>
<h3>Data</h3>
<p>Sparse matrices come up in some specific types of data, most notably observations that record the occurrence or count of an activity.</p>
<p>Three examples include:</p>
<ul>
<li>Whether or not a user has watched a movie in a movie catalog.</li>
<li>Whether or not a user has purchased a product in a product catalog.</li>
<li>Count of the number of listens of a song in a song catalog.</li>
</ul>
<h3>Data Preparation</h3>
<p>Sparse matrices come up in encoding schemes used in the preparation of data.</p>
<p>Three common examples include:</p>
<ul>
<li>One-hot encoding, used to represent categorical data as sparse binary vectors.</li>
<li>Count encoding, used to represent the frequency of words in a vocabulary for a document</li>
<li>TF-IDF encoding, used to represent normalized word frequency scores in a vocabulary.</li>
</ul>
<h3>Areas of Study</h3>
<p>Some areas of study within machine learning must develop specialized methods to address sparsity directly as the input data is almost always sparse.</p>
<p>Three examples include:</p>
<ul>
<li>Natural language processing for working with documents of text.</li>
<li>Recommender systems for working with product usage within a catalog.</li>
<li>Computer vision when working with images that contain lots of black pixels.</li>
</ul>
<blockquote><p>If there are 100,000 words in the language model, then the feature vector has length 100,000, but for a short email message almost all the features will have count zero.</p></blockquote>
<p>&#8212; Page 22, <a href="http://amzn.to/2C4LhMW">Artificial Intelligence: A Modern Approach</a>, Third Edition, 2009.</p>
<h2>Working with Sparse Matrices</h2>
<p>The solution to representing and working with sparse matrices is to use an alternate data structure to represent the sparse data.</p>
<p>The zero values can be ignored and only the data or non-zero values in the sparse matrix need to be stored or acted upon.</p>
<p>There are multiple data structures that can be used to efficiently construct a sparse matrix; three common examples are listed below.</p>
<ul>
<li><strong>Dictionary of Keys</strong>. A dictionary is used where a row and column index is mapped to a value.</li>
<li><strong>List of Lists</strong>. Each row of the matrix is stored as a list, with each sublist containing the column index and the value.</li>
<li><strong>Coordinate List</strong>. A list of tuples is stored with each tuple containing the row index, column index, and the value.</li>
</ul>
<p>There are also data structures that are more suitable for performing efficient operations; two commonly used examples are listed below.</p>
<ul>
<li><strong>Compressed Sparse Row</strong>. The sparse matrix is represented using three one-dimensional arrays for the non-zero values, the extents of the rows, and the column indexes.</li>
<li><strong>Compressed Sparse Column</strong>. The same as the Compressed Sparse Row method except the column indices are compressed and read first before the row indices.</li>
</ul>
<p>The Compressed Sparse Row, also called CSR for short, is often used to represent sparse matrices in machine learning given the efficient access and matrix multiplication that it supports.</p>
<h2>Sparse Matrices in Python</h2>
<p>SciPy provides tools for creating sparse matrices using multiple data structures, as well as tools for converting a dense matrix to a sparse matrix.</p>
<p>Many linear algebra NumPy and SciPy functions that operate on NumPy arrays can transparently operate on SciPy sparse arrays. Further, machine learning libraries that use NumPy data structures can also operate transparently on SciPy sparse arrays, such as scikit-learn for general machine learning and Keras for deep learning.</p>
<p>A dense matrix stored in a NumPy array can be converted into a sparse matrix using the CSR representation by calling the <em>csr_matrix()</em> function.</p>
<p>In the example below, we define a 3 x 6 sparse matrix as a dense array, convert it to a CSR sparse representation, and then convert it back to a dense array by calling the <em>todense()</em> function.</p><pre class="crayon-plain-tag"># dense to sparse
from numpy import array
from scipy.sparse import csr_matrix
# create dense matrix
A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]])
print(A)
# convert to sparse matrix (CSR method)
S = csr_matrix(A)
print(S)
# reconstruct dense matrix
B = S.todense()
print(B)</pre><p>Running the example first prints the defined dense array, followed by the CSR representation, and then the reconstructed dense matrix.</p><pre class="crayon-plain-tag">[[1 0 0 1 0 0]
 [0 0 2 0 0 1]
 [0 0 0 2 0 0]]

  (0, 0)	1
  (0, 3)	1
  (1, 2)	2
  (1, 5)	1
  (2, 3)	2

[[1 0 0 1 0 0]
 [0 0 2 0 0 1]
 [0 0 0 2 0 0]]</pre><p>NumPy does not provide a function to calculate the sparsity of a matrix.</p>
<p>Nevertheless, we can calculate it easily by first finding the density of the matrix and subtracting it from one. The number of non-zero elements in a NumPy array can be given by the <em>count_nonzero()</em> function and the total number of elements in the array can be given by the size property of the array. Array sparsity can therefore be calculated as</p><pre class="crayon-plain-tag">sparsity = 1.0 - count_nonzero(A) / A.size</pre><p>The example below demonstrates how to calculate the sparsity of an array.</p><pre class="crayon-plain-tag"># calculate sparsity
from numpy import array
from numpy import count_nonzero
# create dense matrix
A = array([[1, 0, 0, 1, 0, 0], [0, 0, 2, 0, 0, 1], [0, 0, 0, 2, 0, 0]])
print(A)
# calculate sparsity
sparsity = 1.0 - count_nonzero(A) / A.size
print(sparsity)</pre><p>Running the example first prints the defined sparse matrix followed by the sparsity of the matrix.</p><pre class="crayon-plain-tag">[[1 0 0 1 0 0]
 [0 0 2 0 0 1]
 [0 0 0 2 0 0]]

0.7222222222222222</pre><p></p>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Develop your own examples for converting a dense array to sparse and calculating sparsity.</li>
<li>Develop an example for the each sparse matrix representation method supported by SciPy.</li>
<li>Select one sparsity representation method and implement it yourself from scratch.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li><a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</li>
<li>Section 2.7 Sparse Linear Systems, <a href="http://amzn.to/2CF5atj">Numerical Recipes: The Art of Scientific Computing</a>, Third Edition, 2007.</li>
<li><a href="http://amzn.to/2C4LhMW">Artificial Intelligence: A Modern Approach</a>, Third Edition, 2009.</li>
<li><a href="http://amzn.to/2DcsQVU">Direct Methods for Sparse Matrices</a>, Second Edition, 2017.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">Sparse matrices (scipy.sparse) API</a></li>
<li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.count_nonzero.html">numpy.count_nonzero() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.size.html">numpy.ndarray.size API</a></li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sparse_matrix">Sparse matrix on Wikipedia</a></li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered sparse matrices, the issues they present, and how to work with them directly in Python.</p>
<p>Specifically, you learned:</p>
<ul>
<li>That sparse matrices contain mostly zero values and are distinct from dense matrices.</li>
<li>The myriad of areas where you are likely to encounter sparse matrices in data, data preparation, and sub-fields of machine learning.</li>
<li>That there are many efficient ways to store and work with sparse matrices and SciPy provides implementations that you can use directly.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/sparse-matrices-for-machine-learning/">A Gentle Introduction to Sparse Matrices for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/sparse-matrices-for-machine-learning/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>A Gentle Introduction to Broadcasting with NumPy Arrays</title>
		<link>https://machinelearningmastery.com/broadcasting-with-numpy-arrays/</link>
		<comments>https://machinelearningmastery.com/broadcasting-with-numpy-arrays/#respond</comments>
		<pubDate>Sun, 11 Mar 2018 18:00:16 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4946</guid>
		<description><![CDATA[<p>Arrays with different sizes cannot be added, subtracted, or generally be used in arithmetic. A way to overcome this is to duplicate the smaller array so that it is the dimensionality and size as the larger array. This is called array broadcasting and is available in NumPy when performing array arithmetic, which can greatly reduce [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/broadcasting-with-numpy-arrays/">A Gentle Introduction to Broadcasting with NumPy Arrays</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Arrays with different sizes cannot be added, subtracted, or generally be used in arithmetic.</p>
<p>A way to overcome this is to duplicate the smaller array so that it is the dimensionality and size as the larger array. This is called array broadcasting and is available in NumPy when performing array arithmetic, which can greatly reduce and simplify your code.</p>
<p>In this tutorial, you will discover the concept of array broadcasting and how to implement it in NumPy.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>The problem of arithmetic with arrays with different sizes.</li>
<li>The solution of broadcasting and common examples in one and two dimensions.</li>
<li>The rule of array broadcasting and when broadcasting fails.</li>
</ul>
<p>Let’s get started.</p>
<div id="attachment_4953" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4953" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/Introduction-to-Broadcasting-with-NumPy-Arrays.jpg" alt="Introduction to Broadcasting with NumPy Arrays" width="640" height="478" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/Introduction-to-Broadcasting-with-NumPy-Arrays.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/Introduction-to-Broadcasting-with-NumPy-Arrays-300x224.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">Introduction to Broadcasting with NumPy Arrays<br />Photo by <a href="https://www.flickr.com/photos/rimuhosting/7689904958/">pbkwee</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 4 parts; they are:</p>
<ol>
<li>Limitation with Array Arithmetic</li>
<li>Array Broadcasting</li>
<li>Broadcasting in NumPy</li>
<li>Limitations of Broadcasting</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Limitation with Array Arithmetic</h2>
<p>You can perform arithmetic directly on NumPy arrays, such as addition and subtraction.</p>
<p>For example, two arrays can be added together to create a new array where the values at each index are added together.</p>
<p>For example, an array a can be defined as [1, 2, 3] and array b can be defined as [1, 2, 3] and adding together will result in a new array with the values [2, 4, 6].</p><pre class="crayon-plain-tag">a = [1, 2, 3]
b = [1, 2, 3]
c = a + b
c = [1 + 1, 2 + 2, 3 + 3]</pre><p>Strictly, arithmetic may only be performed on arrays that have the same dimensions and dimensions with the same size.</p>
<p>This means that a one-dimensional array with the length of 10 can only perform arithmetic with another one-dimensional array with the length 10.</p>
<p>This limitation on array arithmetic is quite limiting indeed. Thankfully, NumPy provides a built-in workaround to allow arithmetic between arrays with differing sizes.</p>
<h2>Array Broadcasting</h2>
<p>Broadcasting is the name given to the method that NumPy uses to allow array arithmetic between arrays with a different shape or size.</p>
<p>Although the technique was developed for <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">NumPy</a>, it has also been adopted more broadly in other numerical computational libraries, such as <a href="http://deeplearning.net/software/theano/tutorial/broadcasting.html">Theano</a>, <a href="https://www.tensorflow.org/performance/xla/broadcasting">TensorFlow</a>, and <a href="https://www.gnu.org/software/octave/doc/v4.2.1/Broadcasting.html">Octave</a>.</p>
<p>Broadcasting solves the problem of arithmetic between arrays of differing shapes by in effect replicating the smaller array along the last mismatched dimension.</p>
<blockquote><p>The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.</p></blockquote>
<p>&#8212; <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">Broadcasting</a>, SciPy.org</p>
<p>NumPy does not actually duplicate the smaller array; instead, it makes memory and computationally efficient use of existing structures in memory that in effect achieve the same result.</p>
<p>The concept has also permeated linear algebra notation to simplify the explanation of simple operations.</p>
<blockquote><p>In the context of deep learning, we also use some less conventional notation. We allow the addition of matrix and a vector, yielding another matrix: C = A + b, where Ci,j = Ai,j + bj. In other words, the vector b is added to each row of the matrix. This shorthand eliminates the need to define a matrix with b copied into each row before doing the addition. This implicit copying of b to many locations is called broadcasting.</p></blockquote>
<p>&#8212; Page 34, <a href="http://amzn.to/2qJRxrv">Deep Learning</a>, 2016.</p>
<h2>Broadcasting in NumPy</h2>
<p>We can make broadcasting concrete by looking at three examples in NumPy.</p>
<p>The examples in this section are not exhaustive, but instead are common to the types of broadcasting you may see or implement.</p>
<h3>Scalar and One-Dimensional Array</h3>
<p>A single value or scalar can be used in arithmetic with a one-dimensional array.</p>
<p>For example, we can imagine a one-dimensional array &#8220;a&#8221; with three values [a1, a2, a3] added to a scalar &#8220;b&#8221;.</p><pre class="crayon-plain-tag">a = [a1, a2, a3]
b</pre><p>The scalar will need to be broadcast across the one-dimensional array by duplicating the value it 2 more times.</p><pre class="crayon-plain-tag">b = [b1, b2, b3]</pre><p>The two one-dimensional arrays can then be added directly.</p><pre class="crayon-plain-tag">c = a + b
c = [a1 + b1, a2 + b2, a3 + b3]</pre><p>The example below demonstrates this in NumPy.</p><pre class="crayon-plain-tag"># scalar and one-dimensional
from numpy import array
a = array([1, 2, 3])
print(a)
b = 2
print(b)
c = a + b
print(c)</pre><p>Running the example first prints the defined one-dimensional array, then the scalar, followed by the result where the scalar is added to each value in the array.</p><pre class="crayon-plain-tag">[1 2 3]

2

[3 4 5]</pre><p></p>
<h3>Scalar and Two-Dimensional Array</h3>
<p>A scalar value can be used in arithmetic with a two-dimensional array.</p>
<p>For example, we can imagine a two-dimensional array &#8220;A&#8221; with 2 rows and 3 columns added to the scalar &#8220;b&#8221;.</p><pre class="crayon-plain-tag">a11, a12, a13
A = (a21, a22, a23)

b</pre><p>The scalar will need to be broadcast across each row of the two-dimensional array by duplicating it 5 more times.</p><pre class="crayon-plain-tag">b11, b12, b13
B = (b21, b22, b23)</pre><p>The two two-dimensional arrays can then be added directly.</p><pre class="crayon-plain-tag">C = A + B

     a11 + b11, a12 + b12, a13 + b13
C = (a21 + b21, a22 + b22, a23 + b23)</pre><p>The example below demonstrates this in NumPy.</p><pre class="crayon-plain-tag"># scalar and two-dimensional
from numpy import array
A = array([[1, 2, 3], [1, 2, 3]])
print(A)
b = 2
print(b)
C = A + b
print(C)</pre><p>Running the example first prints the defined two-dimensional array, then the scalar, then the result of the addition with the value &#8220;2&#8221; added to each value in the array.</p><pre class="crayon-plain-tag">[[1 2 3]
 [1 2 3]]

2

[[3 4 5]
 [3 4 5]]</pre><p></p>
<h3>One-Dimensional and Two-Dimensional Arrays</h3>
<p>A one-dimensional array can be used in arithmetic with a two-dimensional array.</p>
<p>For example, we can imagine a two-dimensional array &#8220;A&#8221; with 2 rows and 3 columns added to a one-dimensional array &#8220;b&#8221; with 3 values.</p><pre class="crayon-plain-tag">a11, a12, a13
A = (a21, a22, a23)

b = (b1, b2, b3)</pre><p>The one-dimensional array is broadcast across each row of the two-dimensional array by creating a second copy to result in a new two-dimensional array &#8220;B&#8221;.</p><pre class="crayon-plain-tag">b11, b12, b13
B = (b21, b22, b23)</pre><p>The two two-dimensional arrays can then be added directly.</p><pre class="crayon-plain-tag">C = A + B

     a11 + b11, a12 + b12, a13 + b13
C = (a21 + b21, a22 + b22, a23 + b23)</pre><p>Below is a worked example in NumPy.</p><pre class="crayon-plain-tag"># one-dimensional and two-dimensional
from numpy import array
A = array([[1, 2, 3], [1, 2, 3]])
print(A)
b = array([1, 2, 3])
print(b)
C = A + b
print(C)</pre><p>Running the example first prints the defined two-dimensional array, then the defined one-dimensional array, followed by the result C where in effect each value in the two-dimensional array is doubled.</p><pre class="crayon-plain-tag">[[1 2 3]
 [1 2 3]]

[1 2 3]

[[2 4 6]
 [2 4 6]]</pre><p></p>
<h2>Limitations of Broadcasting</h2>
<p>Broadcasting is a handy shortcut that proves very useful in practice when working with NumPy arrays.</p>
<p>That being said, it does not work for all cases, and in fact imposes a strict rule that must be satisfied for broadcasting to be performed.</p>
<p>Arithmetic, including broadcasting, can only be performed when the shape of each dimension in the arrays are equal or one has the dimension size of 1. The dimensions are considered in reverse order, starting with the trailing dimension; for example, looking at columns before rows in a two-dimensional case.</p>
<p>This make more sense when we consider that NumPy will in effect pad missing dimensions with a size of &#8220;1&#8221; when comparing arrays.</p>
<p>Therefore, the comparison between a two-dimensional array &#8220;A&#8221; with 2 rows and 3 columns and a vector &#8220;b&#8221; with 3 elements:</p><pre class="crayon-plain-tag">A.shape = (2 x 3)
b.shape = (3)</pre><p>In effect, this becomes a comparison between:</p><pre class="crayon-plain-tag">A.shape = (2 x 3)
b.shape = (1 x 3)</pre><p>This same notion applies to the comparison between a scalar that is treated as an array with the required number of dimensions:</p><pre class="crayon-plain-tag">A.shape = (2 x 3)
b.shape = (1)</pre><p>This becomes a comparison between:</p><pre class="crayon-plain-tag">A.shape = (2 x 3)
b.shape = (1 x 1)</pre><p>When the comparison fails, the broadcast cannot be performed, and an error is raised.</p>
<p>The example below attempts to broadcast a two-element array to a 2 x 3 array. This comparison is in effect:</p><pre class="crayon-plain-tag">A.shape = (2 x 3)
b.shape = (1 x 2)</pre><p>We can see that the last dimensions (columns) do not match and we would expect the broadcast to fail.</p>
<p>The example below demonstrates this in NumPy.</p><pre class="crayon-plain-tag"># broadcasting error
from numpy import array
A = array([[1, 2, 3], [1, 2, 3]])
print(A.shape)
b = array([1, 2])
print(b.shape)
C = A + b
print(C)</pre><p>Running the example first prints the shapes of the arrays then raises an error when attempting to broadcast, as we expected.</p><pre class="crayon-plain-tag">(2, 3)
(2,)
ValueError: operands could not be broadcast together with shapes (2,3) (2,)</pre><p></p>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Create three new and different examples of broadcasting with NumPy arrays.</li>
<li>Implement your own broadcasting function for manually broadcasting in one and two-dimensional cases.</li>
<li>Benchmark NumPy broadcasting and your own custom broadcasting functions with one and two dimensional cases with very large arrays.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li>Chapter 2, <a href="http://amzn.to/2CFmZZw">Deep Learning</a>, 2016.</li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html">Broadcasting, NumPy API</a>, SciPy.org</li>
<li><a href="https://www.tensorflow.org/performance/xla/broadcasting">Broadcasting semantics in TensorFlow</a></li>
<li><a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc">Array Broadcasting in numpy</a>, EricsBroadcastingDoc</li>
<li><a href="http://deeplearning.net/software/theano/tutorial/broadcasting.html">Broadcasting</a>, Theano</li>
<li><a href="https://eli.thegreenplace.net/2015/broadcasting-arrays-in-numpy/">Broadcasting arrays in Numpy</a>, 2015.</li>
<li><a href="https://www.gnu.org/software/octave/doc/v4.2.1/Broadcasting.html">Broadcasting in Octave</a></li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered the concept of array broadcasting and how to implement in NumPy.</p>
<p>Specifically, you learned:</p>
<ul>
<li>The problem of arithmetic with arrays with different sizes.</li>
<li>The solution of broadcasting and common examples in one and two dimensions.</li>
<li>The rule of array broadcasting and when broadcasting fails.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/broadcasting-with-numpy-arrays/">A Gentle Introduction to Broadcasting with NumPy Arrays</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/broadcasting-with-numpy-arrays/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>10 Examples of Linear Algebra in Machine Learning</title>
		<link>https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/</link>
		<comments>https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/#comments</comments>
		<pubDate>Thu, 08 Mar 2018 18:00:08 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4942</guid>
		<description><![CDATA[<p>Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms. It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code. Although linear algebra is integral to the field of machine learning, the tight relationship [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/">10 Examples of Linear Algebra in Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Linear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms.</p>
<p>It is a key foundation to the field of machine learning, from notations used to describe the operation of algorithms to the implementation of algorithms in code.</p>
<p>Although linear algebra is integral to the field of machine learning, the tight relationship is often left unexplained or explained using abstract concepts such as vector spaces or specific matrix operations.</p>
<p>In this post, you will discover 10 common examples of machine learning that you may be familiar with that use, require and are really best understood using linear algebra.</p>
<p>After reading this post, you will know:</p>
<ul>
<li>The use of linear algebra structures when working with data, such as tabular datasets and images.</li>
<li>Linear algebra concepts when working with data preparation, such as one hot encoding and dimensionality reduction.</li>
<li>The ingrained use of linear algebra notation and methods in sub-fields such as deep learning, natural language processing, and recommender systems.</li>
</ul>
<p>Let&#8217;s get started.</p>
<div id="attachment_4944" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4944" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/10-Examples-of-Linear-Algebra-in-Machine-Learning.jpg" alt="10 Examples of Linear Algebra in Machine Learning" width="640" height="427" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/10-Examples-of-Linear-Algebra-in-Machine-Learning.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/10-Examples-of-Linear-Algebra-in-Machine-Learning-300x200.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">10 Examples of Linear Algebra in Machine Learning<br />Photo by <a href="https://www.flickr.com/photos/jsbarbosa/33573815251/">j. Barbosa</a>, some rights reserved.</p></div>
<h2>Overview</h2>
<p>In this post, we will review 10 obvious and concrete examples of linear algebra in machine learning.</p>
<p>I tried to pick examples that you may be familiar with or have even worked with before. They are:</p>
<ol>
<li>Dataset and Data Files</li>
<li>Images and Photographs</li>
<li>One-Hot Encoding</li>
<li>Linear Regression</li>
<li>Regularization</li>
<li>Principal Component Analysis</li>
<li>Singular-Value Decomposition</li>
<li>Latent Semantic Analysis</li>
<li>Recommender Systems</li>
<li>Deep Learning</li>
</ol>
<p>Do you have your own favorite example of linear algebra in machine learning?<br />
Let me know in the comments below.</p>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>1. Dataset and Data Files</h2>
<p>In machine learning, you fit a model on a dataset.</p>
<p>This is the table-like set of numbers where each row represents an observation and each column represents a feature of the observation.</p>
<p>For example, below is a snippet of the <a href="http://archive.ics.uci.edu/ml/datasets/Iris">Iris flowers dataset</a>:</p><pre class="crayon-plain-tag">5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa</pre><p>This data is in fact a matrix: a key data structure in linear algebra.</p>
<p>Further, when you split the data into inputs and outputs to fit a supervised machine learning model, such as the measurements and the flower species, you have a matrix (X) and a vector (y). The vector is another key data structure in linear algebra.</p>
<p>Each row has the same length, i.e. the same number of columns, therefore we can say that the data is vectorized where rows can be provided to a model one at a time or in a batch and the model can be pre-configured to expect rows of a fixed width.</p>
<h2>2. Images and Photographs</h2>
<p>Perhaps you are more used to working with images or photographs in computer vision applications.</p>
<p>Each image that you work with is itself a table structure with a width and height and one pixel value in each cell for black and white images or 3 pixel values in each cell for a color image.</p>
<p>A photo is yet another example of a matrix from linear algebra.</p>
<p>Operations on the image, such as cropping, scaling, shearing, and so on are all described using the notation and operations of linear algebra.</p>
<h2>3. One Hot Encoding</h2>
<p>Sometimes you work with categorical data in machine learning.</p>
<p>Perhaps the class labels for classification problems, or perhaps categorical input variables.</p>
<p>It is common to encode categorical variables to make them easier to work with and learn by some techniques. A popular encoding for categorical variables is the one hot encoding.</p>
<p>A one hot encoding is where a table is created to represent the variable with one column for each category and a row for each example in the dataset. A check, or one-value, is added in the column for the categorical value for a given row, and a zero-value is added to all other columns.</p>
<p>For example, the color variable with the 3 rows:</p><pre class="crayon-plain-tag">red
green
blue
...</pre><p>Might be encoded as:</p><pre class="crayon-plain-tag">red, green, blue
1, 0, 0
0, 1, 0
0, 0, 1
...</pre><p>Each row is encoded as a binary vector, a vector with zero or one values and this is an example of a sparse representation, a whole sub-field of linear algebra.</p>
<h2>4. Linear Regression</h2>
<p>Linear regression is an old method from statistics for describing the relationships between variables.</p>
<p>It is often used in machine learning for predicting numerical values in simpler regression problems.</p>
<p>There are many ways to describe and solve the linear regression problem, i.e. finding a set of coefficients that when multiplied by each of the input variables and added together results in the best prediction of the output variable.</p>
<p>If you have used a machine learning tool or library, the most common way of solving linear regression is via a least squares optimization that is solved using matrix factorization methods from linear regression, such as an LU decomposition or a singular-value decomposition, or SVD.</p>
<p>Even the common way of summarizing the linear regression equation uses linear algebra notation:</p><pre class="crayon-plain-tag">y = A . b</pre><p>Where y is the output variable A is the dataset and b are the model coefficients.</p>
<h2>5. Regularization</h2>
<p>In applied machine learning, we often seek the simplest possible models that achieve the best skill on our problem.</p>
<p>Simpler models are often better at generalizing from specific examples to unseen data.</p>
<p>In many methods that involve coefficients, such as regression methods and artificial neural networks, simpler models are often characterized by models that have smaller coefficient values.</p>
<p>A technique that is often used to encourage a model to minimize the size of coefficients while it is being fit on data is called regularization. Common implementations include the L2 and L1 forms of regularization.</p>
<p>Both of these forms of regularization are in fact a measure of the magnitude or length of the coefficients as a vector and are methods lifted directly from linear algebra called the vector norm.</p>
<h2>6. Principal Component Analysis</h2>
<p>Often, a dataset has many columns, perhaps tens, hundreds, thousands, or more.</p>
<p>Modeling data with many features is challenging, and models built from data that include irrelevant features are often less skillful than models trained from the most relevant data.</p>
<p>It is hard to know which features of the data are relevant and which are not.</p>
<p>Methods for automatically reducing the number of columns of a dataset are called dimensionality reduction, and perhaps the most popular method is called the principal component analysis, or PCA for short.</p>
<p>This method is used in machine learning to create projections of high-dimensional data for both visualization and for training models.</p>
<p>The core of the PCA method is a matrix factorization method from linear algebra. The eigendecomposition can be used and more robust implementations may use the singular-value decomposition, or SVD.</p>
<h2>7. Singular-Value Decomposition</h2>
<p>Another popular dimensionality reduction method is the singular-value decomposition method, or SVD for short.</p>
<p>As mentioned, and as the name of the method suggests, it is a matrix factorization method from the field of linear algebra.</p>
<p>It has wide use in linear algebra and can be used directly in applications such as feature selection, visualization, noise reduction, and more.</p>
<p>We will see two more cases below of using the SVD in machine learning.</p>
<h2>8. Latent Semantic Analysis</h2>
<p>In the sub-field of machine learning for working with text data called natural language processing, it is common to represent documents as large matrices of word occurrences.</p>
<p>For example, the columns of the matrix may be the known words in the vocabulary and rows may be sentences, paragraphs, pages, or documents of text with cells in the matrix marked as the count or frequency of the number of times the word occurred.</p>
<p>This is a sparse matrix representation of the text. Matrix factorization methods, such as the singular-value decomposition can be applied to this sparse matrix, which has the effect of distilling the representation down to its most relevant essence. Documents processed in this way are much easier to compare, query, and use as the basis for a supervised machine learning model.</p>
<p>This form of data preparation is called Latent Semantic Analysis, or LSA for short, and is also known by the name Latent Semantic Indexing, or LSI.</p>
<h2>9. Recommender Systems</h2>
<p>Predictive modeling problems that involve the recommendation of products are called recommender systems, a sub-field of machine learning.</p>
<p>Examples include the recommendation of books based on previous purchases and purchases by customers like you on Amazon, and the recommendation of movies and TV shows to watch based on your viewing history and viewing history of subscribers like you on Netflix.</p>
<p>The development of recommender systems is primarily concerned with linear algebra methods. A simple example is in the calculation of the similarity between sparse customer behavior vectors using distance measures such as Euclidean distance or dot products.</p>
<p>Matrix factorization methods like the singular-value decomposition are used widely in recommender systems to distill item and user data to their essence for querying and searching and comparison.</p>
<h2>10. Deep Learning</h2>
<p>Artificial neural networks are nonlinear machine learning algorithms that are inspired by elements of the information processing in the brain and have proven effective at a range of problems, not the least of which is predictive modeling.</p>
<p>Deep learning is the recent resurgence in the use of artificial neural networks with newer methods and faster hardware that allow for the development and training of larger and deeper (more layers) networks on very large datasets. Deep learning methods are routinely achieving state-of-the-art results on a range of challenging problems such as machine translation, photo captioning, speech recognition, and much more.</p>
<p>At their core, the execution of neural networks involves linear algebra data structures multiplied and added together. Scaled up to multiple dimensions, deep learning methods work with vectors, matrices, and even tensors of inputs and coefficients, where a tensor is a matrix with more than two dimensions.</p>
<p>Linear algebra is central to the description of deep learning methods via matrix notation to the implementation of deep learning methods such as Google&#8217;s TensorFlow Python library that has the word &#8220;tensor&#8221; in its name.</p>
<h2>Summary</h2>
<p>In this post, you discovered 10 common examples of machine learning that you may be familiar with that use and require linear algebra.</p>
<p>Specifically, you learned:</p>
<ul>
<li>The use of linear algebra structures when working with data such as tabular datasets and images.</li>
<li>Linear algebra concepts when working with data preparation such as one hot encoding and dimensionality reduction.</li>
<li>The ingrained use of linear algebra notation and methods in sub-fields such as deep learning, natural language processing, and recommender systems.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/">10 Examples of Linear Algebra in Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/examples-of-linear-algebra-in-machine-learning/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		</item>
		<item>
		<title>No Bullshit Guide To Linear Algebra Review</title>
		<link>https://machinelearningmastery.com/no-bullshit-guide-to-linear-algebra-review/</link>
		<comments>https://machinelearningmastery.com/no-bullshit-guide-to-linear-algebra-review/#comments</comments>
		<pubDate>Tue, 06 Mar 2018 18:00:39 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4888</guid>
		<description><![CDATA[<p>There are many books that provide an introduction to the field of linear algebra. Most are textbooks targeted at undergraduate students and are full of theoretical digressions that are barely relevant and mostly distracting to a beginner or practitioner to the field. In this post, you will discover the book &#8220;&#8221; that provides a gentle [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/no-bullshit-guide-to-linear-algebra-review/">No Bullshit Guide To Linear Algebra Review</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>There are many books that provide an introduction to the field of linear algebra.</p>
<p>Most are textbooks targeted at undergraduate students and are full of theoretical digressions that are barely relevant and mostly distracting to a beginner or practitioner to the field.</p>
<p>In this post, you will discover the book &#8220;<a class="easyazon-link"  href="http://www.amazon.com/dp/0992001021?tag=inspiredalgor-20">No bullshit guide to linear algebra</a>&#8221; that provides a gentle introduction to the field of linear algebra and assumes no prior mathematical knowledge.</p>
<p>After reading this post, you will know:</p>
<ul>
<li>About the goals and benefits of the book to a beginner or practitioner.</li>
<li>The contents of the book and general topics presented in each chapter.</li>
<li>A selected reading list targeted for machine learning practitioners looking to get up to speed fast.</li>
</ul>
<p>Let’s get started.</p>
<div id="attachment_4893" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4893" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/No-Bullshit-Guide-To-Linear-Algebra-Review.jpg" alt="No Bullshit Guide To Linear Algebra Review" width="640" height="428" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/No-Bullshit-Guide-To-Linear-Algebra-Review.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/No-Bullshit-Guide-To-Linear-Algebra-Review-300x201.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">No Bullshit Guide To Linear Algebra Review<br />Photo by <a href="https://www.flickr.com/photos/ralky/7233272960/">Ralf Kayser</a>, some rights reserved.</p></div>
<h2>Book Overview</h2>
<p>The book provides an introduction to linear algebra, comparable to an undergraduate university course on the subject.</p>
<p>The key approach of the book is no crap and straight to the point. This means a laser focus on a given operation or technique and no (or few) detours or digressions.</p>
<p>The book was written by Ivan Savov, the second edition of which was released in 2017. Ivan has an undergraduate degree in electrical engineering and a Masters and Ph.D. in physics and has worked for the last 15 years as a private tutor for math and physics. He knows the subject and where students encounter difficulties.</p>
<a class="easyazon-link"  href="http://www.amazon.com/dp/0992001021?tag=inspiredalgor-20"><img src="https://images-na.ssl-images-amazon.com/images/I/51CPkcClUuL.jpg" class="aligncenter" alt="Amazon Image" height="500" width="324"  /></a>
<h3>No Prerequisite Math</h3>
<p>What makes this an excellent book for the machine learning practitioner is that the book is self-contained. It does not assume any prior mathematics background and all prerequisite math, which is minimal, is covered in the first chapter titled &#8220;<em>Math fundamentals</em>.&#8221;</p>
<p>It is the perfect book if you have never studied linear algebra, or if you studied it in school decades ago and have forgotten practically everything.</p>
<h3>Exercises to Practice</h3>
<p>Another aspect that makes this book great for machine learning practitioners is that it includes exercises.</p>
<p>Each section ends with a few pop-quiz style questions.</p>
<p>Each chapter ends with a problem set for you to work through.</p>
<p>Finally, Appendix A provides answers to all exercises in the book.</p>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Table of Contents</h2>
<p>This section provides a summary of the table of contents of the book.</p>
<ol>
<li><strong>Math fundamentals</strong>. Covers the prerequisite math topics required to start learning linear algebra. Topics include numbers, functions, trigonometry, complex numbers, and set notation.</li>
<li><strong>Intro to linear algebra</strong>. An introduction into vector and matrix algebra, the very foundation of linear algebra. Topics include vector and matrix operations and linearity.</li>
<li><strong>Computational linear algebra</strong>. This chapter covers the issues that you will encounter when you start to implement linear algebra and must deal with the operations at any kind of scale. Topics include matrix equations, matrix multiplication, and determinants. Some Python examples are given.</li>
<li><strong>Geometric aspects of linear algebra</strong>. Covers the geometric intuition for vector algebra, which is quite common. Topics include lines and planes, projections and vector spaces.</li>
<li><strong>Linear transformations</strong>. Covers the core fiber of linear algebra as Ivan describes it. Introduces linear transformations.</li>
<li><strong>Theoretical linear algebra</strong>. Covers the last steps of matrix algebra prior to applications. Covers topics such as matrix factorization methods, types of matrices, and more.</li>
<li><strong>Applications</strong>. This chapter covers an impressive list of applications of linear algebra to a range of domains from electronics, graphs, computer graphics, and more. An impressive chapter to make the methods learned throughout the book concrete.</li>
<li><strong>Probability theory</strong>. Provides a crash course on probability theory in the context of linear algebra including Markov chains and the PageRank algorithm.</li>
<li><strong>Quantum mechanics</strong>. Provides a crash course into quantum mechanics through the lens of linear algebra, a specialty area of the authors.</li>
</ol>
<h2>Selections for Machine Learning Practitioners</h2>
<p>The book is excellent, and I recommend reading it from cover-to-cover, if you&#8217;re really into it.</p>
<p>But, as a machine learning practitioner, you do not need to read it all.</p>
<p>Below is a list of selected reading from the book that I recommend to get on top of linear algebra fast:</p>
<ul>
<li><strong>Concept Maps</strong>. Page v. A collection of mind-map type diagrams are provided directly after the table of contents that show how the concepts in the book, and, in fact, the concepts in the field of linear algebra, relate. If you are a visual thinker, these may help fit the pieces together.</li>
<li>Section 1.15, <strong>Vectors</strong>. Page 69. Provides a terse introduction to vectors, prior to any vector algebra. Useful background.</li>
<li>Chapter 2, <strong>Intro to Linear Algebra</strong>. Pages 101-130. Read this whole chapter. It covers:
<ul>
<li>Definitions of terms in linear algebra.</li>
<li>Vector operations such as arithmetic and vector norm.</li>
<li>Matrix operations such as arithmetic and dot product.</li>
<li>Linearity and what exactly this key concept means in linear algebra</li>
<li>Overview of how the different aspects of linear algebra (geometric, theory, etc.) relate.</li>
</ul>
</li>
<li>Section 3.2 <strong>Matrix Equations</strong>. Page 147. Includes explanations and clear diagrams for calculating matrix operations, not least the must-know matrix multiplication</li>
<li>Section 6.1 <strong>Eigenvalues and eigenvectors</strong>. Page 262. Provides an introduction to the eigendecomposition that is used as a key operation in methods such as the principal component analysis.</li>
<li>Section 6.2 <strong>Special types of matrices</strong>. Page 275. Provides an introduction to various different types of matrices such as diagonal, symmetric, orthogonal, and more.</li>
<li>Section 6.6 <strong>Matrix Decompositions</strong>. Page 295. An introduction matrix factorization methods, re-covering the eigendecomposition, but also covering the LU, QR, and Singular-Value decomposition.</li>
<li>Section 7.7 <strong>Least squares approximate solutions</strong>. Page 241. An introduction to the matrix formulation of least squares called linear least squares.</li>
<li>Appendix B, <strong>Notation</strong>. A summary of math and linear algebra notation.</li>
</ul>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<ul>
<li><a href="http://amzn.to/2k76D4C">No Bullshit Guide To Linear Algebra on Amazon</a></li>
<li><a href="https://minireference.com/">Mini Reference Publisher Homepage</a></li>
<li><a href="https://twitter.com/mcgillweb">Ivan Savov on Twitter</a></li>
<li><a href="https://minireference.com/blog/linear-algebra-tutorial/">Linear algebra explained in four pages</a>, 2013.</li>
</ul>
<h2>Summary</h2>
<p>In this post, you discovered the book &#8220;No Bullshit Guide To Linear Algebra&#8221; that provides a gentle introduction to the field of linear algebra and assumes no prior mathematical knowledge.</p>
<p>Specifically, you learned:</p>
<ul>
<li>About the goals and benefits of the book to a beginner or practitioner.</li>
<li>The contents of the book and general topics presented in each chapter.</li>
<li>A selected reading list targeted for machine learning practitioners looking to get up to speed fast.</li>
</ul>
<p>Have you read this book? What did you think?<br />
Let me know in the comments below.</p>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/no-bullshit-guide-to-linear-algebra-review/">No Bullshit Guide To Linear Algebra Review</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/no-bullshit-guide-to-linear-algebra-review/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>How to Solve Linear Regression Using Linear Algebra</title>
		<link>https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/</link>
		<comments>https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/#comments</comments>
		<pubDate>Sun, 04 Mar 2018 18:00:58 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4873</guid>
		<description><![CDATA[<p>Linear regression is a method for modeling the relationship between one or more independent variables and a dependent variable. It is a staple of statistics and is often considered a good introductory machine learning method. It is also a method that can be reformulated using matrix notation and solved using matrix operations. In this tutorial, [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/">How to Solve Linear Regression Using Linear Algebra</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Linear regression is a method for modeling the relationship between one or more independent variables and a dependent variable.</p>
<p>It is a staple of statistics and is often considered a good introductory machine learning method. It is also a method that can be reformulated using matrix notation and solved using matrix operations.</p>
<p>In this tutorial, you will discover the matrix formulation of linear regression and how to solve it using direct and matrix factorization methods.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>Linear regression and the matrix reformulation with the normal equations.</li>
<li>How to solve linear regression using a QR matrix decomposition.</li>
<li>How to solve linear regression using SVD and the pseudoinverse.</li>
</ul>
<p>Let&#8217;s get started.</p>
<div id="attachment_4887" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4887" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/How-to-Solve-Linear-Regression-Using-Linear-Algebra.jpg" alt="How to Solve Linear Regression Using Linear Algebra" width="640" height="425" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/How-to-Solve-Linear-Regression-Using-Linear-Algebra.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/How-to-Solve-Linear-Regression-Using-Linear-Algebra-300x199.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">How to Solve Linear Regression Using Linear Algebra<br />Photo by <a href="https://www.flickr.com/photos/thartz00/4855670046/">likeaduck</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 6 parts; they are:</p>
<ol>
<li>Linear Regression</li>
<li>Matrix Formulation of Linear Regression</li>
<li>Linear Regression Dataset</li>
<li>Solve Directly</li>
<li>Solve via QR Decomposition</li>
<li>Solve via Singular-Value Decomposition</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Linear Regression</h2>
<p>Linear regression is a method for modeling the relationship between two scalar values: the input variable x and the output variable y.</p>
<p>The model assumes that y is a linear function or a weighted sum of the input variable.</p><pre class="crayon-plain-tag">y = f(x)</pre><p>Or, stated with the coefficients.</p><pre class="crayon-plain-tag">y = b0 + b1 . x1</pre><p>The model can also be used to model an output variable given multiple input variables called multivariate linear regression (below, brackets were added for readability).</p><pre class="crayon-plain-tag">y = b0 + (b1 . x1) + (b2 . x2) + ...</pre><p>The objective of creating a linear regression model is to find the values for the coefficient values (b) that minimize the error in the prediction of the output variable y.</p>
<p>Matrix Formulation of Linear Regression</p>
<p>Linear regression can be stated using Matrix notation; for example:</p><pre class="crayon-plain-tag">y = X . b</pre><p>Or, without the dot notation.</p><pre class="crayon-plain-tag">y = Xb</pre><p>Where X is the input data and each column is a data feature, b is a vector of coefficients and y is a vector of output variables for each row in X.</p><pre class="crayon-plain-tag">x11, x12, x13
X = (x21, x22, x23)
     x31, x32, x33
     x41, x42, x43

     b1
b = (b2)
     b3

     y1
y = (y2)
     y3
     y4</pre><p>Reformulated, the problem becomes a system of linear equations where the b vector values are unknown. This type of system is referred to as overdetermined because there are more equations than there are unknowns, i.e. each coefficient is used on each row of data.</p>
<p>It is a challenging problem to solve analytically because there are multiple inconsistent solutions, e.g. multiple possible values for the coefficients. Further, all solutions will have some error because there is no line that will pass nearly through all points, therefore the approach to solving the equations must be able to handle that.</p>
<p>The way this is typically achieved is by finding a solution where the values for b in the model minimize the squared error. This is called linear least squares.</p><pre class="crayon-plain-tag">||X . b - y||^2 = sum i=1 to m ( sum j=1 to n Xij . bj - yi)^2</pre><p>This formulation has a unique solution as long as the input columns are independent (e.g. uncorrelated).</p>
<blockquote><p>We cannot always get the error e = b &#8211; Ax down to zero. When e is zero, x is an exact solution to Ax = b. When the length of e is as small as possible, xhat is a least squares solution.</p></blockquote>
<p>&#8212; Page 219, <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</p>
<p>In matrix notation, this problem is formulated using the so-named normal equation:</p><pre class="crayon-plain-tag">X^T . X . b = X^T . y</pre><p>This can be re-arranged in order to specify the solution for b as:</p><pre class="crayon-plain-tag">b = (X^T . X)^-1 . X^T . y</pre><p>This can be solved directly, although given the presence of the matrix inverse can be numerically challenging or unstable.</p>
<h2>Linear Regression Dataset</h2>
<p>In order to explore the matrix formulation of linear regression, let&#8217;s first define a dataset as a context.</p>
<p>We will use a simple 2D dataset where the data is easy to visualize as a scatter plot and models are easy to visualize as a line that attempts to fit the data points.</p>
<p>The example below defines a 5&#215;2 matrix dataset, splits it into X and y components, and plots the dataset as a scatter plot.</p><pre class="crayon-plain-tag">from numpy import array
from matplotlib import pyplot
data = array([
	[0.05, 0.12],
	[0.18, 0.22],
	[0.31, 0.35],
	[0.42, 0.38],
	[0.5, 0.49],
	])
print(data)
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# plot dataset
pyplot.scatter(X, y)
pyplot.show()</pre><p>Running the example first prints the defined dataset.</p><pre class="crayon-plain-tag">[[ 0.05  0.12]
 [ 0.18  0.22]
 [ 0.31  0.35]
 [ 0.42  0.38]
 [ 0.5   0.49]]</pre><p>A scatter plot of the dataset is then created showing that a straight line cannot fit this data exactly.</p>
<div id="attachment_4883" style="max-width: 1290px" class="wp-caption aligncenter"><img class="size-full wp-image-4883" src="https://machinelearningmastery.com/wp-content/uploads/2017/12/Scatter-Plot-of-Linear-Regression-Dataset.png" alt="Scatter Plot of Linear Regression Dataset" width="1280" height="960" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Linear-Regression-Dataset.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Linear-Regression-Dataset-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Linear-Regression-Dataset-768x576.png 768w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Linear-Regression-Dataset-1024x768.png 1024w" sizes="(max-width: 1280px) 100vw, 1280px" /><p class="wp-caption-text">Scatter Plot of Linear Regression Dataset</p></div>
<h2>Solve Directly</h2>
<p>The first approach is to attempt to solve the regression problem directly.</p>
<p>That is, given X, what are the set of coefficients b that when multiplied by X will give y. As we saw in a previous section, the normal equations define how to calculate b directly.</p><pre class="crayon-plain-tag">b = (X^T . X)^-1 . X^T . y</pre><p>This can be calculated directly in NumPy using the inv() function for calculating the matrix inverse.</p><pre class="crayon-plain-tag">b = inv(X.T.dot(X)).dot(X.T).dot(y)</pre><p>Once the coefficients are calculated, we can use them to predict outcomes given X.</p><pre class="crayon-plain-tag">yhat = X.dot(b)</pre><p>Putting this together with the dataset defined in the previous section, the complete example is listed below.</p><pre class="crayon-plain-tag"># solve directly
from numpy import array
from numpy.linalg import inv
from matplotlib import pyplot
data = array([
	[0.05, 0.12],
	[0.18, 0.22],
	[0.31, 0.35],
	[0.42, 0.38],
	[0.5, 0.49],
	])
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# linear least squares
b = inv(X.T.dot(X)).dot(X.T).dot(y)
print(b)
# predict using coefficients
yhat = X.dot(b)
# plot data and predictions
pyplot.scatter(X, y)
pyplot.plot(X, yhat, color='red')
pyplot.show()</pre><p>Running the example performs the calculation and prints the coefficient vector b.</p><pre class="crayon-plain-tag">[ 1.00233226]</pre><p>A scatter plot of the dataset is then created with a line plot for the model, showing a reasonable fit to the data.</p>
<div id="attachment_4884" style="max-width: 1290px" class="wp-caption aligncenter"><img class="size-full wp-image-4884" src="https://machinelearningmastery.com/wp-content/uploads/2017/12/Scatter-Plot-of-Direct-Solution-to-the-Linear-Regression-Problem.png" alt="Scatter Plot of Direct Solution to the Linear Regression Problem" width="1280" height="960" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Direct-Solution-to-the-Linear-Regression-Problem.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Direct-Solution-to-the-Linear-Regression-Problem-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Direct-Solution-to-the-Linear-Regression-Problem-768x576.png 768w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-Direct-Solution-to-the-Linear-Regression-Problem-1024x768.png 1024w" sizes="(max-width: 1280px) 100vw, 1280px" /><p class="wp-caption-text">Scatter Plot of Direct Solution to the Linear Regression Problem</p></div>
<p>A problem with this approach is the matrix inverse that is both computationally expensive and numerically unstable. An alternative approach is to use a matrix decomposition to avoid this operation. We will look at two examples in the following sections.</p>
<h2>Solve via QR Decomposition</h2>
<p>The QR decomposition is an approach of breaking a matrix down into its constituent elements.</p><pre class="crayon-plain-tag">A = Q . R</pre><p>Where A is the matrix that we wish to decompose, Q a matrix with the size m x m, and R is an upper triangle matrix with the size m x n.</p>
<p>The QR decomposition is a popular approach for solving the linear least squares equation.</p>
<p>Stepping over all of the derivation, the coefficients can be found using the Q and R elements as follows:</p><pre class="crayon-plain-tag">b = R^-1 . Q.T . y</pre><p>The approach still involves a matrix inversion, but in this case only on the simpler R matrix.</p>
<p>The QR decomposition can be found using the qr() function in NumPy. The calculation of the coefficients in NumPy looks as follows:</p><pre class="crayon-plain-tag"># QR decomposition
Q, R = qr(X)
b = inv(R).dot(Q.T).dot(y)</pre><p>Tying this together with the dataset, the complete example is listed below.</p><pre class="crayon-plain-tag"># least squares via QR decomposition
from numpy import array
from numpy.linalg import inv
from numpy.linalg import qr
from matplotlib import pyplot
data = array([
[0.05, 0.12],
[0.18, 0.22],
[0.31, 0.35],
[0.42, 0.38],
[0.5, 0.49],
])
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# QR decomposition
Q, R = qr(X)
b = inv(R).dot(Q.T).dot(y)
print(b)
# predict using coefficients
yhat = X.dot(b)
# plot data and predictions
pyplot.scatter(X, y)
pyplot.plot(X, yhat, color='red')
pyplot.show()</pre><p>Running the example first prints the coefficient solution and plots the data with the model.</p><pre class="crayon-plain-tag">[ 1.00233226]</pre><p>The QR decomposition approach is more computationally efficient and more numerically stable than calculating the normal equation directly, but does not work for all data matrices.</p>
<div id="attachment_4885" style="max-width: 1290px" class="wp-caption aligncenter"><img class="size-full wp-image-4885" src="https://machinelearningmastery.com/wp-content/uploads/2017/12/Scatter-Plot-of-QR-Decomposition-Solution-to-the-Linear-Regression-Problem.png" alt="Scatter Plot of QR Decomposition Solution to the Linear Regression Problem" width="1280" height="960" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-QR-Decomposition-Solution-to-the-Linear-Regression-Problem.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-QR-Decomposition-Solution-to-the-Linear-Regression-Problem-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-QR-Decomposition-Solution-to-the-Linear-Regression-Problem-768x576.png 768w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-QR-Decomposition-Solution-to-the-Linear-Regression-Problem-1024x768.png 1024w" sizes="(max-width: 1280px) 100vw, 1280px" /><p class="wp-caption-text">Scatter Plot of QR Decomposition Solution to the Linear Regression Problem</p></div>
<h2>Solve via Singular-Value Decomposition</h2>
<p>The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method like the QR decomposition.</p><pre class="crayon-plain-tag">X = U . Sigma . V^*</pre><p>Where A is the real n x m matrix that we wish to decompose, U is a m x m matrix, Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix, and V^* is the conjugate transpose of an n x n matrix where * is a superscript.</p>
<p>Unlike the QR decomposition, all matrices have an SVD decomposition. As a basis for solving the system of linear equations for linear regression, SVD is more stable and the preferred approach.</p>
<p>Once decomposed, the coefficients can be found by calculating the pseudoinverse of the input matrix X and multiplying that by the output vector y.</p><pre class="crayon-plain-tag">b = X^+ . y</pre><p>Where the pseudoinverse is calculated as following:</p><pre class="crayon-plain-tag">X^+ = U . D^+ . V^T</pre><p>Where X^+ is the pseudoinverse of X and the + is a superscript, D^+ is the pseudoinverse of the diagonal matrix Sigma and V^T is the transpose of V^*.</p>
<blockquote><p>Matrix inversion is not defined for matrices that are not square. [&#8230;] When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.</p></blockquote>
<p>&#8212; Page 46, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</p>
<p>We can get U and V from the SVD operation. D^+ can be calculated by creating a diagonal matrix from Sigma and calculating the reciprocal of each non-zero element in Sigma.</p><pre class="crayon-plain-tag">s11,   0,   0
Sigma = (  0, s22,   0)
           0,   0, s33

     1/s11,     0,     0
D = (    0, 1/s22,     0)
         0,     0, 1/s33</pre><p>We can calculate the SVD, then the pseudoinverse manually. Instead, NumPy provides the function pinv() that we can use directly.</p>
<p>The complete example is listed below.</p><pre class="crayon-plain-tag"># least squares via SVD with pseudoinverse
from numpy import array
from numpy.linalg import pinv
from matplotlib import pyplot
data = array([
	[0.05, 0.12],
	[0.18, 0.22],
	[0.31, 0.35],
	[0.42, 0.38],
	[0.5, 0.49],
	])
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# calculate coefficients
b = pinv(X).dot(y)
print(b)
# predict using coefficients
yhat = X.dot(b)
# plot data and predictions
pyplot.scatter(X, y)
pyplot.plot(X, yhat, color='red')
pyplot.show()</pre><p>Running the example prints the coefficient and plots the data with a red line showing the predictions from the model.</p><pre class="crayon-plain-tag">[ 1.00233226]</pre><p>In fact, NumPy provides a function to replace these two steps in the lstsq() function that you can use directly.</p>
<div id="attachment_4886" style="max-width: 1290px" class="wp-caption aligncenter"><img class="size-full wp-image-4886" src="https://machinelearningmastery.com/wp-content/uploads/2017/12/Scatter-Plot-of-SVD-Solution-to-the-Linear-Regression-Problem.png" alt="Scatter Plot of SVD Solution to the Linear Regression Problem" width="1280" height="960" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-SVD-Solution-to-the-Linear-Regression-Problem.png 1280w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-SVD-Solution-to-the-Linear-Regression-Problem-300x225.png 300w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-SVD-Solution-to-the-Linear-Regression-Problem-768x576.png 768w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2017/12/Scatter-Plot-of-SVD-Solution-to-the-Linear-Regression-Problem-1024x768.png 1024w" sizes="(max-width: 1280px) 100vw, 1280px" /><p class="wp-caption-text">Scatter Plot of SVD Solution to the Linear Regression Problem</p></div>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Implement linear regression using the built-in lstsq() NumPy function</li>
<li>Test each linear regression on your own small contrived dataset.</li>
<li>Load a tabular dataset and test each linear regression method and compare the results.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li>Section 7.7 Least squares approximate solutions. <a href="http://amzn.to/2k76D4">No Bullshit Guide To Linear Algebra</a>, 2017.</li>
<li>Section 4.3 Least Squares Approximations, <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</li>
<li>Lecture 11, Least Squares Problems, <a href="http://amzn.to/2kjEF4S">Numerical Linear Algebra</a>, 1997.</li>
<li>Chapter 5, Orthogonalization and Least Squares, <a href="http://amzn.to/2B9xnLD">Matrix Computations</a>, 2012.</li>
<li>Chapter 12, Singular-Value and Jordan Decompositions, <a href="http://amzn.to/2A9ceNv">Linear Algebra and Matrix Analysis for Statistics</a>, 2014.</li>
<li>Section 2.9 The Moore-Penrose Pseudoinverse, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</li>
<li>Section 15.4 General Linear Least Squares, <a href="http://amzn.to/2BezVEE">Numerical Recipes: The Art of Scientific Computing</a>, Third Edition, 2007.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.inv.html">numpy.linalg.inv() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.qr.html">numpy.linalg.qr() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html">numpy.linalg.svd() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.diag.html">numpy.diag() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.pinv.html">numpy.linalg.pinv() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html">numpy.linalg.lstsq() API</a></li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression">Linear regression on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Least_squares">Least squares on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)">Linear least squares (mathematics) on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Overdetermined_system">Overdetermined system on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">Singular-value decomposition on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore–Penrose inverse</a></li>
</ul>
<h3>Tutorials</h3>
<ul>
<li><a href="https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b">The Linear Algebra View of Least-Squares Regression</a><br />
<a href="https://meshlogic.github.io/posts/jupyter/linear-algebra/linear-algebra-with-python-and-numpy-2/">Linear Algebra with Python and NumPy</a></li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered the matrix formulation of linear regression and how to solve it using direct and matrix factorization methods.</p>
<p>Specifically, you learned:</p>
<ul>
<li>Linear regression and the matrix reformulation with the normal equations.</li>
<li>How to solve linear regression using a QR matrix decomposition.</li>
<li>How to solve linear regression using SVD and the pseudoinverse.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/">How to Solve Linear Regression Using Linear Algebra</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/feed/</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>How to Calculate the Principal Component Analysis from Scratch in Python</title>
		<link>https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/</link>
		<comments>https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/#comments</comments>
		<pubDate>Thu, 01 Mar 2018 18:00:14 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4865</guid>
		<description><![CDATA[<p>An important machine learning method for dimensionality reduction is called Principal Component Analysis. It is a method that uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions. In this tutorial, you will discover the Principal Component Analysis machine learning method [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/">How to Calculate the Principal Component Analysis from Scratch in Python</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>An important machine learning method for dimensionality reduction is called Principal Component Analysis.</p>
<p>It is a method that uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions.</p>
<p>In this tutorial, you will discover the Principal Component Analysis machine learning method for dimensionality reduction and how to implement it from scratch in Python.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>The procedure for calculating the Principal Component Analysis and how to choose principal components.</li>
<li>How to calculate the Principal Component Analysis from scratch in NumPy.</li>
<li>How to calculate the Principal Component Analysis for reuse on more data in scikit-learn.</li>
</ul>
<p>Let&#8217;s get started.</p>
<div id="attachment_4871" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4871" src="https://machinelearningmastery.com/wp-content/uploads/2018/03/How-to-Calculate-the-Principal-Component-Analysis-from-Scratch-in-Python.jpg" alt="How to Calculate the Principal Component Analysis from Scratch in Python" width="640" height="361" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/How-to-Calculate-the-Principal-Component-Analysis-from-Scratch-in-Python.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/03/How-to-Calculate-the-Principal-Component-Analysis-from-Scratch-in-Python-300x169.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">How to Calculate the Principal Component Analysis from Scratch in Python<br />Photo by <a href="https://www.flickr.com/photos/mc-pictures/7870255710/">mickey</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 3 parts; they are:</p>
<ol>
<li>Principal Component Analysis</li>
<li>Manually Calculate Principal Component Analysis</li>
<li>Reusable Principal Component Analysis</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Principal Component Analysis</h2>
<p>Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.</p>
<p>It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.</p>
<p>The PCA method can be described and implemented using the tools of linear algebra.</p>
<p>PCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B. Let&#8217;s walk through the steps of this operation.</p><pre class="crayon-plain-tag">a11, a12
A = (a21, a22)
     a31, a32

B = PCA(A)</pre><p>The first step is to calculate the mean values of each column.</p><pre class="crayon-plain-tag">M = mean(A)</pre><p>or</p><pre class="crayon-plain-tag">(a11 + a21 + a31) / 3
M(m11, m12) = (a12 + a22 + a32) / 3</pre><p>Next, we need to center the values in each column by subtracting the mean column value.</p><pre class="crayon-plain-tag">C = A - M</pre><p>The next step is to calculate the covariance matrix of the centered matrix C.</p>
<p>Correlation is a normalized measure of the amount and direction (positive or negative) that two columns change together. Covariance is a generalized and unnormalized version of correlation across multiple columns. A covariance matrix is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself.</p><pre class="crayon-plain-tag">V = cov(C)</pre><p>Finally, we calculate the eigendecomposition of the covariance matrix V. This results in a list of eigenvalues and a list of eigenvectors.</p><pre class="crayon-plain-tag">values, vectors = eig(V)</pre><p>The eigenvectors represent the directions or components for the reduced subspace of B, whereas the eigenvalues represent the magnitudes for the directions.</p>
<p>The eigenvectors can be sorted by the eigenvalues in descending order to provide a ranking of the components or axes of the new subspace for A.</p>
<p>If all eigenvalues have a similar value, then we know that the existing representation may already be reasonably compressed or dense and that the projection may offer little. If there are eigenvalues close to zero, they represent components or axes of B that may be discarded.</p>
<p>A total of m or less components must be selected to comprise the chosen subspace. Ideally, we would select k eigenvectors, called principal components, that have the k largest eigenvalues.</p><pre class="crayon-plain-tag">B = select(values, vectors)</pre><p>Other matrix decomposition methods can be used such as Singular-Value Decomposition, or SVD. As such, generally the values are referred to as singular values and the vectors of the subspace are referred to as principal components.</p>
<p>Once chosen, data can be projected into the subspace via matrix multiplication.</p><pre class="crayon-plain-tag">P = B^T . A</pre><p>Where A is the original data that we wish to project, B^T is the transpose of the chosen principal components and P is the projection of A.</p>
<p>This is called the covariance method for calculating the PCA, although there are alternative ways to to calculate it.</p>
<h2>Manually Calculate Principal Component Analysis</h2>
<p>There is no pca() function in NumPy, but we can easily calculate the Principal Component Analysis step-by-step using NumPy functions.</p>
<p>The example below defines a small 3&#215;2 matrix, centers the data in the matrix, calculates the covariance matrix of the centered data, and then the eigendecomposition of the covariance matrix. The eigenvectors and eigenvalues are taken as the principal components and singular values and used to project the original data.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import mean
from numpy import cov
from numpy.linalg import eig
# define a matrix
A = array([[1, 2], [3, 4], [5, 6]])
print(A)
# calculate the mean of each column
M = mean(A.T, axis=1)
print(M)
# center columns by subtracting column means
C = A - M
print(C)
# calculate covariance matrix of centered matrix
V = cov(C.T)
print(V)
# eigendecomposition of covariance matrix
values, vectors = eig(V)
print(vectors)
print(values)
# project data
P = vectors.T.dot(C.T)
print(P.T)</pre><p>Running the example first prints the original matrix, then the eigenvectors and eigenvalues of the centered covariance matrix, followed finally by the projection of the original matrix.</p>
<p>Interestingly, we can see that only the first eigenvector is required, suggesting that we could project our 3&#215;2 matrix onto a 3&#215;1 matrix with little loss.</p><pre class="crayon-plain-tag">[[1 2]
 [3 4]
 [5 6]]

[[ 0.70710678 -0.70710678]
 [ 0.70710678  0.70710678]]

[ 8.  0.]

[[-2.82842712  0.        ]
 [ 0.          0.        ]
 [ 2.82842712  0.        ]]</pre><p></p>
<h2>Reusable Principal Component Analysis</h2>
<p>We can calculate a Principal Component Analysis on a dataset using the PCA() class in the scikit-learn library. The benefit of this approach is that once the projection is calculated, it can be applied to new data again and again quite easily.</p>
<p>When creating the class, the number of components can be specified as a parameter.</p>
<p>The class is first fit on a dataset by calling the fit() function, and then the original dataset or other data can be projected into a subspace with the chosen number of dimensions by calling the transform() function.</p>
<p>Once fit, the singular values and principal components can be accessed on the PCA class via the explained_variance_ and components_ attributes.</p>
<p>The example below demonstrates using this class by first creating an instance, fitting it on a 3&#215;2 matrix, accessing the values and vectors of the projection, and transforming the original data.</p><pre class="crayon-plain-tag"># Principal Component Analysis
from numpy import array
from sklearn.decomposition import PCA
# define a matrix
A = array([[1, 2], [3, 4], [5, 6]])
print(A)
# create the PCA instance
pca = PCA(2)
# fit on data
pca.fit(A)
# access values and vectors
print(pca.components_)
print(pca.explained_variance_)
# transform data
B = pca.transform(A)
print(B)</pre><p>Running the example first prints the 3&#215;2 data matrix, then the principal components and values, followed by the projection of the original matrix.</p>
<p>We can see, that with some very minor floating point rounding that we achieve the same principal components, singular values, and projection as in the previous example.</p><pre class="crayon-plain-tag">[[1 2]
 [3 4]
 [5 6]]

[[ 0.70710678  0.70710678]
 [ 0.70710678 -0.70710678]]

[  8.00000000e+00   2.25080839e-33]

[[ -2.82842712e+00   2.22044605e-16]
 [  0.00000000e+00   0.00000000e+00]
 [  2.82842712e+00  -2.22044605e-16]]</pre><p></p>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Re-run the examples with your own small contrived matrix values.</li>
<li>Load a dataset and calculate the PCA on it and compare the results from the two methods.</li>
<li>Search for and locate 10 examples where PCA has been used in machine learning papers.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li>Section 7.3 Principal Component Analysis (PCA by the SVD), <a href="http://amzn.to/2CZgTTB">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</li>
<li>Section 2.12 Example: Principal Components Analysis, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html">numpy.mean() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.cov.html">numpy.cov() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html">numpy.linalg.eig() API</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn.decomposition.PCA API</a></li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covariance_matrix">Covariance matrix</a></li>
</ul>
<h3>Tutorials</h3>
<ul>
<li><a href="https://glowingpython.blogspot.com.au/2011/07/principal-component-analysis-with-numpy.html">Principal Component Analysis with numpy</a>, 2011.</li>
<li><a href="https://glowingpython.blogspot.com.au/2011/07/pca-and-image-compression-with-numpy.html">PCA and image compression with numpy</a>, 2011.</li>
<li><a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html">Implementing a Principal Component Analysis (PCA)</a>, 2014.</li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered the Principal Component Analysis machine learning method for dimensionality reduction.</p>
<p>Specifically, you learned:</p>
<ul>
<li>The procedure for calculating the Principal Component Analysis and how to choose principal components.</li>
<li>How to calculate the Principal Component Analysis from scratch in NumPy.</li>
<li>How to calculate the Principal Component Analysis for reuse on more data in scikit-learn.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/">How to Calculate the Principal Component Analysis from Scratch in Python</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		</item>
		<item>
		<title>A Gentle Introduction to Expected Value, Variance, and Covariance with NumPy</title>
		<link>https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/</link>
		<comments>https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/#respond</comments>
		<pubDate>Tue, 27 Feb 2018 18:00:14 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4846</guid>
		<description><![CDATA[<p>Fundamental statistics are useful tools in applied machine learning for a better understanding your data. They are also the tools that provide the foundation for more advanced linear algebra operations and machine learning methods, such as the covariance matrix and principal component analysis respectively. As such, it is important to have a strong grip on [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/">A Gentle Introduction to Expected Value, Variance, and Covariance with NumPy</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Fundamental statistics are useful tools in applied machine learning for a better understanding your data.</p>
<p>They are also the tools that provide the foundation for more advanced linear algebra operations and machine learning methods, such as the covariance matrix and principal component analysis respectively. As such, it is important to have a strong grip on fundamental statistics in the context of linear algebra notation.</p>
<p>In this tutorial, you will discover how fundamental statistical operations work and how to implement them using NumPy with notation and terminology from linear algebra.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>What the expected value, average, and mean are and how to calculate them.</li>
<li>What the variance and standard deviation are and how to calculate them.</li>
<li>What the covariance, correlation, and covariance matrix are and how to calculate them.</li>
</ul>
<p>Let&#8217;s get started.</p>
<ul>
<li><strong>Updated Mar/2018</strong>: Fixed a small typo in the result for vector variance example. Thanks Bob.</li>
</ul>
<div id="attachment_4863" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4863" src="https://machinelearningmastery.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Expected-Value-Variance-and-Covariance-with-NumPy.jpg" alt="A Gentle Introduction to Expected Value, Variance, and Covariance with NumPy" width="640" height="427" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Expected-Value-Variance-and-Covariance-with-NumPy.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Expected-Value-Variance-and-Covariance-with-NumPy-300x200.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">A Gentle Introduction to Expected Value, Variance, and Covariance with NumPy<br />Photo by <a href="https://www.flickr.com/photos/learnscope/15866965009/">Robyn Jay</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 4 parts; they are:</p>
<ol>
<li>Expected Value</li>
<li>Variance</li>
<li>Covariance</li>
<li>Covariance Matrix</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Expected Value</h2>
<p>In probability, the average value of some random variable X is called the expected value or the expectation.</p>
<p>The expected value uses the notation E with square brackets around the name of the variable; for example:</p><pre class="crayon-plain-tag">E[X]</pre><p>It is calculated as the probability weighted sum of values that can be drawn.</p><pre class="crayon-plain-tag">E[X] = sum(x1 * p1, x2 * p2, x3 * p3, ..., xn * pn)</pre><p>In simple cases, such as the flipping of a coin or rolling a dice, the probability of each event is just as likely. Therefore, the expected value can be calculated as the sum of all values multiplied by the reciprocal of the number of values.</p><pre class="crayon-plain-tag">E[X] = sum(x1, x2, x3, ..., xn) . 1/n</pre><p>In statistics, the mean, or more technically the arithmetic mean or sample mean, can be estimated from a sample of examples drawn from the domain. It is confusing because mean, average, and expected value are used interchangeably.</p>
<p>In the abstract, the mean is denoted by the lower case Greek letter mu and is calculated from the sample of observations, rather than all possible values.</p><pre class="crayon-plain-tag">mu = sum(x1, x2, x3, ..., xn) . 1/n</pre><p>Or, written more compactly:</p><pre class="crayon-plain-tag">mu = sum(x . P(x))</pre><p>Where x is the vector of observations and P(x) is the calculated probability for each value.</p>
<p>When calculated for a specific variable, such as x, the mean is denoted as a lower case variable name with a line above, called x-bar.</p><pre class="crayon-plain-tag">_
x = sum from 1 to n (xi) . 1/n</pre><p>The arithmetic mean can be calculated for a vector or matrix in NumPy by using the mean() function.</p>
<p>The example below defines a 6-element vector and calculates the mean.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import mean
v = array([1,2,3,4,5,6])
print(v)
result = mean(v)
print(result)</pre><p>Running the example first prints the defined vector and the mean of the values in the vector.</p><pre class="crayon-plain-tag">[1 2 3 4 5 6]

3.5</pre><p>The mean function can calculate the row or column means of a matrix by specifying the axis argument and the value 0 or 1 respectively.</p>
<p>The example below defines a 2&#215;6 matrix and calculates both column and row means.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import mean
M = array([[1,2,3,4,5,6],[1,2,3,4,5,6]])
print(M)
col_mean = mean(M, axis=0)
print(col_mean)
row_mean = mean(M, axis=1)
print(row_mean)</pre><p>Running the example first prints the defined matrix, then the calculated column and row mean values.</p><pre class="crayon-plain-tag">[[1 2 3 4 5 6]
 [1 2 3 4 5 6]]

[ 1.  2.  3.  4.  5.  6.]

[ 3.5  3.5]</pre><p></p>
<h2>Variance</h2>
<p>In probability, the variance of some random variable X is a measure of how much values in the distribution vary on average with respect to the mean.</p>
<p>The variance is denoted as the function Var() on the variable.</p><pre class="crayon-plain-tag">Var[X]</pre><p>Variance is calculated as the average squared difference of each value in the distribution from the expected value. Or the expected squared difference from the expected value.</p><pre class="crayon-plain-tag">Var[X] = E[(X - E[X])^2]</pre><p>Assuming the expected value of the variable has been calculated (E[X]), the variance of the random variable can be calculated as the sum of the squared difference of each example from the expected value multiplied by the probability of that value.</p><pre class="crayon-plain-tag">Var[X] = sum (p(x1) . (x1 - E[X])^2, p(x2) . (x2 - E[X])^2, ..., p(x1) . (xn - E[X])^2)</pre><p>If the probability of each example in the distribution is equal, variance calculation can drop the individual probabilities and multiply the sum of squared differences by the reciprocal of the number of examples in the distribution.</p><pre class="crayon-plain-tag">Var[X] = sum ((x1 - E[X])^2, (x2 - E[X])^2, ...,(xn - E[X])^2) . 1/n</pre><p>In statistics, the variance can be estimated from a sample of examples drawn from the domain.</p>
<p>In the abstract, the sample variance is denoted by the lower case sigma with a 2 superscript indicating the units are squared, not that you must square the final value. The sum of the squared differences is multiplied by the reciprocal of the number of examples minus 1 to correct for a bias.</p><pre class="crayon-plain-tag">sigma^2 = sum from 1 to n ( (xi - mu)^2 ) . 1 / (n - 1)</pre><p>In NumPy, the variance can be calculated for a vector or a matrix using the var() function. By default, the var() function calculates the population variance. To calculate the sample variance, you must set the ddof argument to the value 1.</p>
<p>The example below defines a 6-element vector and calculates the sample variance.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import var
v = array([1,2,3,4,5,6])
print(v)
result = var(v, ddof=1)
print(result)</pre><p>Running the example first prints the defined vector and then the calculated sample variance of the values in the vector.</p><pre class="crayon-plain-tag">[1 2 3 4 5 6]

3.5</pre><p>The var function can calculate the row or column variances of a matrix by specifying the axis argument and the value 0 or 1 respectively, the same as the mean function above.</p>
<p>The example below defines a 2&#215;6 matrix and calculates both column and row sample variances.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import var
M = array([[1,2,3,4,5,6],[1,2,3,4,5,6]])
print(M)
col_mean = var(M, ddof=1, axis=0)
print(col_mean)
row_mean = var(M, ddof=1, axis=1)
print(row_mean)</pre><p>Running the example first prints the defined matrix and then the column and row sample variance values.</p><pre class="crayon-plain-tag">[[1 2 3 4 5 6]
 [1 2 3 4 5 6]]

[ 0.  0.  0.  0.  0.  0.]

[ 3.5  3.5]</pre><p>The standard deviation is calculated as the square root of the variance and is denoted as lowercase &#8220;s&#8221;.</p><pre class="crayon-plain-tag">s = sqrt(sigma^2)</pre><p>To keep with this notation, sometimes the variance is indicated as s^2, with 2 as a superscript, again showing that the units are squared.</p>
<p>NumPy also provides a function for calculating the standard deviation directly via the std() function. As with the var() function, the ddof argumentmust be set to 1 to calculate the unbiased sample standard deviation and column and row standard deviations can be calculated by setting the axis argument to 0 and 1 respectively.</p>
<p>The example below demonstrates how to calculate the sample standard deviation for the rows and columns of a matrix.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import std
M = array([[1,2,3,4,5,6],[1,2,3,4,5,6]])
print(M)
col_mean = std(M, ddof=1, axis=0)
print(col_mean)
row_mean = std(M, ddof=1, axis=1)
print(row_mean)</pre><p>Running the example first prints the defined matrix and then the column and row sample standard deviation values.</p><pre class="crayon-plain-tag">[[1 2 3 4 5 6]
 [1 2 3 4 5 6]]

[ 0.  0.  0.  0.  0.  0.]

[ 1.87082869  1.87082869]</pre><p></p>
<h2>Covariance</h2>
<p>In probability, covariance is the measure of the joint probability for two random variables. It describes how the two variables change together.</p>
<p>It is denoted as the function cov(X, Y), where X and Y are the two random variables being considered.</p><pre class="crayon-plain-tag">cov(X,Y)</pre><p>Covariance is calculated as expected value or average of the product of the differences of each random variable from their expected values, where E[X] is the expected value for X and E[Y] is the expected value of y.</p><pre class="crayon-plain-tag">cov(X, Y) = E[(X - E[X] . (Y - E[Y])]</pre><p>Assuming the expected values for X and Y have been calculated, the covariance can be calculated as the sum of the difference of x values from their expected value multiplied by the difference of the y values from their expected values multiplied by the reciprocal of the number of examples in the population.</p><pre class="crayon-plain-tag">cov(X, Y) = sum (x - E[X]) * (y - E[Y]) * 1/n</pre><p>In statistics, the sample covariance can be calculated in the same way, although with a bias correction, the same as with the variance.</p><pre class="crayon-plain-tag">cov(X, Y) = sum (x - E[X]) * (y - E[Y]) * 1/(n - 1)</pre><p>The sign of the covariance can be interpreted as whether the two variables increase together (positive) or decrease together (negative). The magnitude of the covariance is not easily interpreted. A covariance value of zero indicates that both variables are completely independent.</p>
<p>NumPy does not have a function to calculate the covariance between two variables directly. Instead, it has a function for calculating a covariance matrix called cov() that we can use to retrieve the covariance. By default, the cov()function will calculate the unbiased or sample covariance between the provided random variables.</p>
<p>The example below defines two vectors of equal length with one increasing and one decreasing. We would expect the covariance between these variables to be negative.</p>
<p>We access just the covariance for the two variables as the [0,1] element of the square covariance matrix returned.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import cov
x = array([1,2,3,4,5,6,7,8,9])
print(x)
y = array([9,8,7,6,5,4,3,2,1])
print(y)
Sigma = cov(x,y)[0,1]
print(Sigma)</pre><p>Running the example first prints the two vectors followed by the covariance for the values in the two vectors. The value is negative, as we expected.</p><pre class="crayon-plain-tag">[1 2 3 4 5 6 7 8 9]
[9 8 7 6 5 4 3 2 1]

-7.5</pre><p>The covariance can be normalized to a score between -1 and 1 to make the magnitude interpretable by dividing it by the standard deviation of X and Y. The result is called the correlation of the variables, also called the Pearson correlation coefficient, named for the developer of the method.</p><pre class="crayon-plain-tag">r = cov(X, Y) / sX sY</pre><p>Where r is the correlation coefficient of X and Y, cov(X, Y) is the sample covariance of X and Y and sX and sY are the standard deviations of X and Y respectively.</p>
<p>NumPy provides the corrcoef() function for calculating the correlation between two variables directly. Like cov(), it returns a matrix, in this case a correlation matrix. As with the results from cov() we can access just the correlation of interest from the [0,1] value from the returned squared matrix.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import corrcoef
x = array([1,2,3,4,5,6,7,8,9])
print(x)
y = array([9,8,7,6,5,4,3,2,1])
print(y)
Sigma = corrcoef(x,y)
print(Sigma)</pre><p>Running the example first prints the two defined vectors followed by the correlation coefficient. We can see that the vectors are maximally negatively correlated as we designed.</p><pre class="crayon-plain-tag">[1 2 3 4 5 6 7 8 9]
[9 8 7 6 5 4 3 2 1]

-1.0</pre><p></p>
<h2>Covariance Matrix</h2>
<p>The covariance matrix is a square and symmetric matrix that describes the covariance between two or more random variables.</p>
<p>The diagonal of the covariance matrix are the variances of each of the random variables.</p>
<p>A covariance matrix is a generalization of the covariance of two variables and captures the way in which all variables in the dataset may change together.</p>
<p>The covariance matrix is denoted as the uppercase Greek letter Sigma. The covariance for each pair of random variables is calculated as above.</p><pre class="crayon-plain-tag">Sigma = E[(X - E[X] . (Y - E[Y])]</pre><p>Where:</p><pre class="crayon-plain-tag">Sigma(ij) = cov(Xi, Xj)</pre><p>And X is a matrix where each column represents a random variable.</p>
<p>The covariance matrix provides a useful tool for separating the structured relationships in a matrix of random variables. This can be used to decorrelate variables or applied as a transform to other variables. It is a key element used in the Principal Component Analysis data reduction method, or PCA for short.</p>
<p>The covariance matrix can be calculated in NumPy using the cov() function. By default, this function will calculate the sample covariance matrix.</p>
<p>The cov() function can be called with a single matrix containing columns on which to calculate the covariance matrix, or two arrays, such as one for each variable.</p>
<p>Below is an example that defines two 9-element vectors and calculates the unbiased covariance matrix from them.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import cov
x = array([1,2,3,4,5,6,7,8,9])
print(x)
y = array([9,8,7,6,5,4,3,2,1])
print(y)
Sigma = cov(x,y)
print(Sigma)</pre><p>Running the example first prints the two vectors and then the calculated covariance matrix.</p>
<p>The values of the arrays were contrived such that as one variable increases, the other decreases. We would expect to see a negative sign on the covariance for these two variables, and this is what we see in the covariance matrix.</p><pre class="crayon-plain-tag">[1 2 3 4 5 6 7 8 9]

[9 8 7 6 5 4 3 2 1]

[[ 7.5 -7.5]
 [-7.5  7.5]]</pre><p>The covariance matrix is used widely in linear algebra and the intersection of linear algebra and statistics called multivariate analysis. We have only had a small taste in this post.</p>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Explore each example using your own small contrived data.</li>
<li>Load data from a CSV file and apply each operation to the data columns.</li>
<li>Write your own functions to implement each statistical operation.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li><a href="http://amzn.to/2AUcEc5">Applied Multivariate Statistical Analysis</a>, 2012.</li>
<li><a href="http://amzn.to/2AWIViz">Applied Multivariate Statistical Analysis</a>, 2015.</li>
<li>Chapter 12 Linear Algebra in Probability &amp; Statistics, <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</li>
<li>Chapter 3, Probability and Information Theory, <a href="http://amzn.to/2j4oKuP">Deep Learning</a>, 2016.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.statistics.html">NumPy Statistics Functions</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html">numpy.mean() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.var.html">numpy.var() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.std.html">numpy.std() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.cov.html">numpy.cov() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.corrcoef.html">numpy.corrcoef() API</a></li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Expected_value">Expected value on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean">Mean on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variance">Variance on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Standard_deviation">Standard deviation on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covariance">Covariance on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sample_mean_and_covariance">Sample mean and covariance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covariance_matrix">Covariance matrix on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices">Estimation of covariance matrices on Wikipedia</a></li>
</ul>
<h3>Posts</h3>
<ul>
<li><a href="http://fouryears.eu/2016/11/23/what-is-the-covariance-matrix/">What is the Covariance Matrix?</a>, 2016</li>
<li><a href="http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/">A geometric interpretation of the covariance matrix</a>, 2014.</li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered how fundamental statistical operations work and how to implement them using NumPy with notation and terminology from linear algebra.</p>
<p>Specifically, you learned:</p>
<ul>
<li>What the expected value, average, and mean are and how to calculate then.</li>
<li>What the variance and standard deviation are and how to calculate them.</li>
<li>What the covariance, correlation, and covariance matrix are and how to calculate them.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/">A Gentle Introduction to Expected Value, Variance, and Covariance with NumPy</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/introduction-to-expected-value-variance-and-covariance/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>A Gentle Introduction to Singular-Value Decomposition for Machine Learning</title>
		<link>https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/</link>
		<comments>https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/#comments</comments>
		<pubDate>Sun, 25 Feb 2018 18:00:33 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4838</guid>
		<description><![CDATA[<p>Matrix decomposition, also known as matrix factorization, involves describing a given matrix using its constituent elements. Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes it more stable than other methods, such as the eigendecomposition. As such, it is often used [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/">A Gentle Introduction to Singular-Value Decomposition for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<p>Matrix decomposition, also known as matrix factorization, involves describing a given matrix using its constituent elements.</p>
<p>Perhaps the most known and widely used matrix decomposition method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes it more stable than other methods, such as the eigendecomposition. As such, it is often used in a wide array of applications including compressing, denoising, and data reduction.</p>
<p>In this tutorial, you will discover the Singular-Value Decomposition method for decomposing a matrix into its constituent elements.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>What Singular-value decomposition is and what is involved.</li>
<li>How to calculate an SVD and reconstruct a rectangular and square matrix from SVD elements.</li>
<li>How to calculate the pseudoinverse and perform dimensionality reduction using the SVD..</li>
</ul>
<p>Let&#8217;s get started.</p>
<div id="attachment_4844" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4844" src="https://machinelearningmastery.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Singular-Value-Decomposition.jpg" alt="A Gentle Introduction to Singular-Value Decomposition" width="640" height="426" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Singular-Value-Decomposition.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/A-Gentle-Introduction-to-Singular-Value-Decomposition-300x200.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">A Gentle Introduction to Singular-Value Decomposition<br />Photo by <a href="https://www.flickr.com/photos/husker_alum/8628799410/">Chris Heald</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into 5 parts; they are:</p>
<ol>
<li>Singular-Value Decomposition</li>
<li>Calculate Singular-Value Decomposition</li>
<li>Reconstruct Matrix from SVD</li>
<li>SVD for Pseudoinverse</li>
<li>SVD for Dimensionality Reduction</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Singular-Value Decomposition</h2>
<p>The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.</p>
<p>For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers.</p><pre class="crayon-plain-tag">A = U . Sigma . V^T</pre><p>Where A is the real m x n matrix that we wish to decompose, U is an m x m matrix, Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix, and V^T is the  transpose of an n x n matrix where T is a superscript.</p>
<blockquote><p>The Singular Value Decomposition is a highlight of linear algebra.</p></blockquote>
<p>&#8212; Page 371, <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</p>
<p>The diagonal values in the Sigma matrix are known as the singular values of the original matrix A. The columns of the U matrix are called the left-singular vectors of A, and the columns of V are called the right-singular vectors of A.</p>
<p>The SVD is calculated via iterative numerical methods. We will not go into the details of these methods. Every rectangular matrix has a singular value decomposition, although the resulting matrices may contain complex numbers and the limitations of floating point arithmetic may cause some matrices to fail to decompose neatly.</p>
<blockquote><p>The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, the SVD is more generally applicable.</p></blockquote>
<p>&#8212; Pages 44-45, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</p>
<p>The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.</p>
<blockquote><p>The singular value decomposition (SVD) has numerous applications in statistics, machine learning, and computer science. Applying the SVD to a matrix is like looking inside it with X-ray vision&#8230;</p></blockquote>
<p>&#8212; Page 297, <a href="http://amzn.to/2k76D4C">No Bullshit Guide To Linear Algebra</a>, 2017</p>
<h2>Calculate Singular-Value Decomposition</h2>
<p>The SVD can be calculated by calling the svd() function.</p>
<p>The function takes a matrix and returns the U, Sigma and V^T elements. The Sigma diagonal matrix is returned as a vector of singular values. The V matrix is returned in a transposed form, e.g. V.T.</p>
<p>The example below defines a 3&#215;2 matrix and calculates the Singular-value decomposition.</p><pre class="crayon-plain-tag"># Singular-value decomposition
from numpy import array
from scipy.linalg import svd
# define a matrix
A = array([[1, 2], [3, 4], [5, 6]])
print(A)
# SVD
U, s, V = svd(A)
print(U)
print(s)
print(V)</pre><p>Running the example first prints the defined 3&#215;2 matrix, then the 3&#215;3 U matrix, 2 element Sigma vector, and 2&#215;2 V^T matrix elements calculated from the decomposition.</p><pre class="crayon-plain-tag">[[1 2]
 [3 4]
 [5 6]]

[[-0.2298477   0.88346102  0.40824829]
 [-0.52474482  0.24078249 -0.81649658]
 [-0.81964194 -0.40189603  0.40824829]]

[ 9.52551809  0.51430058]

[[-0.61962948 -0.78489445]
 [-0.78489445  0.61962948]]</pre><p></p>
<h2>Reconstruct Matrix from SVD</h2>
<p>The original matrix can be reconstructed from the U, Sigma, and V^T elements.</p>
<p>The U, s, and V elements returned from the svd() cannot be multiplied directly.</p>
<p>The s vector must be converted into a diagonal matrix using the diag() function. By default, this function will create a square matrix that is m x m, relative to our original matrix. This causes a problem as the size of the matrices do not fit the rules of matrix multiplication, where the number of columns in a matrix must match the number of rows in the subsequent matrix.</p>
<p>After creating the square Sigma diagonal matrix, the sizes of the matrices are relative to the original n x m matrix that we are decomposing, as follows:</p><pre class="crayon-plain-tag">U (m x m) . Sigma (m x m) . V^T (n x n)</pre><p>Where, in fact, we require:</p><pre class="crayon-plain-tag">U (m x m) . Sigma (m x n) . V^T (n x n)</pre><p>We can achieve this by creating a new Sigma matrix of all zero values that is m x n (e.g. more rows) and populate the first n x n part of the matrix with the square diagonal matrix calculated via diag().</p><pre class="crayon-plain-tag"># Reconstruct SVD
from numpy import array
from numpy import diag
from numpy import dot
from numpy import zeros
from scipy.linalg import svd
# define a matrix
A = array([[1, 2], [3, 4], [5, 6]])
print(A)
# Singular-value decomposition
U, s, V = svd(A)
# create m x n Sigma matrix
Sigma = zeros((A.shape[0], A.shape[1]))
# populate Sigma with n x n diagonal matrix
Sigma[:A.shape[1], :A.shape[1]] = diag(s)
# reconstruct matrix
B = U.dot(Sigma.dot(V))
print(B)</pre><p>Running the example first prints the original matrix, then the matrix reconstructed from the SVD elements.</p><pre class="crayon-plain-tag">[[1 2]
 [3 4]
 [5 6]]

[[ 1.  2.]
 [ 3.  4.]
 [ 5.  6.]]</pre><p>The above complication with the Sigma diagonal only exists with the case where m and n are not equal. The diagonal matrix can be used directly when reconstructing a square matrix, as follows.</p><pre class="crayon-plain-tag"># Reconstruct SVD
from numpy import array
from numpy import diag
from numpy import dot
from scipy.linalg import svd
# define a matrix
A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(A)
# Singular-value decomposition
U, s, V = svd(A)
# create n x n Sigma matrix
Sigma = diag(s)
# reconstruct matrix
B = U.dot(Sigma.dot(V))
print(B)</pre><p>Running the example prints the original 3&#215;3 matrix and the version reconstructed directly from the SVD elements.</p><pre class="crayon-plain-tag">[[1 2 3]
 [4 5 6]
 [7 8 9]]

[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]]</pre><p></p>
<h2>SVD for Pseudoinverse</h2>
<p>The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular matrices where the number of rows and columns are not equal.</p>
<p>It is also called the the Moore-Penrose Inverse after two independent discoverers of the method or the Generalized Inverse.</p>
<blockquote><p>Matrix inversion is not defined for matrices that are not square. [&#8230;] When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.</p></blockquote>
<p>&#8212; Page 46, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</p>
<p>The pseudoinverse is denoted as A^+, where A is the matrix that is being inverted and + is a superscript.</p>
<p>The pseudoinverse is calculated using the singular value decomposition of A:</p><pre class="crayon-plain-tag">A^+ = V . D^+ . U^T</pre><p>Or, without the dot notation:</p><pre class="crayon-plain-tag">A^+ = V . D^+ . U^T</pre><p>Where A^+ is the pseudoinverse, D^+ is the pseudoinverse of the diagonal matrix Sigma and U^T is the transpose of U.</p>
<p>We can get U and V from the SVD operation.</p><pre class="crayon-plain-tag">A = U . Sigma . V^T</pre><p>The D^+ can be calculated by creating a diagonal matrix from Sigma, calculating the reciprocal of each non-zero element in Sigma, and taking the transpose if the original matrix was rectangular.</p><pre class="crayon-plain-tag">s11,   0,   0
Sigma = (  0, s22,   0)
           0,   0, s33</pre><p></p><pre class="crayon-plain-tag">1/s11,     0,     0
D^+ = (    0, 1/s22,     0)
           0,     0, 1/s33</pre><p>The pseudoinverse provides one way of solving the linear regression equation, specifically when there are more rows than there are columns, which is often the case.</p>
<p>NumPy provides the function pinv() for calculating the pseudoinverse of a rectangular matrix.</p>
<p>The example below defines a 4&#215;2 matrix and calculates the pseudoinverse.</p><pre class="crayon-plain-tag"># Pseudoinverse
from numpy import array
from numpy.linalg import pinv
# define matrix
A = array([
	[0.1, 0.2],
	[0.3, 0.4],
	[0.5, 0.6],
	[0.7, 0.8]])
print(A)
# calculate pseudoinverse
B = pinv(A)
print(B)</pre><p>Running the example first prints the defined matrix, and then the calculated pseudoinverse.</p><pre class="crayon-plain-tag">[[ 0.1  0.2]
 [ 0.3  0.4]
 [ 0.5  0.6]
 [ 0.7  0.8]]

[[ -1.00000000e+01  -5.00000000e+00   9.04289323e-15   5.00000000e+00]
 [  8.50000000e+00   4.50000000e+00   5.00000000e-01  -3.50000000e+00]]</pre><p>We can calculate the pseudoinverse manually via the SVD and compare the results to the pinv() function.</p>
<p>First we must calculate the SVD. Next we must calculate the reciprocal of each value in the s array. Then the s array can be transformed into a diagonal matrix with an added row of zeros to make it rectangular. Finally, we can calculate the pseudoinverse from the elements.</p>
<p>The specific implementation is:</p><pre class="crayon-plain-tag">A^+ = V^T . D^T . U^V</pre><p>The full example is listed below.</p><pre class="crayon-plain-tag"># Pseudoinverse via SVD
from numpy import array
from numpy.linalg import svd
from numpy import zeros
from numpy import diag
# define matrix
A = array([
	[0.1, 0.2],
	[0.3, 0.4],
	[0.5, 0.6],
	[0.7, 0.8]])
print(A)
# calculate svd
U, s, V = svd(A)
# reciprocals of s
d = 1.0 / s
# create m x n D matrix
D = zeros(A.shape)
# populate D with n x n diagonal matrix
D[:A.shape[1], :A.shape[1]] = diag(d)
# calculate pseudoinverse
B = V.T.dot(D.T).dot(U.T)
print(B)</pre><p>Running the example first prints the defined rectangular matrix and the pseudoinverse that matches the above results from the pinv() function.</p><pre class="crayon-plain-tag">[[ 0.1  0.2]
 [ 0.3  0.4]
 [ 0.5  0.6]
 [ 0.7  0.8]]

[[ -1.00000000e+01  -5.00000000e+00   9.04831765e-15   5.00000000e+00]
 [  8.50000000e+00   4.50000000e+00   5.00000000e-01  -3.50000000e+00]]</pre><p></p>
<h2>SVD for Dimensionality Reduction</h2>
<p>A popular application of SVD is for dimensionality reduction.</p>
<p>Data with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem.</p>
<p>The result is a matrix with a lower rank that is said to approximate the original matrix.</p>
<p>To do this we can perform an SVD operation on the original data and select the top k largest singular values in Sigma. These columns can be selected from Sigma and the rows selected from V^T.</p>
<p>An approximate B of the original vector A can then be reconstructed.</p><pre class="crayon-plain-tag">B = U . Sigmak . V^Tk</pre><p>In natural language processing, this approach can be used on matrices of word occurrences or word frequencies in documents and is called Latent Semantic Analysis or Latent Semantic Indexing.</p>
<p>In practice, we can retain and work with a descriptive subset of the data called T. This is a dense summary of the matrix or a projection.</p><pre class="crayon-plain-tag">T = U . Sigmak</pre><p>Further, this transform can be calculated and applied to the original matrix A as well as other similar matrices.</p><pre class="crayon-plain-tag">T = V^Tk . A</pre><p>The example below demonstrates data reduction with the SVD.</p>
<p>First a 3&#215;10 matrix is defined, with more columns than rows. The SVD is calculated and only the first two features are selected. The elements are recombined to give an accurate reproduction of the original matrix. Finally the transform is calculated two different ways.</p><pre class="crayon-plain-tag">from numpy import array
from numpy import diag
from numpy import zeros
from scipy.linalg import svd
# define a matrix
A = array([
	[1,2,3,4,5,6,7,8,9,10],
	[11,12,13,14,15,16,17,18,19,20],
	[21,22,23,24,25,26,27,28,29,30]])
print(A)
# Singular-value decomposition
U, s, V = svd(A)
# create m x n Sigma matrix
Sigma = zeros((A.shape[0], A.shape[1]))
# populate Sigma with n x n diagonal matrix
Sigma[:A.shape[0], :A.shape[0]] = diag(s)
# select
n_elements = 2
Sigma = Sigma[:, :n_elements]
V = V[:n_elements, :]
# reconstruct
B = U.dot(Sigma.dot(V))
print(B)
# transform
T = U.dot(Sigma)
print(T)
T = A.dot(V.T)
print(T)</pre><p>Running the example first prints the defined matrix then the reconstructed approximation, followed by two equivalent transforms of the original matrix.</p><pre class="crayon-plain-tag">[[ 1  2  3  4  5  6  7  8  9 10]
 [11 12 13 14 15 16 17 18 19 20]
 [21 22 23 24 25 26 27 28 29 30]]

[[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]
 [ 11.  12.  13.  14.  15.  16.  17.  18.  19.  20.]
 [ 21.  22.  23.  24.  25.  26.  27.  28.  29.  30.]]

[[-18.52157747   6.47697214]
 [-49.81310011   1.91182038]
 [-81.10462276  -2.65333138]]

[[-18.52157747   6.47697214]
 [-49.81310011   1.91182038]
 [-81.10462276  -2.65333138]]</pre><p>The scikit-learn provides a TruncatedSVD class that implements this capability directly.</p>
<p>The TruncatedSVD class can be created in which you must specify the number of desirable features or components to select, e.g. 2. Once created, you can fit the transform (e.g. calculate V^Tk) by calling the fit() function, then apply it to the original matrix by calling the transform() function. The result is the transform of A called T above.</p>
<p>The example below demonstrates the TruncatedSVD class.</p><pre class="crayon-plain-tag">from numpy import array
from sklearn.decomposition import TruncatedSVD
# define array
A = array([
	[1,2,3,4,5,6,7,8,9,10],
	[11,12,13,14,15,16,17,18,19,20],
	[21,22,23,24,25,26,27,28,29,30]])
print(A)
# svd
svd = TruncatedSVD(n_components=2)
svd.fit(A)
result = svd.transform(A)
print(result)</pre><p>Running the example first prints the defined matrix, followed by the transformed version of the matrix.</p>
<p>We can see that the values match those calculated manually above, except for the sign on some values. We can expect there to be some instability when it comes to the sign given the nature of the calculations involved and the differences in the underlying libraries and methods used. This instability of sign should not be a problem in practice as long as the transform is trained for reuse.</p><pre class="crayon-plain-tag">[[ 1  2  3  4  5  6  7  8  9 10]
 [11 12 13 14 15 16 17 18 19 20]
 [21 22 23 24 25 26 27 28 29 30]]

[[ 18.52157747   6.47697214]
 [ 49.81310011   1.91182038]
 [ 81.10462276  -2.65333138]]</pre><p></p>
<h2>Extensions</h2>
<p>This section lists some ideas for extending the tutorial that you may wish to explore.</p>
<ul>
<li>Experiment with the SVD method on your own data.</li>
<li>Research and list 10 applications of SVD in machine learning.</li>
<li>Apply SVD as a data reduction technique on a tabular dataset.</li>
</ul>
<p>If you explore any of these extensions, I&#8217;d love to know.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Books</h3>
<ul>
<li>Chapter 12, Singular-Value and Jordan Decompositions, <a href="http://amzn.to/2A9ceNv">Linear Algebra and Matrix Analysis for Statistics</a>, 2014.</li>
<li>Chapter 4, The Singular Value Decomposition and Chapter 5, More on the SVD, <a href="http://amzn.to/2kjEF4S">Numerical Linear Algebra</a>, 1997.</li>
<li>Section 2.4 The Singular Value Decomposition, <a href="http://amzn.to/2B9xnLD">Matrix Computations</a>, 2012.</li>
<li>Chapter 7 The Singular Value Decomposition (SVD), <a href="http://amzn.to/2AZ7R8j">Introduction to Linear Algebra</a>, Fifth Edition, 2016.</li>
<li>Section 2.8 Singular Value Decomposition, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</li>
<li>Section 7.D Polar Decomposition and Singular Value Decomposition, <a href="http://amzn.to/2BGuEqI">Linear Algebra Done Right</a>, Third Edition, 2015.</li>
<li>Lecture 3 The Singular Value Decomposition, <a href="http://amzn.to/2BI9kRH">Numerical Linear Algebra</a>, 1997.</li>
<li>Section 2.6 Singular Value Decomposition, <a href="http://amzn.to/2BezVEE">Numerical Recipes: The Art of Scientific Computing</a>, Third Edition, 2007.</li>
<li>Section 2.9 The Moore-Penrose Pseudoinverse, <a href="http://amzn.to/2B3MsuU">Deep Learning</a>, 2016.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html">numpy.linalg.svd() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matrix.H.html">numpy.matrix.H API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.diag.html">numpy.diag() API</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.pinv.html">numpy.linalg.pinv() API</a>.</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">sklearn.decomposition.TruncatedSVD API</a></li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Matrix_decomposition">Matrix decomposition on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Singular-value_decomposition">Singular-value decomposition on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Singular_value">Singular value on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent semantic analysis on Wikipedia</a></li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered the Singular-value decomposition method for decomposing a matrix into its constituent elements.</p>
<p>Specifically, you learned:</p>
<ul>
<li>What Singular-value decomposition is and what is involved.</li>
<li>How to calculate an SVD and reconstruct a rectangular and square matrix from SVD elements.</li>
<li>How to calculate the pseudoinverse and perform dimensionality reduction using the SVD.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/">A Gentle Introduction to Singular-Value Decomposition for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		</item>
		<item>
		<title>Linear Algebra Cheat Sheet for Machine Learning</title>
		<link>https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning/</link>
		<comments>https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning/#comments</comments>
		<pubDate>Thu, 22 Feb 2018 18:00:09 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4831</guid>
		<description><![CDATA[<p>All of the Linear Algebra Operations that You Need to Use in NumPy for Machine Learning. The Python numerical computation library called NumPy provides many linear algebra functions that may be useful as a machine learning practitioner. In this tutorial, you will discover the key functions for working with vectors and matrices that you may [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning/">Linear Algebra Cheat Sheet for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<h3 style="text-align: center;">All of the Linear Algebra Operations that You Need to Use<br />
in NumPy for Machine Learning.</h3>
<p>The Python numerical computation library called NumPy provides many linear algebra functions that may be useful as a machine learning practitioner.</p>
<p>In this tutorial, you will discover the key functions for working with vectors and matrices that you may find useful as a machine learning practitioner.</p>
<p>This is a cheat sheet and all examples are short and assume you are familiar with the operation being performed.</p>
<p>You may want to bookmark this page for future reference.</p>
<div id="attachment_4837" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4837" src="https://machinelearningmastery.com/wp-content/uploads/2018/02/Linear-Algebra-Cheat-Sheet-for-Machine-Learning.jpg" alt="Linear Algebra Cheat Sheet for Machine Learning" width="640" height="425" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/Linear-Algebra-Cheat-Sheet-for-Machine-Learning.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/Linear-Algebra-Cheat-Sheet-for-Machine-Learning-300x199.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">Linear Algebra Cheat Sheet for Machine Learning<br />Photo by <a href="https://www.flickr.com/photos/bewegtbildgestalter/1274219020/">Christoph Landers</a>, some rights reserved.</p></div>
<h2>Overview</h2>
<p>This tutorial is divided into 7 parts; they are:</p>
<ol>
<li>Arrays</li>
<li>Vectors</li>
<li>Matrices</li>
<li>Types of Matrices</li>
<li>Matrix Operations</li>
<li>Matrix Factorization</li>
<li>Statistics</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>1. Arrays</h2>
<p>There are many ways to create NumPy arrays.</p>
<h3>Array</h3>
<p></p><pre class="crayon-plain-tag">from numpy import array
A = array([[1,2,3],[1,2,3],[1,2,3]])</pre><p></p>
<h3>Empty</h3>
<p></p><pre class="crayon-plain-tag">from numpy import empty
A = empty([3,3])</pre><p></p>
<h3>Zeros</h3>
<p></p><pre class="crayon-plain-tag">from numpy import zeros
A = zeros([3,5])</pre><p></p>
<h3>Ones</h3>
<p></p><pre class="crayon-plain-tag">from numpy import ones
A = ones([5, 5])</pre><p></p>
<h2>2. Vectors</h2>
<p>A vector is a list or column of scalars.</p>
<h3>Vector Addition</h3>
<p></p><pre class="crayon-plain-tag">c = a + b</pre><p></p>
<h3>Vector Subtraction</h3>
<p></p><pre class="crayon-plain-tag">c = a - b</pre><p></p>
<h3>Vector Multiplication</h3>
<p></p><pre class="crayon-plain-tag">c = a * b</pre><p></p>
<h3>Vector Division</h3>
<p></p><pre class="crayon-plain-tag">c = a / b</pre><p></p>
<h3>Vector Dot Product</h3>
<p></p><pre class="crayon-plain-tag">c = a.dot(b)</pre><p></p>
<h3>Vector-Scalar Multiplication</h3>
<p></p><pre class="crayon-plain-tag">c = a * 2.2</pre><p></p>
<h3>Vector Norm</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import norm
l2 = norm(v)</pre><p></p>
<h2>3. Matrices</h2>
<p>A matrix is a two-dimensional array of scalars.</p>
<h3>Matrix Addition</h3>
<p></p><pre class="crayon-plain-tag">C = A + B</pre><p></p>
<h3>Matrix Subtraction</h3>
<p></p><pre class="crayon-plain-tag">C = A - B</pre><p></p>
<h3>Matrix Multiplication (Hadamard Product)</h3>
<p></p><pre class="crayon-plain-tag">C = A * B</pre><p></p>
<h3>Matrix Division</h3>
<p></p><pre class="crayon-plain-tag">C = A / B</pre><p></p>
<h3>Matrix-Matrix Multiplication (Dot Product)</h3>
<p></p><pre class="crayon-plain-tag">C = A.dot(B)</pre><p></p>
<h3>Matrix-Vector Multiplication (Dot Product)</h3>
<p></p><pre class="crayon-plain-tag">C = A.dot(b)</pre><p></p>
<h3>Matrix-Scalar Multiplication</h3>
<p></p><pre class="crayon-plain-tag">C = A.dot(2.2)</pre><p></p>
<h2>4. Types of Matrices</h2>
<p>Different types of matrices are often used as elements in broader calculations.</p>
<h3>Triangle Matrix</h3>
<p></p><pre class="crayon-plain-tag"># lower
from numpy import tril
lower = tril(M)
# upper
from numpy import triu
upper = triu(M)</pre><p></p>
<h3>Diagonal Matrix</h3>
<p></p><pre class="crayon-plain-tag">from numpy import diag
d = diag(M)</pre><p></p>
<h3>Identity Matrix</h3>
<p></p><pre class="crayon-plain-tag">from numpy import identity
I = identity(3)</pre><p></p>
<h2>5. Matrix Operations</h2>
<p>Matrix operations are often used as elements in broader calculations.</p>
<h3>Matrix Transpose</h3>
<p></p><pre class="crayon-plain-tag">B = A.T</pre><p></p>
<h3>Matrix Inversion</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import inv
B = inv(A)</pre><p></p>
<h3>Matrix Trace</h3>
<p></p><pre class="crayon-plain-tag">from numpy import trace
B = trace(A)</pre><p></p>
<h3>Matrix Determinant</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import det
B = det(A)</pre><p></p>
<h3>Matrix Rank</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import matrix_rank
r = matrix_rank(A)</pre><p></p>
<h2>6. Matrix Factorization</h2>
<p>Matrix factorization, or matrix decomposition, breaks a matrix down into its constituent parts to make other operations simpler and more numerically stable.</p>
<h3>LU Decomposition</h3>
<p></p><pre class="crayon-plain-tag">from scipy.linalg import lu
P, L, U = lu(A)</pre><p></p>
<h3>QR Decomposition</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import qr
Q, R = qr(A, 'complete')</pre><p></p>
<h3>Eigendecomposition</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import eig
values, vectors = eig(A)</pre><p></p>
<h3>Singular-Value Decomposition</h3>
<p></p><pre class="crayon-plain-tag">from scipy.linalg import svd
U, s, V = svd(A)</pre><p></p>
<h2>7. Statistics</h2>
<p>Statistics summarize the contents of vectors or matrices and are often used as components in broader operations.</p>
<h3>Mean</h3>
<p></p><pre class="crayon-plain-tag">from numpy import mean
result = mean(v)</pre><p></p>
<h3>Variance</h3>
<p></p><pre class="crayon-plain-tag">from numpy import var
result = var(v, ddof=1)</pre><p></p>
<h3>Standard Deviation</h3>
<p></p><pre class="crayon-plain-tag">from numpy import std
result = std(v, ddof=1)</pre><p></p>
<h3>Covariance Matrix</h3>
<p></p><pre class="crayon-plain-tag">from numpy import cov
sigma = cov(v1, v2)</pre><p></p>
<h3>Linear Least Squares</h3>
<p></p><pre class="crayon-plain-tag">from numpy.linalg import lstsq
b = lstsq(X, y)</pre><p></p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>NumPy API</h3>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.linalg.html">Linear algebra</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.statistics.html">Statistics</a></li>
</ul>
<h3>Other Cheat Sheets</h3>
<ul>
<li><a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_SciPy_Cheat_Sheet_Linear_Algebra.pdf">Python For Data Science Cheat Sheet, DataCamp (PDF)</a></li>
<li><a href="https://minireference.com/static/tutorials/linear_algebra_in_4_pages.pdf">Linear algebra explained in four pages (PDF)</a></li>
<li><a href="https://github.com/scalanlp/breeze/wiki/Linear-Algebra-Cheat-Sheet">Linear Algebra Cheat Sheet</a></li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered the key functions for linear algebra that you may find useful as a machine learning practitioner.</p>
<p>Are there other key linear algebra functions that you use or know of?<br />
Let me know in the comments below.</p>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning/">Linear Algebra Cheat Sheet for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/linear-algebra-cheat-sheet-for-machine-learning/feed/</wfw:commentRss>
		<slash:comments>8</slash:comments>
		</item>
		<item>
		<title>Top Resources for Learning Linear Algebra for Machine Learning</title>
		<link>https://machinelearningmastery.com/resources-for-linear-algebra-in-machine-learning/</link>
		<comments>https://machinelearningmastery.com/resources-for-linear-algebra-in-machine-learning/#respond</comments>
		<pubDate>Tue, 20 Feb 2018 18:00:25 +0000</pubDate>
		<dc:creator><![CDATA[Jason Brownlee]]></dc:creator>
				<category><![CDATA[Linear Algebra]]></category>

		<guid isPermaLink="false">https://machinelearningmastery.com/?p=4809</guid>
		<description><![CDATA[<p>How to Get Help with Linear Algebra for Machine Learning? Linear algebra is a field of mathematics and an important pillar of the field of machine learning. It can be a challenging topic for beginners, or for practitioners who have not looked at the topic in decades. In this post, you will discover how to [&#8230;]</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/resources-for-linear-algebra-in-machine-learning/">Top Resources for Learning Linear Algebra for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></description>
				<content:encoded><![CDATA[<h3 style="text-align: center;">How to Get Help with Linear Algebra for Machine Learning?</h3>
<p>Linear algebra is a field of mathematics and an important pillar of the field of machine learning.</p>
<p>It can be a challenging topic for beginners, or for practitioners who have not looked at the topic in decades.</p>
<p>In this post, you will discover how to get help with linear algebra for machine learning.</p>
<p>After reading this post, you will know:</p>
<ul>
<li>Wikipedia articles and textbooks that you can refer to on linear algebra.</li>
<li>University courses and online courses that you can take to learn or review linear algebra.</li>
<li>Question-and-answer sites where you can post questions on linear algebra topics.</li>
</ul>
<p>Let&#8217;s get started.</p>
<div id="attachment_4830" style="max-width: 650px" class="wp-caption aligncenter"><img class="size-full wp-image-4830" src="https://machinelearningmastery.com/wp-content/uploads/2018/02/Top-Resources-for-Learning-Linear-Algebra-for-Machine-Learning.jpg" alt="Top Resources for Learning Linear Algebra for Machine Learning" width="640" height="361" srcset="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/Top-Resources-for-Learning-Linear-Algebra-for-Machine-Learning.jpg 640w, http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2018/02/Top-Resources-for-Learning-Linear-Algebra-for-Machine-Learning-300x169.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" /><p class="wp-caption-text">Top Resources for Learning Linear Algebra for Machine Learning<br />Photos by <a href="https://www.flickr.com/photos/mc-pictures/7870273950/">mickey</a>, some rights reserved.</p></div>
<h2>Overview</h2>
<p>This post is divided into 6 sections; they are:</p>
<ol>
<li>Linear Algebra on Wikipedia</li>
<li>Linear Algebra Textbooks</li>
<li>Linear Algebra University Courses</li>
<li>Linear Algebra Online Courses</li>
<li>Ask Questions About Linear Algebra</li>
<li>NumPy Resources</li>
</ol>
<!-- Start shortcoder --><p><div class="woo-sc-hr"></div></p>
<center>
<h3>Need help with Linear Algebra for Machine Learning?</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;">Download Your FREE Mini-Course</a><script data-leadbox="143e9e9f3f72a2:164f8be4f346dc" data-url="https://machinelearningmastery.lpages.co/leadbox/143e9e9f3f72a2%3A164f8be4f346dc/5667039158992896/" data-config="%7B%7D" type="text/javascript" src="https://machinelearningmastery.lpages.co/leadbox-1515526651.js"></script></p>
</center>
<p><div class="woo-sc-hr"></div></p><!-- End shortcoder v4.1.6-->
<h2>Linear Algebra on Wikipedia</h2>
<p>Wikipedia is a great place to start.</p>
<p>All of the important topics are covered, the descriptions are concise, and the equations are consistent and readable. What is missing is the more human level descriptions such as analogies and intuitions.</p>
<p>Nevertheless, when you have questions about linear algebra, I recommend stopping by Wikipedia first.</p>
<p>Some good high-level pages to start on include:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Linear_algebra">Linear Algebra</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">Matrix (mathematics)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_decomposition">Matrix decomposition</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_linear_algebra_topics">List of linear algebra topics</a></li>
</ul>
<h2>Linear Algebra Textbooks</h2>
<p>I strongly recommend getting a good textbook on the topic of linear algebra and using it as a reference.</p>
<p>The benefit of a good textbook is that the explanations of the various operations you require will be consistent (or should be). The downside of textbooks is that they can be very expensive.</p>
<p>A good textbook is often easy to spot because it will be the basis for a range of undergraduate or postgraduate courses at top universities.</p>
<p>Some introductory textbooks on linear algebra I recommend include:</p>
<a class="easyazon-link"  href="http://www.amazon.com/dp/0980232775?tag=inspiredalgor-20"><img src="https://images-na.ssl-images-amazon.com/images/I/41Qxd6fnlyL.jpg" class="aligncenter" alt="Amazon Image" height="500" width="403"  /></a>
<ul>
<li><a href="http://amzn.to/2j2J0g4">Introduction to Linear Algebra</a>, Fifth Edition, Gilbert Strang, 2016.</li>
<li><a href="http://amzn.to/2BGuEqI">Linear Algebra Done Right</a>, Third Edition, 2015.</li>
<li><a href="http://amzn.to/2k76D4C">No Bullshit Guide To Linear Algebra</a>, Ivan Savov, 2017.</li>
</ul>
<p>Some more advanced textbooks I recommend include:</p>
<a class="easyazon-link"  href="http://www.amazon.com/dp/1421407949?tag=inspiredalgor-20"><img src="https://images-na.ssl-images-amazon.com/images/I/41f5vxegABL.jpg" class="aligncenter" alt="Amazon Image" height="500" width="365"  /></a>
<ul>
<li><a href="http://amzn.to/2B9xnLD">Matrix Computations</a>, Gene Golub and Charles Van Loan, 2012.</li>
<li><a href="http://amzn.to/2kjEF4S">Numerical Linear Algebra</a>, Lloyd Trefethen and David Bau 1997.</li>
</ul>
<p>I&#8217;d also recommend a good textbook on multivariate statistics, which is the intersection of linear algebra, and numerical statistical methods. Some good introductory textbooks include:</p>
<a class="easyazon-link"  href="http://www.amazon.com/dp/8120345878?tag=inspiredalgor-20"><img src="https://images-na.ssl-images-amazon.com/images/I/41SBsAIa9fL.jpg" class="aligncenter" alt="Amazon Image" height="500" width="378"  /></a>
<ul>
<li><a href="http://amzn.to/2AUcEc5">Applied Multivariate Statistical Analysis</a>, Richard Johnson and Dean Wichern, 2012.</li>
<li><a href="http://amzn.to/2AWIViz">Applied Multivariate Statistical Analysis</a>, Wolfgang Karl Hardle and Leopold Simar, 2015.</li>
</ul>
<p>There are also many good free online books written by academics. See the end of the <a href="https://en.wikipedia.org/wiki/Linear_algebra#Further_reading">Linear Algebra page on Wikipedia</a> for an extensive (and impressive) reading list.</p>
<h2>Linear Algebra University Courses</h2>
<p>University courses on linear algebra are useful in that they layout the topics that an undergraduate student is expected to know.</p>
<p>As a machine learning practitioner, it is more than you need, but does provide context for the elements that you do need to know.</p>
<p>Many university courses now provide PDF versions of lecture slides, notes, and readings. Some even provide pre-recorded video lectures, which can be invaluable.</p>
<p>I would encourage you to use university course material surgically by dipping into courses to get deeper knowledge on specific topics. I think working through a given course end-to-end is too time consuming and covers too much for the average machine learning practitioner.</p>
<p>Some recommended courses from top US schools include:</p>
<ul>
<li><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm">Linear Algebra</a> at MIT by Gilbert Strang.</li>
<li><a href="http://cs.brown.edu/courses/cs053/current/index.htm">The Matrix in Computer Science</a> at Brown by Philip Klein.</li>
<li><a href="https://github.com/fastai/numerical-linear-algebra/">Computational Linear Algebra</a> for Coders at University of San Francisco by Rachel Thomas.</li>
</ul>
<h2>Linear Algebra Online Courses</h2>
<p>Online courses are different from university courses.</p>
<p>They are designed for distance education and often are less complete or less rigorous than a full undergraduate course. This is a good feature for machine learning practitioners looking to get up to speed fast on the topic.</p>
<p>If the course is short, it may be worth taking it through end-to-end. Generally, and like university courses, I would recommend being surgical with the topics and dip in as needed.</p>
<p>Some online courses I recommend include:</p>
<ul>
<li><a href="https://www.khanacademy.org/math/linear-algebra">Linear Algebra</a> on Khan Academy</li>
<li><a href="https://www.edx.org/course/laff-linear-algebra-foundations-to-frontiers">Linear Algebra: Foundations to Frontiers</a> on edX</li>
</ul>
<h2>Ask Questions About Linear Algebra</h2>
<p>There are a lot of places that you can ask questions about linear algebra online given the current abundance of question-and-answer platforms.</p>
<p>Below is a list of the top places I recommend posting a question. Remember to search for your question before posting in case it has been asked and answered before.</p>
<ul>
<li><a href="https://math.stackexchange.com/?tags=linear-algebra">Linear Algebra tag on the Mathematics Stack Exchange</a></li>
<li><a href="https://stats.stackexchange.com/questions/tagged/linear-algebra">Linear Algebra tag on Cross Validated</a></li>
<li><a href="https://stackoverflow.com/questions/tagged/linear-algebra">Linear Algebra tag on Stack Overflow</a></li>
<li><a href="https://www.quora.com/topic/Linear-Algebra">Linear Algebra on Quora</a></li>
<li><a href="https://www.reddit.com/r/math/">Math Subreddit</a></li>
</ul>
<h2>NumPy Resources</h2>
<p>You may need help with NumPy when implementing your linear algebra in Python.</p>
<p>The NumPy API documentation is excellent, below are a few resources that you can use to learn more about how NumPy works or how to use specific NumPy functions.</p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/">NumPy Reference</a></li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/routines.array-creation.html">NumPy Array Creation Routines</a></li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html">NumPy Array Manipulation Routines</a></li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/routines.linalg.html">NumPy Linear Algebra</a></li>
<li><a href="https://docs.scipy.org/doc/scipy/reference/linalg.html">SciPy Linear Algebra</a></li>
</ul>
<p>If you are looking for a broader understanding on NumPy and SciPy usage, the below books provide a good starting reference:</p>
<ul>
<li><a href="http://amzn.to/2B1sfXi">Python for Data Analysis</a>, 2017.</li>
<li><a href="http://amzn.to/2yujXnT">Elegant SciPy</a>, 2017.</li>
<li><a href="http://amzn.to/2j3kEzd">Guide to NumPy</a>, 2015.</li>
</ul>
<h2>Summary</h2>
<p>In this post, you discovered how to get help with linear algebra for machine learning.</p>
<p>Specifically, you learned about:</p>
<ul>
<li>Wikipedia articles and textbooks that you can refer to on linear algebra.</li>
<li>University courses and online courses that you can take to learn or review linear algebra.</li>
<li>Question-and-answer sites where you can post questions on linear algebra topics.</li>
</ul>
<p>Do you have any questions?<br />
Ask your questions in the comments below and I will do my best to answer.</p>
<p>The post <a rel="nofollow" href="https://machinelearningmastery.com/resources-for-linear-algebra-in-machine-learning/">Top Resources for Learning Linear Algebra for Machine Learning</a> appeared first on <a rel="nofollow" href="https://machinelearningmastery.com">Machine Learning Mastery</a>.</p>
]]></content:encoded>
			<wfw:commentRss>https://machinelearningmastery.com/resources-for-linear-algebra-in-machine-learning/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
