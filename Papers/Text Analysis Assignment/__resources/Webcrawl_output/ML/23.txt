Navigation

	

		

Machine Learning Mastery
Making developers awesome at machine learning

	    
	        			

Start Here
     
Blog
     
Books
     
About
     
Contact

		
    
        
        
    
    
			Need help with Deep Learning? Take the FREE Mini-Course

			    
	
	
	

	
	

		Home

	Empty Menu	
		

	

	Return to Content


       
    
	    
    
    	    

            
                                               

	
	How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras	
By Jason Brownlee on August 9, 2016  in Deep Learning  
	



				
					
						
							
							Share on TwitterTweet
						
											
				
								
					
						
							
							Share on Facebook
							Share
						
											
				
				
				
					
						
							Share on LinkedIn
							Share
						
											
				
								
					
						
							
							Share on Google Plus
							Share
						
											
				
				Hyperparameter optimization is a big part of deep learning.
The reason is that neural networks are notoriously difficult to configure and there are a lot of parameters that need to be set. On top of that, individual models can be very slow to train.
In this post you will discover how you can use the grid search capability from the scikit-learn python machine learning library to tune the hyperparameters of Keras deep learning models.
After reading this post you will know:
How to wrap Keras models for use in scikit-learn and how to use grid search.How to grid search common neural network parameters such as learning rate, dropout rate, epochs and number of neurons.How to define your own hyperparameter tuning experiments on your own projects.
Let’s get started.
Update Nov/2016: Fixed minor issue in displaying grid search results in code examples.Update Oct/2016: Updated examples for Keras 1.1.0, TensorFlow 0.10.0 and scikit-learn v0.18.Update Mar/2017: Updated example for Keras 2.0.2, TensorFlow 1.0.1 and Theano 0.9.0.Update Sept/2017: Updated example to use Keras 2 “epochs” instead of Keras 1 “nb_epochs”.Update March/2018: Added alternate link to download the dataset as the original appears to have been taken down.
How to Grid Search Hyperparameters for Deep Learning Models in Python With KerasPhoto by 3V Photo, some rights reserved.
Overview
In this post, I want to show you both how you can use the scikit-learn grid search capability and give you a suite of examples that you can copy-and-paste into your own project as a starting point.
Below is a list of the topics we are going to cover:
How to use Keras models in scikit-learn.How to use grid search in scikit-learn.How to tune batch size and training epochs.How to tune optimization algorithms.How to tune learning rate and momentum.How to tune network weight initialization.How to tune activation functions.How to tune dropout regularization.How to tune the number of neurons in the hidden layer.
How to Use Keras Models in scikit-learn
Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.
To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the build_fn argument when constructing the KerasClassifier class.
For example:

		
		
			
			
			
			
def create_model():
	...
	return model

model = KerasClassifier(build_fn=create_model)
			
				
					12345
				def create_model():	...	return model model = KerasClassifier(build_fn=create_model)
			
		

The constructor for the KerasClassifier class can take default arguments that are passed on to the calls to model.fit(), such as the number of epochs and the batch size.
For example:

		
		
			
			
			
			
def create_model():
	...
	return model

model = KerasClassifier(build_fn=create_model, epochs=10)
			
				
					12345
				def create_model():	...	return model model = KerasClassifier(build_fn=create_model, epochs=10)
			
		

The constructor for the KerasClassifier class can also take new arguments that can be passed to your custom create_model() function. These new arguments must also be defined in the signature of your create_model() function with default parameters.
For example:

		
		
			
			
			
			
def create_model(dropout_rate=0.0):
	...
	return model

model = KerasClassifier(build_fn=create_model, dropout_rate=0.2)
			
				
					12345
				def create_model(dropout_rate=0.0):	...	return model model = KerasClassifier(build_fn=create_model, dropout_rate=0.2)
			
		

You can learn more about the scikit-learn wrapper in Keras API documentation.
How to Use Grid Search in scikit-learn
Grid search is a model hyperparameter optimization technique.
In scikit-learn this technique is provided in the GridSearchCV class.
When constructing this class you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.
By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.
By default, the grid search will only use one thread. By setting the n_jobs argument in the GridSearchCV constructor to -1, the process will use all cores on your machine. Depending on your Keras backend, this may interfere with the main neural network training process.
The GridSearchCV process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor.
Below is an example of defining a simple grid search:

		
		
			
			
			
			
param_grid = dict(epochs=[10,20,30])
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
			
				
					123
				param_grid = dict(epochs=[10,20,30])grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)
			
		

Once completed, you can access the outcome of the grid search in the result object returned from grid.fit(). The best_score_ member provides access to the best score observed during the optimization procedure and the best_params_ describes the combination of parameters that achieved the best results.
You can learn more about the GridSearchCV class in the scikit-learn API documentation.
Problem Description
Now that we know how to use Keras models with scikit-learn and how to use grid search in scikit-learn, let’s look at a bunch of examples.
All examples will be demonstrated on a small standard machine learning dataset called the Pima Indians onset of diabetes classification dataset. This is a small dataset with all numerical attributes that is easy to work with.
Download the dataset and place it in your currently working directly with the name pima-indians-diabetes.csv (update: download from here).
As we proceed through the examples in this post, we will aggregate the best parameters. This is not the best way to grid search because parameters can interact, but it is good for demonstration purposes.
Note on Parallelizing Grid Search
All examples are configured to use parallelism (n_jobs=-1).
If you get an error like the one below:

		
		
			
			
			
			
INFO (theano.gof.compilelock): Waiting for existing lock by process '55614' (I am process '55613')
INFO (theano.gof.compilelock): To manually release the lock, delete ...
			
				
					12
				INFO (theano.gof.compilelock): Waiting for existing lock by process '55614' (I am process '55613')INFO (theano.gof.compilelock): To manually release the lock, delete ...
			
		

Kill the process and change the code to not perform the grid search in parallel, set n_jobs=1.


Need help with Deep Learning in Python?
Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).
Click to sign-up now and also get a free PDF Ebook version of the course.
Start Your FREE Mini-Course Now! 


How to Tune Batch Size and Number of Epochs
In this first simple example, we look at tuning the batch size and number of epochs used when fitting the network.
The batch size in iterative gradient descent is the number of patterns shown to the network before the weights are updated. It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.
The number of epochs is the number of times that the entire training dataset is shown to the network during training. Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks.
Here we will evaluate a suite of different mini batch sizes from 10 to 100 in steps of 20.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the batch size and epochs
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model():
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, verbose=0)
# define the grid search parameters
batch_size = [10, 20, 40, 60, 80, 100]
epochs = [10, 50, 100]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					1234567891011121314151617181920212223242526272829303132333435363738
				# Use scikit-learn to grid search the batch size and epochsimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifier# Function to create model, required for KerasClassifierdef create_model():	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, activation='relu'))	model.add(Dense(1, activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, verbose=0)# define the grid search parametersbatch_size = [10, 20, 40, 60, 80, 100]epochs = [10, 50, 100]param_grid = dict(batch_size=batch_size, epochs=epochs)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.686198 using {'epochs': 100, 'batch_size': 20}
0.348958 (0.024774) with: {'epochs': 10, 'batch_size': 10}
0.348958 (0.024774) with: {'epochs': 50, 'batch_size': 10}
0.466146 (0.149269) with: {'epochs': 100, 'batch_size': 10}
0.647135 (0.021236) with: {'epochs': 10, 'batch_size': 20}
0.660156 (0.014616) with: {'epochs': 50, 'batch_size': 20}
0.686198 (0.024774) with: {'epochs': 100, 'batch_size': 20}
0.489583 (0.075566) with: {'epochs': 10, 'batch_size': 40}
0.652344 (0.019918) with: {'epochs': 50, 'batch_size': 40}
0.654948 (0.027866) with: {'epochs': 100, 'batch_size': 40}
0.518229 (0.032264) with: {'epochs': 10, 'batch_size': 60}
0.605469 (0.052213) with: {'epochs': 50, 'batch_size': 60}
0.665365 (0.004872) with: {'epochs': 100, 'batch_size': 60}
0.537760 (0.143537) with: {'epochs': 10, 'batch_size': 80}
0.591146 (0.094954) with: {'epochs': 50, 'batch_size': 80}
0.658854 (0.054904) with: {'epochs': 100, 'batch_size': 80}
0.402344 (0.107735) with: {'epochs': 10, 'batch_size': 100}
0.652344 (0.033299) with: {'epochs': 50, 'batch_size': 100}
0.542969 (0.157934) with: {'epochs': 100, 'batch_size': 100}
			
				
					12345678910111213141516171819
				Best: 0.686198 using {'epochs': 100, 'batch_size': 20}0.348958 (0.024774) with: {'epochs': 10, 'batch_size': 10}0.348958 (0.024774) with: {'epochs': 50, 'batch_size': 10}0.466146 (0.149269) with: {'epochs': 100, 'batch_size': 10}0.647135 (0.021236) with: {'epochs': 10, 'batch_size': 20}0.660156 (0.014616) with: {'epochs': 50, 'batch_size': 20}0.686198 (0.024774) with: {'epochs': 100, 'batch_size': 20}0.489583 (0.075566) with: {'epochs': 10, 'batch_size': 40}0.652344 (0.019918) with: {'epochs': 50, 'batch_size': 40}0.654948 (0.027866) with: {'epochs': 100, 'batch_size': 40}0.518229 (0.032264) with: {'epochs': 10, 'batch_size': 60}0.605469 (0.052213) with: {'epochs': 50, 'batch_size': 60}0.665365 (0.004872) with: {'epochs': 100, 'batch_size': 60}0.537760 (0.143537) with: {'epochs': 10, 'batch_size': 80}0.591146 (0.094954) with: {'epochs': 50, 'batch_size': 80}0.658854 (0.054904) with: {'epochs': 100, 'batch_size': 80}0.402344 (0.107735) with: {'epochs': 10, 'batch_size': 100}0.652344 (0.033299) with: {'epochs': 50, 'batch_size': 100}0.542969 (0.157934) with: {'epochs': 100, 'batch_size': 100}
			
		

We can see that the batch size of 20 and 100 epochs achieved the best result of about 68% accuracy.
How to Tune the Training Optimization Algorithm
Keras offers a suite of different state-of-the-art optimization algorithms.
In this example, we tune the optimization algorithm used to train the network, each with default parameters.
This is an odd example, because often you will choose one approach a priori and instead focus on tuning its parameters on your problem (e.g. see the next example).
Here we will evaluate the suite of optimization algorithms supported by the Keras API.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the batch size and epochs
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(optimizer='adam'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
param_grid = dict(optimizer=optimizer)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					12345678910111213141516171819202122232425262728293031323334353637
				# Use scikit-learn to grid search the batch size and epochsimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifier# Function to create model, required for KerasClassifierdef create_model(optimizer='adam'):	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, activation='relu'))	model.add(Dense(1, activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parametersoptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']param_grid = dict(optimizer=optimizer)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.704427 using {'optimizer': 'Adam'}
0.348958 (0.024774) with: {'optimizer': 'SGD'}
0.348958 (0.024774) with: {'optimizer': 'RMSprop'}
0.471354 (0.156586) with: {'optimizer': 'Adagrad'}
0.669271 (0.029635) with: {'optimizer': 'Adadelta'}
0.704427 (0.031466) with: {'optimizer': 'Adam'}
0.682292 (0.016367) with: {'optimizer': 'Adamax'}
0.703125 (0.003189) with: {'optimizer': 'Nadam'}
			
				
					12345678
				Best: 0.704427 using {'optimizer': 'Adam'}0.348958 (0.024774) with: {'optimizer': 'SGD'}0.348958 (0.024774) with: {'optimizer': 'RMSprop'}0.471354 (0.156586) with: {'optimizer': 'Adagrad'}0.669271 (0.029635) with: {'optimizer': 'Adadelta'}0.704427 (0.031466) with: {'optimizer': 'Adam'}0.682292 (0.016367) with: {'optimizer': 'Adamax'}0.703125 (0.003189) with: {'optimizer': 'Nadam'}
			
		

The results suggest that the ADAM optimization algorithm is the best with a score of about 70% accuracy.
How to Tune Learning Rate and Momentum
It is common to pre-select an optimization algorithm to train your network and tune its parameters.
By far the most common optimization algorithm is plain old Stochastic Gradient Descent (SGD) because it is so well understood. In this example, we will look at optimizing the SGD learning rate and momentum parameters.
Learning rate controls how much to update the weight at the end of each batch and the momentum controls how much to let the previous update influence the current weight update.
We will try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice).
Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size) and the number of epochs.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the learning rate and momentum
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import SGD
# Function to create model, required for KerasClassifier
def create_model(learn_rate=0.01, momentum=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	optimizer = SGD(lr=learn_rate, momentum=momentum)
	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]
param_grid = dict(learn_rate=learn_rate, momentum=momentum)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					12345678910111213141516171819202122232425262728293031323334353637383940
				# Use scikit-learn to grid search the learning rate and momentumimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.optimizers import SGD# Function to create model, required for KerasClassifierdef create_model(learn_rate=0.01, momentum=0):	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, activation='relu'))	model.add(Dense(1, activation='sigmoid'))	# Compile model	optimizer = SGD(lr=learn_rate, momentum=momentum)	model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parameterslearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]param_grid = dict(learn_rate=learn_rate, momentum=momentum)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.680990 using {'learn_rate': 0.01, 'momentum': 0.0}
0.348958 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.0}
0.348958 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.2}
0.467448 (0.151098) with: {'learn_rate': 0.001, 'momentum': 0.4}
0.662760 (0.012075) with: {'learn_rate': 0.001, 'momentum': 0.6}
0.669271 (0.030647) with: {'learn_rate': 0.001, 'momentum': 0.8}
0.666667 (0.035564) with: {'learn_rate': 0.001, 'momentum': 0.9}
0.680990 (0.024360) with: {'learn_rate': 0.01, 'momentum': 0.0}
0.677083 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.2}
0.427083 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.4}
0.427083 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.6}
0.544271 (0.146518) with: {'learn_rate': 0.01, 'momentum': 0.8}
0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}
0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}
0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.2}
0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.4}
0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.6}
0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.8}
0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}
0.533854 (0.149269) with: {'learn_rate': 0.2, 'momentum': 0.0}
0.427083 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.2}
0.427083 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.4}
0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}
0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}
0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}
0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.0}
0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.2}
0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.4}
0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}
0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}
0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.9}
			
				
					12345678910111213141516171819202122232425262728293031
				Best: 0.680990 using {'learn_rate': 0.01, 'momentum': 0.0}0.348958 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.0}0.348958 (0.024774) with: {'learn_rate': 0.001, 'momentum': 0.2}0.467448 (0.151098) with: {'learn_rate': 0.001, 'momentum': 0.4}0.662760 (0.012075) with: {'learn_rate': 0.001, 'momentum': 0.6}0.669271 (0.030647) with: {'learn_rate': 0.001, 'momentum': 0.8}0.666667 (0.035564) with: {'learn_rate': 0.001, 'momentum': 0.9}0.680990 (0.024360) with: {'learn_rate': 0.01, 'momentum': 0.0}0.677083 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.2}0.427083 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.4}0.427083 (0.134575) with: {'learn_rate': 0.01, 'momentum': 0.6}0.544271 (0.146518) with: {'learn_rate': 0.01, 'momentum': 0.8}0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.2}0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.4}0.572917 (0.134575) with: {'learn_rate': 0.1, 'momentum': 0.6}0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.8}0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}0.533854 (0.149269) with: {'learn_rate': 0.2, 'momentum': 0.0}0.427083 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.2}0.427083 (0.134575) with: {'learn_rate': 0.2, 'momentum': 0.4}0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.0}0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.2}0.455729 (0.146518) with: {'learn_rate': 0.3, 'momentum': 0.4}0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}0.348958 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.9}
			
		

We can see that relatively SGD is not very good on this problem, nevertheless best results were achieved using a learning rate of 0.01 and a momentum of 0.0 with an accuracy of about 68%.
How to Tune Network Weight Initialization
Neural network weight initialization used to be simple: use small random values.
Now there is a suite of different techniques to choose from. Keras provides a laundry list.
In this example, we will look at tuning the selection of network weight initialization by evaluating all of the available techniques.
We will use the same weight initialization method on each layer. Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. In the example below we use rectifier for the hidden layer. We use sigmoid for the output layer because the predictions are binary.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the weight initialization
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(init_mode='uniform'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))
	model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
param_grid = dict(init_mode=init_mode)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					12345678910111213141516171819202122232425262728293031323334353637
				# Use scikit-learn to grid search the weight initializationimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifier# Function to create model, required for KerasClassifierdef create_model(init_mode='uniform'):	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))	model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parametersinit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']param_grid = dict(init_mode=init_mode)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.720052 using {'init_mode': 'uniform'}
0.720052 (0.024360) with: {'init_mode': 'uniform'}
0.348958 (0.024774) with: {'init_mode': 'lecun_uniform'}
0.712240 (0.012075) with: {'init_mode': 'normal'}
0.651042 (0.024774) with: {'init_mode': 'zero'}
0.700521 (0.010253) with: {'init_mode': 'glorot_normal'}
0.674479 (0.011201) with: {'init_mode': 'glorot_uniform'}
0.661458 (0.028940) with: {'init_mode': 'he_normal'}
0.678385 (0.004872) with: {'init_mode': 'he_uniform'}
			
				
					123456789
				Best: 0.720052 using {'init_mode': 'uniform'}0.720052 (0.024360) with: {'init_mode': 'uniform'}0.348958 (0.024774) with: {'init_mode': 'lecun_uniform'}0.712240 (0.012075) with: {'init_mode': 'normal'}0.651042 (0.024774) with: {'init_mode': 'zero'}0.700521 (0.010253) with: {'init_mode': 'glorot_normal'}0.674479 (0.011201) with: {'init_mode': 'glorot_uniform'}0.661458 (0.028940) with: {'init_mode': 'he_normal'}0.678385 (0.004872) with: {'init_mode': 'he_uniform'}
			
		

We can see that the best results were achieved with a uniform weight initialization scheme achieving a performance of about 72%.
How to Tune the Neuron Activation Function
The activation function controls the non-linearity of individual neurons and when to fire.
Generally, the rectifier activation function is the most popular, but it used to be the sigmoid and the tanh functions and these functions may still be more suitable for different problems.
In this example, we will evaluate the suite of different activation functions available in Keras. We will only use these functions in the hidden layer, as we require a sigmoid activation function in the output for the binary classification problem.
Generally, it is a good idea to prepare data to the range of the different transfer functions, which we will not do in this case.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the activation function
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model(activation='relu'):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']
param_grid = dict(activation=activation)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					12345678910111213141516171819202122232425262728293031323334353637
				# Use scikit-learn to grid search the activation functionimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifier# Function to create model, required for KerasClassifierdef create_model(activation='relu'):	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parametersactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']param_grid = dict(activation=activation)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.722656 using {'activation': 'linear'}
0.649740 (0.009744) with: {'activation': 'softmax'}
0.720052 (0.032106) with: {'activation': 'softplus'}
0.688802 (0.019225) with: {'activation': 'softsign'}
0.720052 (0.018136) with: {'activation': 'relu'}
0.691406 (0.019401) with: {'activation': 'tanh'}
0.680990 (0.009207) with: {'activation': 'sigmoid'}
0.691406 (0.014616) with: {'activation': 'hard_sigmoid'}
0.722656 (0.003189) with: {'activation': 'linear'}
			
				
					123456789
				Best: 0.722656 using {'activation': 'linear'}0.649740 (0.009744) with: {'activation': 'softmax'}0.720052 (0.032106) with: {'activation': 'softplus'}0.688802 (0.019225) with: {'activation': 'softsign'}0.720052 (0.018136) with: {'activation': 'relu'}0.691406 (0.019401) with: {'activation': 'tanh'}0.680990 (0.009207) with: {'activation': 'sigmoid'}0.691406 (0.014616) with: {'activation': 'hard_sigmoid'}0.722656 (0.003189) with: {'activation': 'linear'}
			
		

Surprisingly (to me at least), the ‘linear’ activation function achieved the best results with an accuracy of about 72%.
How to Tune Dropout Regularization
In this example, we will look at tuning the dropout rate for regularization in an effort to limit overfitting and improve the model’s ability to generalize.
To get good results, dropout is best combined with a weight constraint such as the max norm constraint.
For more on using dropout in deep learning models with Keras see the post:
Dropout Regularization in Deep Learning Models With Keras
This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between 0.0 and 0.9 (1.0 does not make sense) and maxnorm weight constraint values between 0 and 5.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the dropout rate
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(dropout_rate=0.0, weight_constraint=0):
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))
	model.add(Dropout(dropout_rate))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
weight_constraint = [1, 2, 3, 4, 5]
dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					1234567891011121314151617181920212223242526272829303132333435363738394041
				# Use scikit-learn to grid search the dropout rateimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import Dropoutfrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.constraints import maxnorm# Function to create model, required for KerasClassifierdef create_model(dropout_rate=0.0, weight_constraint=0):	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))	model.add(Dropout(dropout_rate))	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parametersweight_constraint = [1, 2, 3, 4, 5]dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.723958 using {'dropout_rate': 0.2, 'weight_constraint': 4}
0.696615 (0.031948) with: {'dropout_rate': 0.0, 'weight_constraint': 1}
0.696615 (0.031948) with: {'dropout_rate': 0.0, 'weight_constraint': 2}
0.691406 (0.026107) with: {'dropout_rate': 0.0, 'weight_constraint': 3}
0.708333 (0.009744) with: {'dropout_rate': 0.0, 'weight_constraint': 4}
0.708333 (0.009744) with: {'dropout_rate': 0.0, 'weight_constraint': 5}
0.710937 (0.008438) with: {'dropout_rate': 0.1, 'weight_constraint': 1}
0.709635 (0.007366) with: {'dropout_rate': 0.1, 'weight_constraint': 2}
0.709635 (0.007366) with: {'dropout_rate': 0.1, 'weight_constraint': 3}
0.695312 (0.012758) with: {'dropout_rate': 0.1, 'weight_constraint': 4}
0.695312 (0.012758) with: {'dropout_rate': 0.1, 'weight_constraint': 5}
0.701823 (0.017566) with: {'dropout_rate': 0.2, 'weight_constraint': 1}
0.710938 (0.009568) with: {'dropout_rate': 0.2, 'weight_constraint': 2}
0.710938 (0.009568) with: {'dropout_rate': 0.2, 'weight_constraint': 3}
0.723958 (0.027126) with: {'dropout_rate': 0.2, 'weight_constraint': 4}
0.718750 (0.030425) with: {'dropout_rate': 0.2, 'weight_constraint': 5}
0.721354 (0.032734) with: {'dropout_rate': 0.3, 'weight_constraint': 1}
0.707031 (0.036782) with: {'dropout_rate': 0.3, 'weight_constraint': 2}
0.707031 (0.036782) with: {'dropout_rate': 0.3, 'weight_constraint': 3}
0.694010 (0.019225) with: {'dropout_rate': 0.3, 'weight_constraint': 4}
0.709635 (0.006639) with: {'dropout_rate': 0.3, 'weight_constraint': 5}
0.704427 (0.008027) with: {'dropout_rate': 0.4, 'weight_constraint': 1}
0.717448 (0.031304) with: {'dropout_rate': 0.4, 'weight_constraint': 2}
0.718750 (0.030425) with: {'dropout_rate': 0.4, 'weight_constraint': 3}
0.718750 (0.030425) with: {'dropout_rate': 0.4, 'weight_constraint': 4}
0.722656 (0.029232) with: {'dropout_rate': 0.4, 'weight_constraint': 5}
0.720052 (0.028940) with: {'dropout_rate': 0.5, 'weight_constraint': 1}
0.703125 (0.009568) with: {'dropout_rate': 0.5, 'weight_constraint': 2}
0.716146 (0.029635) with: {'dropout_rate': 0.5, 'weight_constraint': 3}
0.709635 (0.008027) with: {'dropout_rate': 0.5, 'weight_constraint': 4}
0.703125 (0.011500) with: {'dropout_rate': 0.5, 'weight_constraint': 5}
0.707031 (0.017758) with: {'dropout_rate': 0.6, 'weight_constraint': 1}
0.701823 (0.018688) with: {'dropout_rate': 0.6, 'weight_constraint': 2}
0.701823 (0.018688) with: {'dropout_rate': 0.6, 'weight_constraint': 3}
0.690104 (0.027498) with: {'dropout_rate': 0.6, 'weight_constraint': 4}
0.695313 (0.022326) with: {'dropout_rate': 0.6, 'weight_constraint': 5}
0.697917 (0.014382) with: {'dropout_rate': 0.7, 'weight_constraint': 1}
0.697917 (0.014382) with: {'dropout_rate': 0.7, 'weight_constraint': 2}
0.687500 (0.008438) with: {'dropout_rate': 0.7, 'weight_constraint': 3}
0.704427 (0.011201) with: {'dropout_rate': 0.7, 'weight_constraint': 4}
0.696615 (0.016367) with: {'dropout_rate': 0.7, 'weight_constraint': 5}
0.680990 (0.025780) with: {'dropout_rate': 0.8, 'weight_constraint': 1}
0.699219 (0.019401) with: {'dropout_rate': 0.8, 'weight_constraint': 2}
0.701823 (0.015733) with: {'dropout_rate': 0.8, 'weight_constraint': 3}
0.684896 (0.023510) with: {'dropout_rate': 0.8, 'weight_constraint': 4}
0.696615 (0.017566) with: {'dropout_rate': 0.8, 'weight_constraint': 5}
0.653646 (0.034104) with: {'dropout_rate': 0.9, 'weight_constraint': 1}
0.677083 (0.012075) with: {'dropout_rate': 0.9, 'weight_constraint': 2}
0.679688 (0.013902) with: {'dropout_rate': 0.9, 'weight_constraint': 3}
0.669271 (0.017566) with: {'dropout_rate': 0.9, 'weight_constraint': 4}
0.669271 (0.012075) with: {'dropout_rate': 0.9, 'weight_constraint': 5}
			
				
					123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051
				Best: 0.723958 using {'dropout_rate': 0.2, 'weight_constraint': 4}0.696615 (0.031948) with: {'dropout_rate': 0.0, 'weight_constraint': 1}0.696615 (0.031948) with: {'dropout_rate': 0.0, 'weight_constraint': 2}0.691406 (0.026107) with: {'dropout_rate': 0.0, 'weight_constraint': 3}0.708333 (0.009744) with: {'dropout_rate': 0.0, 'weight_constraint': 4}0.708333 (0.009744) with: {'dropout_rate': 0.0, 'weight_constraint': 5}0.710937 (0.008438) with: {'dropout_rate': 0.1, 'weight_constraint': 1}0.709635 (0.007366) with: {'dropout_rate': 0.1, 'weight_constraint': 2}0.709635 (0.007366) with: {'dropout_rate': 0.1, 'weight_constraint': 3}0.695312 (0.012758) with: {'dropout_rate': 0.1, 'weight_constraint': 4}0.695312 (0.012758) with: {'dropout_rate': 0.1, 'weight_constraint': 5}0.701823 (0.017566) with: {'dropout_rate': 0.2, 'weight_constraint': 1}0.710938 (0.009568) with: {'dropout_rate': 0.2, 'weight_constraint': 2}0.710938 (0.009568) with: {'dropout_rate': 0.2, 'weight_constraint': 3}0.723958 (0.027126) with: {'dropout_rate': 0.2, 'weight_constraint': 4}0.718750 (0.030425) with: {'dropout_rate': 0.2, 'weight_constraint': 5}0.721354 (0.032734) with: {'dropout_rate': 0.3, 'weight_constraint': 1}0.707031 (0.036782) with: {'dropout_rate': 0.3, 'weight_constraint': 2}0.707031 (0.036782) with: {'dropout_rate': 0.3, 'weight_constraint': 3}0.694010 (0.019225) with: {'dropout_rate': 0.3, 'weight_constraint': 4}0.709635 (0.006639) with: {'dropout_rate': 0.3, 'weight_constraint': 5}0.704427 (0.008027) with: {'dropout_rate': 0.4, 'weight_constraint': 1}0.717448 (0.031304) with: {'dropout_rate': 0.4, 'weight_constraint': 2}0.718750 (0.030425) with: {'dropout_rate': 0.4, 'weight_constraint': 3}0.718750 (0.030425) with: {'dropout_rate': 0.4, 'weight_constraint': 4}0.722656 (0.029232) with: {'dropout_rate': 0.4, 'weight_constraint': 5}0.720052 (0.028940) with: {'dropout_rate': 0.5, 'weight_constraint': 1}0.703125 (0.009568) with: {'dropout_rate': 0.5, 'weight_constraint': 2}0.716146 (0.029635) with: {'dropout_rate': 0.5, 'weight_constraint': 3}0.709635 (0.008027) with: {'dropout_rate': 0.5, 'weight_constraint': 4}0.703125 (0.011500) with: {'dropout_rate': 0.5, 'weight_constraint': 5}0.707031 (0.017758) with: {'dropout_rate': 0.6, 'weight_constraint': 1}0.701823 (0.018688) with: {'dropout_rate': 0.6, 'weight_constraint': 2}0.701823 (0.018688) with: {'dropout_rate': 0.6, 'weight_constraint': 3}0.690104 (0.027498) with: {'dropout_rate': 0.6, 'weight_constraint': 4}0.695313 (0.022326) with: {'dropout_rate': 0.6, 'weight_constraint': 5}0.697917 (0.014382) with: {'dropout_rate': 0.7, 'weight_constraint': 1}0.697917 (0.014382) with: {'dropout_rate': 0.7, 'weight_constraint': 2}0.687500 (0.008438) with: {'dropout_rate': 0.7, 'weight_constraint': 3}0.704427 (0.011201) with: {'dropout_rate': 0.7, 'weight_constraint': 4}0.696615 (0.016367) with: {'dropout_rate': 0.7, 'weight_constraint': 5}0.680990 (0.025780) with: {'dropout_rate': 0.8, 'weight_constraint': 1}0.699219 (0.019401) with: {'dropout_rate': 0.8, 'weight_constraint': 2}0.701823 (0.015733) with: {'dropout_rate': 0.8, 'weight_constraint': 3}0.684896 (0.023510) with: {'dropout_rate': 0.8, 'weight_constraint': 4}0.696615 (0.017566) with: {'dropout_rate': 0.8, 'weight_constraint': 5}0.653646 (0.034104) with: {'dropout_rate': 0.9, 'weight_constraint': 1}0.677083 (0.012075) with: {'dropout_rate': 0.9, 'weight_constraint': 2}0.679688 (0.013902) with: {'dropout_rate': 0.9, 'weight_constraint': 3}0.669271 (0.017566) with: {'dropout_rate': 0.9, 'weight_constraint': 4}0.669271 (0.012075) with: {'dropout_rate': 0.9, 'weight_constraint': 5}
			
		

We can see that the dropout rate of 0.2% and the maxnorm weight constraint of 4 resulted in the best accuracy of about 72%.
How to Tune the Number of Neurons in the Hidden Layer
The number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity of the network, at least at that point in the topology.
Also, generally, a large enough single layer network can approximate any other neural network, at least in theory.
In this example, we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5.
A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons.
The full code listing is provided below.

		
		
			
			
			
			
# Use scikit-learn to grid search the number of neurons
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from keras.constraints import maxnorm
# Function to create model, required for KerasClassifier
def create_model(neurons=1):
	# create model
	model = Sequential()
	model.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))
	model.add(Dropout(0.2))
	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)
# define the grid search parameters
neurons = [1, 5, 10, 15, 20, 25, 30]
param_grid = dict(neurons=neurons)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					12345678910111213141516171819202122232425262728293031323334353637383940
				# Use scikit-learn to grid search the number of neuronsimport numpyfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.layers import Dropoutfrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.constraints import maxnorm# Function to create model, required for KerasClassifierdef create_model(neurons=1):	# create model	model = Sequential()	model.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))	model.add(Dropout(0.2))	model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load datasetdataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)# define the grid search parametersneurons = [1, 5, 10, 15, 20, 25, 30]param_grid = dict(neurons=neurons)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Running this example produces the following output.

		
		
			
			
			
			
Best: 0.714844 using {'neurons': 5}
0.700521 (0.011201) with: {'neurons': 1}
0.714844 (0.011049) with: {'neurons': 5}
0.712240 (0.017566) with: {'neurons': 10}
0.705729 (0.003683) with: {'neurons': 15}
0.696615 (0.020752) with: {'neurons': 20}
0.713542 (0.025976) with: {'neurons': 25}
0.705729 (0.008027) with: {'neurons': 30}
			
				
					12345678
				Best: 0.714844 using {'neurons': 5}0.700521 (0.011201) with: {'neurons': 1}0.714844 (0.011049) with: {'neurons': 5}0.712240 (0.017566) with: {'neurons': 10}0.705729 (0.003683) with: {'neurons': 15}0.696615 (0.020752) with: {'neurons': 20}0.713542 (0.025976) with: {'neurons': 25}0.705729 (0.008027) with: {'neurons': 30}
			
		

We can see that the best results were achieved with a network with 5 neurons in the hidden layer with an accuracy of about 71%.
Tips for Hyperparameter Optimization
This section lists some handy tips to consider when tuning hyperparameters of your neural network.
k-fold Cross Validation. You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.Review the Whole Grid. Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.Parallelize. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of AWS instances.Use a Sample of Your Dataset. Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.Start with Coarse Grids. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.Do not Transfer Results. Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.Reproducibility is a Problem. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post.
Summary
In this post, you discovered how you can tune the hyperparameters of your deep learning networks in Python using Keras and scikit-learn.
Specifically, you learned:
How to wrap Keras models for use in scikit-learn and how to use grid search.How to grid search a suite of different standard neural network parameters for Keras models.How to design your own hyperparameter optimization experiments.
Do you have any experience tuning hyperparameters of large neural networks? Please share your stories below.
Do you have any questions about hyperparameter optimization of neural networks or about this post? Ask your questions in the comments and I will do my best to answer.

			
Frustrated With Your Progress In Deep Learning?

 What If You Could Develop A Network in Minutes
…with just a few lines of Python
Discover how in my new Ebook: Deep Learning With Python
It covers self-study tutorials and end-to-end projects on topics like:
Multilayer Perceptrons, Convolutional Nets and Recurrent Neural Nets, and more…
Finally Bring Deep Learning To
Your Own Projects
Skip the Academics. Just Results.
Click to learn more.


		


				
					
						
							
							Share on TwitterTweet
						
											
				
								
					
						
							
							Share on Facebook
							Share
						
											
				
				
				
					
						
							Share on LinkedIn
							Share
						
											
				
								
					
						
							
							Share on Google Plus
							Share
						
											
				
					
	

	
	
		About Jason Brownlee
		Jason Brownlee, Ph.D. is a machine learning specialist who teaches developers how to get results with modern machine learning methods via hands-on tutorials.				
			
				View all posts by Jason Brownlee →			
		
			
	



	        
	             How To Work Through a Binary Classification Project in Weka Step-By-Step
	            How to Work Through a Regression Machine Learning Project in Weka Step-By-Step 
	            
	        

				 	285 Responses to How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras
		 	

	      	

					                
	            
		      	

	                Yanbo
	                August 9, 2016 at 9:10 am
	                #
	                

				

		   		

				As always excellent post,. I’ve been doing some hyper-parameter optimization by hand, but I’ll definitely give Grid Search a try.
Is it possible to set up a different threshold for sigmoid output in Keras? Rather then using 0.5 I was thinking of trying 0.7 or 0.8

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 15, 2016 at 11:10 am
	                #
	                

				

		   		

				Thanks Yanbo.
I don’t think so, but you could implement your own activation function and do anything you wish.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shudhan
	                September 5, 2016 at 6:20 pm
	                #
	                

				

		   		

				My question is related to this thread. How to get the probablities as the output? I dont want the class output. I read for a regression problem that no activation function is needed in the output layer. Similiar implementation will get me the probabilities ?? or the output will exceed 0 and 1??

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 6, 2016 at 9:41 am
	                #
	                

				

		   		

				Hi Shudhan, you can use a sigmoid activation and treat the outputs like probabilities (they will be in the range of 0-1).

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Swapna
	                November 2, 2017 at 11:51 pm
	                #
	                

				

		   		

				excellent post

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 3, 2017 at 5:18 am
	                #
	                

				

		   		

				Thanks Swapna.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                eclipsedu
	                August 18, 2016 at 5:55 pm
	                #
	                

				

		   		

				Sound awesome!Will this grid search method use the full cpu(which can be 8/16 cores) ?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 19, 2016 at 5:23 am
	                #
	                

				

		   		

				It can if you set n_jobs=-1

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Reza
	                August 18, 2016 at 6:00 pm
	                #
	                

				

		   		

				Hi,
Great post,
Can I use this tips on CNNs in keras as well?
Thanks!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 19, 2016 at 5:24 am
	                #
	                

				

		   		

				They can be a start, but remember it is a good idea to use a repeating structure in a large CNN and you will need to tune the number of filters and pool size.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Prashant
	                August 22, 2016 at 4:55 pm
	                #
	                

				

		   		

				Hi Jason, First of all great post! I applied this by dividing the data into train and test and used train dataset for grid fit. Plan was to capture best parameters in train and apply them on test to see accuracy. But it seems grid.fit and model.fit applied with same parameters on same dataset (in this case train) give different accuracy results. Any idea why this happens. I can share the code if it helps.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 23, 2016 at 6:00 am
	                #
	                

				

		   		

				You will see small variation in the performance of a neural net with the same parameters from run to run. This is because of the stochastic nature of the technique and how very hard it is to fix the random number seed successfully in python/numpy/theano.
You will also see small variation due to the data used to train the method.
Generally, you could use all of your data to grid search to try to reduce the second type of variation (slower). You could store results and use statistical significance tests to compare populations of results to see if differences are significant to sort out the first type or variation.
I hope that helps.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                vinay
	                August 22, 2016 at 9:05 pm
	                #
	                

				

		   		

				hi, I think this will best tutorial i ever found on web….Thanks for sharing….is it possible to use these tips on LSTM, Bilstm cnnlstm

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 23, 2016 at 5:57 am
	                #
	                

				

		   		

				Thanks Vinay, I’m glad it’s useful.
Absolutely, you could use these tactics on other algorithm types.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                shudhan
	                September 2, 2016 at 3:26 pm
	                #
	                

				

		   		

				Best place to learn the tuning.. my question – is it good to follow the order you mentioned to tune the parameters? I know the most significant parameters should be tuned first

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 3, 2016 at 6:56 am
	                #
	                

				

		   		

				Thanks. The order is a good start. It is best to focus on areas where you think you will get the biggest improvement first – which is often the structure of the network (layers and neurons).

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Satheesh
	                September 27, 2016 at 12:24 am
	                #
	                

				

		   		

				when I am using the categorical_entropy loss function and running the grid search with n_jobs more than 1 its throwing error “cannot pickle object class”, but the same thing is working fine with binary_entropyloss. Can you tell me if I am making any mistake in my code:
    def create_model(optimizer=’adam’):
        # create model
        model.add(Dense(30, input_dim=59, init=’normal’, activation=’relu’))
        model.add(Dense(15, init=’normal’, activation=’sigmoid’))
        model.add(Dense(3, init=’normal’, activation=’sigmoid’))
        # Compile model
        model.compile(loss=’categorical_crossentropy’, optimizer=optimizer, metrics=[‘accuracy’])
        return model
    # Create Keras Classifier
    print “——————— Running Grid Search on Keras Classifier for epochs and batch ——————”
    clf = model = KerasClassifier(build_fn = create_model, verbose=0)
    param_grid = {“batch_size”:range(10, 30, 10), “nb_epoch”:range(50, 150, 50)}
    optimizer = [‘SGD’, ‘RMSprop’, ‘Adagrad’, ‘Adadelta’, ‘Adam’, ‘Adamax’, ‘Nadam’]
    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=4)
    grid_result = grid.fit(x_train, y_train)
    print(“Best: %f using %s” % (grid_result.best_score_, grid_result.best_params_))

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 27, 2016 at 7:44 am
	                #
	                

				

		   		

				Strange Satheesh, I have not seen that before.
Let me know if you figure it out.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Kai
	                September 18, 2017 at 10:01 pm
	                #
	                

				

		   		

				I came cross and solved the problem several days ago. Please use “epochs” instead of “nb_epoch” in param_grid dict. Personally, I guess “cannot pickle object class” means the neuron network cannot be built because of some errors. Open to discussion.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 19, 2017 at 7:40 am
	                #
	                

				

		   		

				Glad to hear it. 
I updated the example to use “epochs” to work with Keras 2.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                L Fenu
	                November 9, 2016 at 7:47 pm
	                #
	                

				

		   		

				excellent post, thanks. It’s been very helpful to get me started on hyperparameterisation.
One thing I haven’t been able to do yet is to grid search over parameters which are not proper to the NN but to the trainign set. For example, I can fine-tune the input_dim parameter by creating a function generator which takes care of creating the function that will create the model, like this:
        # fp_subset is a subset of columns of my whole training set.
        create_basic_ANN_model = kt.ANN_model_gen( # defined elsewhere
            input_dim=len(fp_subset), output_dim=1, layers_num=2, layers_sizes=[len(fp_subset)/5, len(fp_subset)/10, ],
            loss=’mean_squared_error’, optimizer=’adadelta’, metrics=[‘mean_squared_error’, ‘mean_absolute_error’]
        )
        model = KerasRegressor(build_fn=create_basic_ANN_model, verbose=1)
        # define the grid search parameters
        batch_size = [10, 100]
        epochs = [5, 10]
        param_grid = dict(batch_size=batch_size, nb_epoch=epochs)
        grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=7)
        grid_results = grid.fit(trX, trY)
this works but only as a for loop over the different fp_subset, which I must define manually.
I could easily pick the best out of every run but it wuld be great if I could fold them all inside a big grid definition and fit, so as to automatically pick the largest.
However, until now haven’t been able to figure out a way to get that in my head.
If the wrapper function is useful to anyone, I can post a generalised version here.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 10, 2016 at 7:42 am
	                #
	                

				

		   		

				Good question.
You might just need to us a loop around the whole lot for different projections/views of your training data.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                L Fenu
	                November 11, 2016 at 1:05 am
	                #
	                

				

		   		

				Thanks. I ended up coding my own for loop, saving the results of each grid in a dict, sorting the hash by the perofrmance metrics, and picking the best model.
Now, the next question is: How do I save the model’s architecture and weights to a .json .hdf5 file? I know how to do that for a simple model. But how do I extract the best model out of the gridsearch results?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 11, 2016 at 10:04 am
	                #
	                

				

		   		

				Well done.
No need. Once you know the parameters, you can use them to train a new standalone model on all of your training data and start making predictions.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Fenu Luca
	                November 15, 2016 at 3:23 am
	                #
	                

				

		   		

				I may have found a way. How about this?
        best_model = grid_result.best_estimator_.model
        best_model_file_path = ‘your_pick_here’
        model2json = best_model.to_json()
        with open( best_model_file_path+’.json’, ‘w’) as json_file:
            json_file.write(model2json)
        best_model.save_weights(best_model_file_path+’.h5′)

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                volador
	                November 14, 2016 at 6:21 pm
	                #
	                

				

		   		

				Hi Jason,  I think this is very best deep learning tutorial on the web. Thanks for your work. I have a question is :how to use the heuristic algorithm to optimize Hyperparameters for Deep Learning Models in Python With Keras, these algorithms like: Genetic algorithm, Particle swarm optimization, and Cuckoo algorithm etc.  If the idea could be experimented, could you give an example

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 15, 2016 at 7:50 am
	                #
	                

				

		   		

				Thanks for your support volador.
You could search the hyperparameter space using a stochastic optimization algorithm like a genetic algorithm and use the mean performance as the cost function orf fitness function. I don’t have a worked example, but it would be relatively easy to setup.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jan de Lange
	                November 15, 2016 at 6:50 am
	                #
	                

				

		   		

				Hi Jason, very helpful intro into gridsearch for Keras. I have used your guidance in my code, but rather than using the default ‘accuracy’ to be optimized, my model requires a specific evaluation function to be optimized. You hint at this possibility in the introduction, but there is no example of it. I have followed the SciKit-learn documentation, but I fail to come up with the correct syntax.
I have posted my question at StackOverflow, but since it is quite specific, it requires understanding of SciKit-learn in combination with Keras.
Perhaps you can have a look? I think it would nicely extend your tutorial.
http://stackoverflow.com/questions/40572743/scikit-learn-grid-search-own-scoring-object-syntax
Thanks, Jan

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 15, 2016 at 8:02 am
	                #
	                

				

		   		

				Sorry Jan, I have not used a custom scoring function before.
Here are a list of built-in scoring functions:
http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
Here is help on defining your own scoring function:
http://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions
Let me know how you go.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jan de Lange
	                November 16, 2016 at 7:31 am
	                #
	                

				

		   		

				Yup, same sources as I referenced in my post at Stackoverflow.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 16, 2016 at 9:35 am
	                #
	                

				

		   		

				Excellent. Good luck Jan.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Anthony Ohazulike
	                December 6, 2016 at 12:46 am
	                #
	                

				

		   		

				Good tutorial again Jason…keep on the good job!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 6, 2016 at 8:26 am
	                #
	                

				

		   		

				Thanks Anthony.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                nrcjea001
	                December 13, 2016 at 10:48 pm
	                #
	                

				

		   		

				Hi Jason
First off, thank you for the tutorial. It’s very helpful.
I was also hoping you would assist on how to adapt the keras grid search to stateful lstms as discussed in 
http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
I’ve coded the following:
 # create model
model = KerasRegressor(build_fn=create_model, nb_epoch=1, batch_size=bats,
                       verbose=2, shuffle=False)
# define the grid search parameters
h1n = [5, 10] # number of hidden neurons
param_grid = dict(h1n=h1n)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)
for i in range(100):
    grid.fit(trainX, trainY)
    grid.reset_states()
Is grid.reset_states() corrrect? or would you suggest creating function callback for reset states. 
Thanks,

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 14, 2016 at 8:27 am
	                #
	                

				

		   		

				Great question.
With stateful LSTMs we must control the resetting of states after each epoch. The sklearn framework does not open this capacity to us – at least it looks that way to me off the cuff.
I think you may have to grid search stateful LSTM params manually with a ton of for loops. Sorry. 
If you discover something different, let me know. i.e. there may be a way in the back door to the sklearn grid search functionality that we can inject our own custom epoch handing.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Thomas Maier
	                December 21, 2016 at 2:53 am
	                #
	                

				

		   		

				Hi Jason
Thanks a lot for this and all the other great tutorials!
I tried to combine this gridsearch/keras approach with a pipeline. It works if I tune nb_epoch or batch_size, but I get an error if I try to tune the optimizer or something else in the keras building function (I did not forget to include the variable as an argument):
def keras_model(optimizer = ‘adam’):
    model = Sequential()
    model.add(Dense(80, input_dim=79, init= ‘normal’))
    model.add(Activation(‘relu’))
    model.add(Dense(1, init=’normal’))
    model.add(Activation(‘linear’))
    model.compile(optimizer=optimizer, loss=’mse’)
    return model
kRegressor = KerasRegressor(build_fn=keras_model, nb_epoch=500, batch_size=10, verbose=0)
estimators = []
estimators.append((‘imputer’, preprocessing.Imputer(strategy=’mean’)))
estimators.append((‘scaler’,  preprocessing.StandardScaler()))
estimators.append((‘kerasR’,  kRegressor))
pipeline = Pipeline(estimators)
param_grid = dict(kerasR__optimizer = [‘adam’,’rmsprop’])
grid = GridSearchCV(pipeline, param_grid, cv=5, scoring=’neg_mean_squared_error’)
Do you know this problem?
Thanks, Thomas

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 21, 2016 at 8:44 am
	                #
	                

				

		   		

				Thanks Thomas. I’ve not seen this issue.
I think we’re starting to push the poor Keras sklearn wrapper to the limit.
Maybe the next step is to build out a few functions to do manual grid searching across network configs.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jimi
	                December 21, 2016 at 3:26 pm
	                #
	                

				

		   		

				Great resource! 
Any thoughts on how to get the “history” objects out of grid search? It could be beneficial to plot the loss and accuracy to see when a model starts to flatten out.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 22, 2016 at 6:30 am
	                #
	                

				

		   		

				Not sure off the cuff Jimi, perhaps repeat the run standalone for the top performing configuration.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                DeepLearning
	                January 4, 2017 at 6:08 am
	                #
	                

				

		   		

				Thanks for the post. Can we optimize the number of hidden layers as well on top of number of neurons in each layers?
Thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 4, 2017 at 9:00 am
	                #
	                

				

		   		

				Yes, it just may be very time consuming depending on the size of the dataset and the number of layers/nodes involved.
Try it on some small datasets from the UCI ML Repo.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                DeepLearning
	                January 4, 2017 at 12:02 pm
	                #
	                

				

		   		

				Thanks. Would you mind looking at below code? 
def create_model(neurons=1, neurons2=1):
	# create model
	model = Sequential()
	model.add(Dense(neurons1, input_dim=8))
        model.add(Dense(neurons2))
	model.add(Dense(1, init=’uniform’, activation=’sigmoid’))
	# Compile model
	model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])
	return model
# define the grid search parameters
neurons1 = [1, 3, 5, 7]
neurons2=[0,1,2]
param_grid = dict(neurons1=neurons1, neurons2=neurons2)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
This code runs without error (I excluded certain X, y parts for brewity) but when I run “grid.fit(X, Y), it gives AssertionError.
I’d appreciate if you can show me where I am wrong.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                DeepLearning
	                January 4, 2017 at 12:26 pm
	                #
	                

				

		   		

				Update” It worked when I deleted 0 from neurons2. Thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 5, 2017 at 9:16 am
	                #
	                

				

		   		

				Excellent, glad to hear it.

				
	                
	                    	                

				

			

	


	      	

					                
	            
		      	

	                Jason Brownlee
	                January 5, 2017 at 9:16 am
	                #
	                

				

		   		

				A Dense() with a value of 0 neurons might blow up. Try removing the 0 from your neurons2 array. 
A good debug strategy is to cut code back to the minimum, make it work, then and add complexity. Here. Try searching a grid of 1 and 1 neurons, make it all work, then expand the grid you search.
Let me know how you go.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                DeepLearning
	                January 9, 2017 at 11:04 am
	                #
	                

				

		   		

				I keep getting error messages and I tried a big for loops that scan for all possible combinations of layer numbers, neuron numbers, other optimization stuff within defined limits. It is very time consuming code, but I could not figure it out how to adjust layer structure and other optimization parameters in the same code using GridSearch. If you would provide a code for that in your blog one day, that would be much appreciated. Thanks.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 10, 2017 at 8:55 am
	                #
	                

				

		   		

				I’ll try to find the time.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Rajneesh
	                January 11, 2017 at 10:48 am
	                #
	                

				

		   		

				Hi Jason,
Many thanks for this awesome tutorial !

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 12, 2017 at 9:24 am
	                #
	                

				

		   		

				I’m glad you found it useful Rajneesh.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Andy
	                January 22, 2017 at 1:02 pm
	                #
	                

				

		   		

				Hi Jason,
Great tutorial! I’m running into a slight issue. I tried running this on my own variation of the code and got the following error:
TypeError: get_params() got an unexpected keyword argument ‘deep’
I copied and pasted your code using the given data set and got the same error. The code is showing an error on the grid_result = grid.fit(X, Y) line. I looked through the other comments and didn’t see anyone with the same issue. Do you know where this could be coming from?
Thanks for your help!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                YechiBechi
	                January 23, 2017 at 2:18 am
	                #
	                

				

		   		

				same issue here,
great tutorial, life saver.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 23, 2017 at 8:35 am
	                #
	                

				

		   		

				Hi Andy, sorry to hear that.
Is this happening with a specific example or with all of them?
Are you able to check your version of Python/sklearn/keras/tf/theano?
UPDATE:
I can confirm the first example still works fine with Python 2.7, sklearn 0.18.1, Keras 1.2.0 and TensorFlow 0.12.1.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Andy
	                January 25, 2017 at 7:12 am
	                #
	                

				

		   		

				The only differences are I am running Python 3.5 and Keras 1.2.1. The example I ran previously was the grid search for the number of neurons in a layer. But I just ran the first example and got the same error.
Do you think the issue is due to the next version of Python? If so, what should my next steps be?
Thanks for your help and quick response!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jannes
	                January 27, 2017 at 5:51 am
	                #
	                

				

		   		

				It’s a bug in Keras 1.2.1. You can either downgrade to 1.2.0 or get the code from their github (where they already fixed it).

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 27, 2017 at 12:22 pm
	                #
	                

				

		   		

				Yes, I have a write up of the problem and available fixes here:
http://stackoverflow.com/questions/41796618/python-keras-cross-val-score-error/41841066#41841066

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Andy
	                January 28, 2017 at 4:21 pm
	                #
	                

				

		   		

				Thank you so much for your help!

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                kono
	                February 8, 2017 at 3:14 am
	                #
	                

				

		   		

				Jason,
Can you use early_stopping to decide n_epoch?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 8, 2017 at 9:36 am
	                #
	                

				

		   		

				Yes, that is a good method to find a generalized model.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jayant
	                February 23, 2017 at 4:33 am
	                #
	                

				

		   		

				Hi Jason,
Really great article. I am a big fan of your blog and your books. Can you please explain your following statement?
“A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.”
I didn’t see anywhere cross-validation being used.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 23, 2017 at 8:56 am
	                #
	                

				

		   		

				Hi Jayant,
Grid search uses k-fold cross-validation to evaluate the performance of each combination of parameters on unseen data.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jing
	                February 28, 2017 at 2:09 am
	                #
	                

				

		   		

				Hi Jason,
 thanks for this awesome tutorial !
 I have two questions: 1. In “model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])”, accuracy is used for evaluate results. But GridSearchCV also has scoring parameter, if I set “scoring=’f1’”,which one is used for evaluate  the results of grid search? 2.How to set two evaluate parameters ,e.g. ‘accuracy’and ’f1’ evaluating the results of grid search？

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 28, 2017 at 8:13 am
	                #
	                

				

		   		

				Hi Jing,
You can set the “scoring” argument for GridSearchCV with a string of the performance measure to use, or the name of your own scoring function. You can learn about this argument here:
http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
You can see a full list of supported scoring measures here:
http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
As far as I know you can only grid search using a single measure.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jing
	                February 28, 2017 at 12:50 pm
	                #
	                

				

		   		

				Thank you so much for your help!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jing
	                February 28, 2017 at 1:54 pm
	                #
	                

				

		   		

				I find no matter what evaluate parameters used in GridSearchCV “scoring”,”metrics” in “model.compile” must be [‘accuracy’],otherwise the program gives “ValueError: The model is not configured to compute accuracy.You should pass ‘metrics=[“accuracy”]’ to the ‘model.compile()’method. So, if  I set:
model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=’recall’)
the grid_result.best_score_ =0.72.My question is: 0.72 is accuracy or recall ? Thank you!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 1, 2017 at 8:31 am
	                #
	                

				

		   		

				Hi Jing,
When using GridSearchCV with Keras, I would suggest not specifying any metrics when compiling your Keras model.
I would suggest only setting the “scoring” argument on the GridSearchCV. I would expect the metric reported by GridSearchCV to be the one that you specified.
I hope that helps.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Dan
	                March 8, 2017 at 4:13 am
	                #
	                

				

		   		

				Great Blogpost. Love it. You are awesome Jason. I got one question to GridsearchCV. As far as i understand the crossvalidation already takes place in there. That’s why we do not need any kfold anymore.
But with this technique we would have no validation set correct? e.g. with a default value of 3 we would have 2 training sets and one test set.
That means in kfold as well as in GridsearchCV there is no requirement for creating a validation set anymore?
Thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 8, 2017 at 9:44 am
	                #
	                

				

		   		

				Hi Dan,
Yes, GridSearchCV performs cross validation and you must specify the number of folds. You can hold back a validation set to double check the parameters found by the search if you like. This is optional.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Dan
	                March 9, 2017 at 3:25 am
	                #
	                

				

		   		

				Thank you for the quick response Jason. Especially considering the huge amount of questions you get.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 9, 2017 at 9:55 am
	                #
	                

				

		   		

				I’m here to help, if I can Dan.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Johan Steunenberg
	                March 22, 2017 at 8:25 pm
	                #
	                

				

		   		

				What I’m missing in the tutorial is the info, how to get the best params in the model with KERAS. Do I pickup the best parameters and call ‘create_model’ again with those parameters or can I call the GridSearchCV’s ‘predict’ function? (I will try out for myself but for completeness it would be good to have it in the tutorial as well.)

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 23, 2017 at 8:49 am
	                #
	                

				

		   		

				I see, but we don’t know the best parameters, we must search for them.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Maycown Miranda
	                April 5, 2017 at 2:09 am
	                #
	                

				

		   		

				Hi, Jason. I am getting
/usr/local/lib/python2.7/dist-packages/keras/wrappers/scikit_learn.py in check_params(self=, params={‘batch_size’: 10, ‘epochs’: 10})
     80             legal_params += inspect.getargspec(fn)[0]
     81         legal_params = set(legal_params)
     82
     83         for params_name in params:
     84             if params_name not in legal_params:
—> 85                 raise ValueError(‘{} is not a legal parameter’.format(params_name))
        params_name = ‘epochs’
     86
     87     def get_params(self, _):
     88         “””Gets parameters for this estimator.
     89 
ValueError: epochs is not a legal parameter

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                April 9, 2017 at 2:32 pm
	                #
	                

				

		   		

				It sounds like you need to upgrade to Keras v2.0 or higher.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Chandra Sutrisno Tjhong
	                November 28, 2017 at 10:46 am
	                #
	                

				

		   		

				I experienced the same problem.I upgraded my keras and the same problem still occurs.

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Usman
	                May 3, 2017 at 7:56 am
	                #
	                

				

		   		

				Nice tutorial. I would like to optimize the number of hidden layers in the model. Can you please guide in this regard, thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 4, 2017 at 7:59 am
	                #
	                

				

		   		

				Thanks Usman.
Consider exploring specific patterns, e.g. small-big-small, etc.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Carl
	                May 5, 2017 at 12:58 pm
	                #
	                

				

		   		

				Do you know any way this could be possible using a network with multiple inputs?
http://imgur.com/a/JJ7f1

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                DanielP
	                May 9, 2017 at 4:26 pm
	                #
	                

				

		   		

				Hi Jason, great to see posts like this – amazing job!
Just noticed, when you tune the optimisation algorithm SGD performs at 34% accuracy. As no parameters are being passed to the SGD function, I’d assume it takes the default configuration, lr=0.01, momentum=0.0. 
Later on, as you look for better configurations for SGD, best result (68%) is found when {‘learn_rate’: 0.01, ‘momentum’: 0.0}. 
It seems to me that these two experiments use exactly the same network configuration (including the same SGD parameters), yet their resulting accuracies differ significantly. Do you have any intuition as to why this may be happening?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 10, 2017 at 8:43 am
	                #
	                

				

		   		

				Hi Daniel, yes great point.
Neural networks are stochastic and give different results when evaluated on the same data.
Ideally, each configuration would be evaluated using the average of multiple (30+) repeats.
This post might help:
http://machinelearningmastery.com/randomness-in-machine-learning/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Pradanuari
	                May 14, 2017 at 3:13 am
	                #
	                

				

		   		

				Hi Jason!
absolutely love your tutorial! But would you mind to give tutorial for how to tune the number of hidden layer?
Thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 14, 2017 at 7:32 am
	                #
	                

				

		   		

				I have an example here:
http://machinelearningmastery.com/exploratory-configuration-multilayer-perceptron-network-time-series-forecasting/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Pradanuari
	                May 14, 2017 at 11:32 pm
	                #
	                

				

		   		

				Thank you so much Jason!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 15, 2017 at 5:53 am
	                #
	                

				

		   		

				I’m glad it helped Pradanuari.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Ibrahim El-Fayoumi
	                May 17, 2017 at 12:53 pm
	                #
	                

				

		   		

				Hello Jason
I tried to use your idea in a similar problem but I am getting error : AttributeError: ‘NoneType’ object has no attribute ‘loss’
it looks like the model does not define loss function?
This is the error I get:
b\site-packages\keras-2.0.4-py3.5.egg\keras\wrappers\scikit_learn.py in fit(self=, x=memmap([[[ 0.,  0.,  0., …,  0.,  0.,  0.],
  …,  0.,  0., …,  0.,  0.,  0.]]], dtype=float32), y=array([[ 0.,  0.,  0., …,  0.,  0.,  0.],
    …0.],
       [ 0.,  0.,  0., …,  0.,  1.,  0.]]), **kwargs={})
    135             self.model = self.build_fn(
    136                 **self.filter_sk_params(self.build_fn.__call__))
    137         else:
    138             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))
    139
–> 140         loss_name = self.model.loss
        loss_name = undefined
        self.model.loss = undefined
    141         if hasattr(loss_name, ‘__name__’):
    142             loss_name = loss_name.__name__
    143         if loss_name == ‘categorical_crossentropy’ and len(y.shape) != 2:
    144             y = to_categorical(y)
AttributeError: ‘NoneType’ object has no attribute ‘loss’
___________________________________________________________________________
Process finished with exit code 1
Regards
Ibrahim

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 18, 2017 at 8:26 am
	                #
	                

				

		   		

				Does the example in the blog post work on your system?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Ibrahim El-Fayoumi
	                May 18, 2017 at 12:18 pm
	                #
	                

				

		   		

				Ok, I think your code needs to be placed after
if __name__ == ‘__main__’:
to work with multiprocess…
But thanks for the post is great…

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 19, 2017 at 8:12 am
	                #
	                

				

		   		

				Not on Linux and OS X when I tested it, but thanks for the tip.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Gautam
	                August 25, 2017 at 11:33 pm
	                #
	                

				

		   		

				n_jobs=-1 doesnt work on Windows.
@Ibrahim: Can you please explain, what part of the code needs to be behind
if __name__ == ‘__main__’: )

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Edward
	                May 21, 2017 at 3:17 am
	                #
	                

				

		   		

				Hello Jason!
I do the first step – try to tune Batch Size and Number of Epochs and get
print(“Best: %f using %s” % (grid_result.best_score_, grid_result.best_params_))
Best: 0.707031 using {‘epochs’: 100, ‘batch_size’: 40}
After that I do the same and get
print(“Best: %f using %s” % (grid_result.best_score_, grid_result.best_params_))
Best: 0.688802 using {‘epochs’: 100, ‘batch_size’: 20}
And so on
The problem is in the grid_result.best_score_
I expect that in the second step (for ample tuning optimizer) I will get grid_result.best_score_ better than in the first step (in the second step i use grid_result.best_params_ from the first step). But it is not true
Tune all Hyperparameters is a very long time
How to fix it?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 21, 2017 at 6:01 am
	                #
	                

				

		   		

				Consider tuning different parameters, like network structure or number of input features.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Edward
	                May 21, 2017 at 7:18 pm
	                #
	                

				

		   		

				Thanks a lot Jason!

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                pattijane
	                May 21, 2017 at 7:44 am
	                #
	                

				

		   		

				Hello, 
I’d like to have your opinion about a problem:
I have two loss function plots, with SGD and Adamax as optimizer with same learning rate.
Loss function of SGD looks like the red one, whereas Adamax’s looks like blue one.
(http://cs231n.github.io/assets/nn3/learningrates.jpeg)
I have better scores with Adamax on validation data. I’m confused about how to proceed, should I choose Adamax and play with learning rates a little more, or go on with SGD and somehow try to improve performance?
Thanks!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 22, 2017 at 7:49 am
	                #
	                

				

		   		

				Explore both, but focus on the validation score of interest (e.g. accuracy, RMSE, etc.) over loss. 
For example, you can get very low loss and get worse accuracy.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                pattijane
	                May 22, 2017 at 6:35 pm
	                #
	                

				

		   		

				Thanks for your response! I experimented with different learning rates and found out a reasonable one, (good for both Adamax and SGD) and now I try to fix learning rate and optimizer and focus on other hyperparameters such as batch-size and number of neurons. Or would be better if I set those first?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 23, 2017 at 7:49 am
	                #
	                

				

		   		

				Number of neurons will have a big effect along with learning rate.
Batch size will have a smaller effect and could be optimized last.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Lotem
	                May 23, 2017 at 1:47 am
	                #
	                

				

		   		

				Thanks for this post!
One question – why not use grid search on all the parameters together, rather than preforming several grid searches and finding each parameter separately? surly the results are not the same…

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                May 23, 2017 at 7:54 am
	                #
	                

				

		   		

				Great question,
In practice, the datasets are large and it can take a long time and require a lot of RAM.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                StatsSorceress
	                May 25, 2017 at 6:52 am
	                #
	                

				

		   		

				Hi Jason,
Excellent post!
It seems to me that if you use the entire training set during your cross-validation, then your cross-validation error is going to give you an optimistically biased estimate of your validation error. I think this is because when you train the final model on the entire dataset, the validation set you create to estimate test performance comes out of the training set. 
My question is: assuming we have a lot of data, should we use perhaps only 50% of the training data for cross-validation for the hyperparameters, and then use the remaining 50% for fitting the final model (and a portion of that remaining 50% would be used for the validation set)? That way we wouldn’t be using the same data twice. I am assuming in this case that we would also have a separate test set.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 2, 2017 at 11:38 am
	                #
	                

				

		   		

				Yes, it is a good idea to hold back a test set when tuning.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Yang
	                May 27, 2017 at 5:35 am
	                #
	                

				

		   		

				Thanks for your valuable post. I learned a lot from it.
When I wrote my code for grid search, I encountered a question: 
I use fit_generator instead of fit in keras.
Is it possible to use grid search with fit_generator ?
I have some Merge layers in my deep learning model.
Hence, the input of the neural network is not a single matrix.
For example:
Suppose we have 1,000 samples
Input =  [Input1,Input2]
Input1 is a 1,000 *3 matrix
Input2 is a 1,000*3*50*50 matrix (image)
When I use the fit in your post, there is a bug….because the input1 and input2 don’t have the same dimension. So I wonder whether the fit_generator can work with grid search ?
Thanks in advance!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Yang
	                May 27, 2017 at 6:46 am
	                #
	                

				

		   		

				Please ignore my previous reply.
I find an answer here: https://github.com/fchollet/keras/issues/6451
Right now, the GridsearchCV using the scikit wrapper for network with multiple inputs is not available.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Kate liu
	                May 28, 2017 at 4:31 pm
	                #
	                

				

		   		

				Hi Jason, thank you for your good tutorial of the grid research with Keras. I followed your example with my own dataset. It could be run. But when I using the autoencoder structure, instead of the sequential structure,  to gird the parameters with my own data. It could not be run. I don’t know the reason. Could you help me? Are there any differences between the gird of sequential structure and the grid of model structure?
The follows are my codes:
from keras.models import Sequential
from keras.layers import Dense, Input
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import numpy as np
from keras.optimizers import SGD, Adam, RMSprop, Adagrad
from keras.regularizers import l1,l2
from keras.models import Model
import pandas as pd
from keras.models import load_model
np.random.seed(2017)
def create_model(optimizer=’rmsprop’):
# encoder layers
encoding_dim =140
input_img = Input(shape=(6,))
encoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(input_img)
encoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(encoded)
encoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(encoded)
encoder_output = Dense(encoding_dim, activation=’relu’,W_regularizer=l1(0.01))(encoded)
# decoder layers
decoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(encoder_output)
decoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(decoded)
decoded = Dense(300, activation=’relu’,W_regularizer=l1(0.01))(decoded)
decoded = Dense(6, activation=’relu’,W_regularizer=l1(0.01))(decoded)
# construct the autoencoder model
autoencoder = Model(input_img, decoded)
# construct the encoder model for plotting
encoder = Model(input_img, encoder_output)
# Compile model
autoencoder.compile(optimizer=’RMSprop’, loss=’mean_squared_error’,metrics=[‘accuracy’])
return autoencoder

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 2, 2017 at 12:09 pm
	                #
	                

				

		   		

				I’m surprised, I would not think the network architecture would make a difference. 
Sorry, I have no good suggestions other than try to debug the cause of the fault.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Kate liu
	                May 28, 2017 at 4:36 pm
	                #
	                

				

		   		

				the command of autoencoder.compile is modified as the follows:
# Compile model
autoencoder.compile(optimizer=optimizer, loss=’mean_squared_error’,metrics=[‘accuracy’])

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Rahul
	                May 30, 2017 at 12:07 am
	                #
	                

				

		   		

				Can we do this for functional API as well ?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 2, 2017 at 12:28 pm
	                #
	                

				

		   		

				Perhaps, I have not done this.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Ian Worthington
	                May 30, 2017 at 10:36 pm
	                #
	                

				

		   		

				Thanks for a great tutorial Jason, appreciated.
njobs=-1 didn’t work very well on my Windows 10 machine: took a very long time and never finished.
https://stackoverflow.com/questions/28005307/gridsearchcv-no-reporting-on-high-verbosity seems to suggest this is (or at least was in 2015) a known problem under Windows so I changed to n_jobs=1, which also allowed me to see throughput using verbose=10.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 2, 2017 at 12:37 pm
	                #
	                

				

		   		

				Thanks for the tip.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Ian Worthington
	                May 31, 2017 at 1:56 am
	                #
	                

				

		   		

				Jason —
Given all the parameters it is possible to adjust, is there any recommendation for which should be fixed first before exploring others, or can ALL results for one change when others are changed?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 2, 2017 at 12:39 pm
	                #
	                

				

		   		

				Great question, see this paper:
https://arxiv.org/abs/1206.5533

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Ian Worthington
	                June 3, 2017 at 2:39 am
	                #
	                

				

		   		

				Thanks Jason, I’ll check it out.

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Mario
	                June 9, 2017 at 12:10 am
	                #
	                

				

		   		

				Hi and thank you for the resource.
Am I right in my understanding that this only works on one machine?
Any hints / pointers on how to run this on a cluster? I have found https://goo.gl/Q9Xy7B as a potential avenue using Spark (no Keras though).
Any comment at all? Information on the subject is scarce.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 9, 2017 at 6:26 am
	                #
	                

				

		   		

				Yes, this example is for a single machine. Sorry, I do not have examples for running on a cluster.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Shaun
	                June 16, 2017 at 11:54 pm
	                #
	                

				

		   		

				Hi Jason, 
I’m a little bit confused about the definition of the “score” or “accuracy”. How are they made? I believe that they are not simply comparing the results with target, otherwise it will be the overfitting model being the best (like the more neurons the better). 
But on the other hand, they are just using those combinations of parameters to train the model, so what is the difference between I manually set the parameters and see my result good or not, with risk of overfitting and the grid search that creates an accuracy score to determine which one is the best?
Best regards,

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 17, 2017 at 7:30 am
	                #
	                

				

		   		

				The grid search will provide an estimate of the skill of the model with a set of parameters.
Any one configuration in the grid search can be set and evaluated manually. 
Neural networks are stochastic and will give different predictions/skill when trained on the same data.
Ideally, if you have the time/compute the grid search should use repeated k-fold cross validation to provide robust estimates of model skill. More here:
http://machinelearningmastery.com/evaluate-skill-deep-learning-models/
Does that help?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shaun
	                June 20, 2017 at 2:30 am
	                #
	                

				

		   		

				I’m new to the NN, a little bit puzzled. So say, if I have to many neurons that leads to overfitting (good on the train set, bad on the validation or test set), can grid search detect it by the score? 
My guess is yes, because there is a validation set in the GridsearchCV. Is that correct?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 20, 2017 at 6:39 am
	                #
	                

				

		   		

				A larget network can overfit. 
The idea is to find a config that does well on the train and validation sets. We require a robust test harness. With enough resources, I’d recommend repeated k-fold cross validation within the grid search. 

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Huyen
	                June 19, 2017 at 4:21 pm
	                #
	                

				

		   		

				One more very useful tutorial, thank Jason.
One question about GridSearch in my case. I have tried to tune parameters of my neural network for regression with 18 inputs size 800 but the time to use GridSearch totally long, like forever even though I have limited to the number. I saw in your code:
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
Normally, n_jobs=1, can I increase that number to improve the performances?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 20, 2017 at 6:36 am
	                #
	                

				

		   		

				We often cannot grid search with neural nets because it takes so long!
Consider running on a large computer in the cloud over the weekend.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Bobo
	                June 21, 2017 at 4:57 am
	                #
	                

				

		   		

				Hi Jason, 
Any idea how to use GridSearchCV if you don’t want cross validation?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 21, 2017 at 8:19 am
	                #
	                

				

		   		

				GridSearch supports k-fold cross-validation by default. That is what the “CV” is in the name:
http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Bobo
	                June 24, 2017 at 3:07 am
	                #
	                

				

		   		

				So sklearn has no GridSearch without cross validation?
In any case I found kind of a hack here to get rid of cv:
https://stackoverflow.com/questions/44636370/scikit-learn-gridsearchcv-without-cross-validation-unsupervised-learning

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                June 24, 2017 at 8:04 am
	                #
	                

				

		   		

				You can configure the k in CV to 1 to it does train/test. Then configure it to repeat.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                makis
	                June 28, 2017 at 11:54 pm
	                #
	                

				

		   		

				Hello. Thank you for the nice tutorial.
I am trying to combine pipeline and gridsearch.
Inside my keras model i use kernel_initializer=init_mode.
Then I am trying to assign values to the init_mode dictionary in order to perform the gridsearch.
I get the following error: ValueError: init_mode is not a legal parameter
My code is here: https://www.dropbox.com/s/57n777j9w8bxf4t/keras_grid.py?dl=0
Any tip? Thank you

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Abhijith Darshan Ravindra
	                July 11, 2017 at 6:31 am
	                #
	                

				

		   		

				Hi Dr. Brownlee,
When I run this in Spyder IDE nothing happens after grid.fit.
It just appears to do nothing.
Any suggestions as to why?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                July 11, 2017 at 10:34 am
	                #
	                

				

		   		

				Consider running from the command line.
The grid search may take a long time.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                DY
	                July 14, 2017 at 6:11 am
	                #
	                

				

		   		

				Hello Dr Brownlee,
I saved your example codes into .py file and run it. Nothing happens after grid.fit. However, if I run line by line from your example codes it works.  Do you know why?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                July 14, 2017 at 8:36 am
	                #
	                

				

		   		

				It may take a long time. Consider reducing the scope of the search to see if you can get results sooner.

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Tryfon
	                September 18, 2017 at 11:46 pm
	                #
	                

				

		   		

				I had the same issue with you (using spyder and python 3.6) but after changing the parameter n_jobs = 1 it worked fine. Also n_jobs = 2 was stuck although spyder showed it was running in the backgound (I checked the CPU usage and was down to 1% vs the 55-80% when it is actually running).
Don’t ask the reason why is that. My guess would be that it has to do with your system and the fact that it might not support parallelization (no CUDA GPU).

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 19, 2017 at 7:47 am
	                #
	                

				

		   		

				Consider running the example from the command line instead.

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Kamal Thapa
	                July 27, 2017 at 3:46 pm
	                #
	                

				

		   		

				How can I do Hyper-parameter optimization for MLPRegressor in scikit learn?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                July 28, 2017 at 8:28 am
	                #
	                

				

		   		

				Yes.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Josep
	                August 3, 2017 at 2:31 am
	                #
	                

				

		   		

				Hi Jason,
I’m unable to apply the grid search to a seq to seq LSTM network (Keras Regressor model in the scikit API). When I set the GridSearchCV scoring algorithm to r^2 (or any scoring function for regression problems) the model.fit expect a 2 dim input vector, not the 3 dim used in Keras.
Otherwise, if I left the default scoring algorithm named “_passthrough_scorer”( I don’t know what it does, I don’t even know what it is) it works but the best_score doesn’t match with the real best parametrization. I’m really confused…I’ll had to write the grid search manually…

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Josep
	                August 3, 2017 at 2:42 am
	                #
	                

				

		   		

				I’ve solved it, I share it if someone have the same issue…,If you set the gridsearch scoring function to “None” it uses the scoring metrics of the Keras model.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Josep
	                August 3, 2017 at 2:49 am
	                #
	                

				

		   		

				Sorry for bothering, but the results of the approach I’ve said are incorrect. I don’t know what to do.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jason Brownlee
	                August 3, 2017 at 6:54 am
	                #
	                

				

		   		

				Hi Josep,
Consider writing your own for loop to iterate over params and run a Cross Validation for the params within the loop.
This is how I do it now for large/complex models.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                kotb
	                August 8, 2017 at 7:10 pm
	                #
	                

				

		   		

				Can i use this grid search without using keras model

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 9, 2017 at 6:27 am
	                #
	                

				

		   		

				For sure!

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Aman Garg
	                August 19, 2017 at 3:35 am
	                #
	                

				

		   		

				Hello Jason, 
Thanks for such a nice tutorial. 
Instead of getting a output as ‘Best: 0.720052 using {‘init_mode’: ‘uniform’}’ , it would be really nice if you could show us how to visualize this result with matplotlib so that it gets more easier.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 19, 2017 at 6:23 am
	                #
	                

				

		   		

				Great suggestion, thanks.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Michael
	                August 20, 2017 at 4:42 am
	                #
	                

				

		   		

				Hi, Jason.  Thanks, again, for all of the blog posts and example code.  I’m trying to tune my binary classification Keras neural network.  My dataset includes about 50,000 entries with 52 (numeric) variables.  Using Grid Search, I’ve tested all sorts of combinations of layer size, number of epochs, batch size, optimizers, activations, learning rates, dropout rates, and L2 regularization parameters.   My grid search shows every combination performs the same.  For example, here is a snippet from my latest results:
Best: 0.876381 using {‘act’: ‘relu’, ‘opt’: ‘Adam’}
0.876381 (0.003878) with: {‘act’: ‘relu’, ‘opt’: ‘Adam’}
0.876381 (0.003878) with: {‘act’: ‘relu’, ‘opt’: ‘SGD’}
0.876381 (0.003878) with: {‘act’: ‘relu’, ‘opt’: ‘Adagrad’}
0.876381 (0.003878) with: {‘act’: ‘relu’, ‘opt’: ‘Adadelta’}
0.876361 (0.003880) with: {‘act’: ‘tanh’, ‘opt’: ‘Adam’}
0.876381 (0.003878) with: {‘act’: ‘tanh’, ‘opt’: ‘SGD’}
But I also get 0.876381 whether I have 1000 nodes or 1 node, and for every other combo I’ve tested.  I’ve also tried different ways of scaling or transforming my input data with no impact.
Do you have any thoughts on why I’m having trouble finding different combinations of parameters that actually have a difference in performance?  
Thank you for your help!  You rock!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                August 20, 2017 at 6:09 am
	                #
	                

				

		   		

				Very odd results. Double check your train/test data. 
Also, see this post for a long list of ideas to try:
http://machinelearningmastery.com/improve-deep-learning-performance/

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Michael
	                August 20, 2017 at 9:20 am
	                #
	                

				

		   		

				Thanks

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Shubham Kumar
	                September 3, 2017 at 11:54 am
	                #
	                

				

		   		

				Hey Jason.
I was using grid search to tune hyperparameters for a CNN-LSTM classification problem.
I used the code template on your blog about sequence classification.
MY original data has 38932 instances, but for tuning I am using only 1000 to save time.
But even then, I am not sure how to best search for those parameters and save time. 
Is it a bad idea to search for hyper parameters in a small subset (almost 1/40th of training in my case).
Will the result vary largely when I use actual data size?
Also, I passed in several parameters for the grid search. Left it overnight and it still hadn’t made enough progress, so I stopped the execution.
How can I speed up this process?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 3, 2017 at 3:44 pm
	                #
	                

				

		   		

				The result will be biased, but perhaps might give you an idea of the direction in which to proceed – this could be enough for you.
I often run a lot of sanity check grid searches on small samples to get ideas on which direction to push.
More data will result in less biased estimates of model skill, often proportionately to some point of diminishing returns.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shubham Kumar
	                September 4, 2017 at 3:10 am
	                #
	                

				

		   		

				Great !
I did read that one of the sanity checks is to check whether the model overfits on a small sample! If yes, then we are good to go…
I am slightly new to building proper models and find this part exciting but a little intimidating at the same time !
I am going to use only a few hyper parameters at a time, and keep  the rest constant and check what happens ! 
Love your posts ! They are amazingly helpful .
Does the Python LSTM book have code snippets in  Python 3 as well?
Coz it becomes a little difficult to search for the right modules and attributes otherwise :/

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 4, 2017 at 4:39 am
	                #
	                

				

		   		

				THanks.
Yes, the code in my LSTM book was tested with Python 2.7 and Python 3.5.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Kaushal Shetty
	                September 8, 2017 at 12:24 am
	                #
	                

				

		   		

				Hi Jason, Is this a valid approach to decide the number of layers?
def neural_train(layer1 = 1,layer2 = 1,layer3 = 1,layers = 1):
    input_tensor = Input(shape=(2001,))
    x = Dense(units = layer1,activation=’relu’)(input_tensor)
    if layers == 2:
        x = Dense(layer2,activation = ‘relu’)(x)
    if layers ==3 :
        x = Dense(layer2,activation = ‘relu’)(x)
        x = Dense(layer3,activation = ‘relu’)(x)
    output_tensor = Dense(10,activation=’softmax’)(x)
    model = Model(input_tensor,output_tensor)
    model.compile(optimizer = ‘rmsprop’,loss=’categorical_crossentropy’,metrics = [‘accuracy’])
    return model
layer1 = [1024,512]
layer2 = [256,100]
layer3 = [60,40]
epochs = [10,11]
layers = [2,3]
param_grid = dict(epochs = epochs,layer1 = layer1,layer2 = layer2,layer3 = layer3,layers=layers)
model = KerasClassifier(build_fn = neural_train)
gsv_model = GridSearchCV(model,param_grid=param_grid)
gsv_model.fit(x_train,y_train)

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 9, 2017 at 11:45 am
	                #
	                

				

		   		

				Maybe, you must have a test harness that you can trust, then explore different configurations of your model.
I have more on robustly evaluating neural nets here:
https://machinelearningmastery.com/evaluate-skill-deep-learning-models/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                ari
	                September 9, 2017 at 1:29 am
	                #
	                

				

		   		

				Very helpful post Jason. Thanks for this. Are there any advantages for using gridsearch over something like hyperas/hyperopt ? To your best knowledge is one faster than the other?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 9, 2017 at 11:58 am
	                #
	                

				

		   		

				Depends on your data and model. Use the took that you prefer.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Shubham Kumar
	                September 10, 2017 at 4:38 am
	                #
	                

				

		   		

				{‘split0_test_score’: array([ 0.6641791,  0.6641791,  0.6641791,  0.6641791]), ‘split1_test_score’: array([ 0.65413534,  0.65413534,  0.65413534,  0.65413534]), ‘split2_test_score’: array([ 0.69924811,  0.69924811,  0.69924811,  0.69924811]), ‘mean_test_score’: array([ 0.6725,  0.6725,  0.6725,  0.6725]), ‘std_test_score’: array([ 0.01931902,  0.01931902,  0.01931902,  0.01931902]), ‘rank_test_score’: array([1, 1, 1, 1]), ‘split0_train_score’: array([ 0.67669174,  0.67669174,  0.67669174,  0.67669174]), ‘split1_train_score’: array([ 0.68164794,  0.68164794,  0.68164794,  0.68164794]), ‘split2_train_score’: array([ 0.65917602,  0.65917602,  0.65917602,  0.65917602]), ‘mean_train_score’: array([ 0.67250523,  0.67250523,  0.67250523,  0.67250523]), ‘std_train_score’: array([ 0.00963991,  0.00963991,  0.00963991,  0.00963991]), ‘mean_fit_time’: array([ 36.72573058,  37.0244147 ,  38.12670692,  40.71116368]), ‘std_fit_time’: array([ 0.4829061 ,  0.35207924,  0.13746276,  2.71443639]), ‘mean_score_time’: array([ 1.49508754,  1.76741695,  2.14029002,  2.67426189]), ‘std_score_time’: array([ 0.04907801,  0.11919153,  0.07953362,  0.13931651]), ‘param_dropout’: masked_array(data = [0.2 0.5 0.6 0.7],
             mask = [False False False False],
       fill_value = ?)
, ‘params’: ({‘dropout’: 0.2}, {‘dropout’: 0.5}, {‘dropout’: 0.6}, {‘dropout’: 0.7})}
Hey. I was hypertuning a model on 4 different choices of hyper parameters. However, in the grid_results_ dictionary, the rank_test_score key has array with all same values. I find that confusing. Shouldn’t it have 4 different values in each place?
Something like [1,3,2,4] ?
What could be the explanation for this?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shubham Kumar
	                September 10, 2017 at 4:50 am
	                #
	                

				

		   		

				It must have something to do with all mean_test_scores being the same ,

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 11, 2017 at 12:03 pm
	                #
	                

				

		   		

				If you are testing 4 different values for one parameter, then you must build 4 models/complete 4 runs.
Does that help?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shubham Kumar
	                September 13, 2017 at 5:20 am
	                #
	                

				

		   		

				I am sorry. That’s confusing. 4 models or complete 4 runs means ? 
Things are different if we are gridsearching/randomsearching just for one hyperparameter?
Does it have something to do with the actual code used to write TensorFlow /keras ?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 13, 2017 at 12:36 pm
	                #
	                

				

		   		

				If you have one parameter and you want to test 4 values, each value needs one run. Ideally, we would run many times for each parameter value and take the average skill score given the stochastic nature of ML algorithms.
For a random search, you run for as long as you like.
Does that help?

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Shubham Kumar
	                September 13, 2017 at 11:17 pm
	                #
	                

				

		   		

				What I understand is that when we have more than 1 (say 2) hyper-parameters in a grid, then for each combination, the code will complete as many epochs as I have specified, with as many training-cross-validation sets as specified (the CV in GridSearchCV). So, going through all those epochs, for each training-cross-validation set, we get the avg accuracy over all the cross-validation sets for every combination.
So when you say 1 run only in the case of a single hyperparameter, that means only 1 training-crossvalidation set? Because only in this case, there won’t be any averaging involved.
Is that what I have to do? Change the training-crossValidation set to just 1?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 15, 2017 at 12:06 pm
	                #
	                

				

		   		

				Yes, 1 run is one CV pass (k folds).

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Rishi
	                September 18, 2017 at 5:18 am
	                #
	                

				

		   		

				Jason,
would you please post an example of inheriting from KerasClassifier (or KerasRegressor) to create your own class? I’m attempting to do this and it works for the most part:
class MLP_Regressor(KerasRegressor):
    def __init__(self, **sk_params):
        super().__init__(build_fn=None, **sk_params)
    def __call__(self, optimizer=’adam’, loss=’mean_squared_error’, **kwargs):
        # more code goes here (that was previously in ‘build_fn’
I can include this in a pipeline and it runs perfectly:
MLP Pipeline(memory=None,
     steps=[(‘MLP’, )])
Only thing is: The Keras documentation includes the ‘build_fn’ keyword argument:
keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)
While the actual KerasClassifier class definition shows the following in its __init__ method:
 def __init__(self, model, optimizer=’adam’, loss=’categorical_crossentropy’, **kwargs):
        super(KerasClassifier, self).__init__(model, optimizer, loss, **kwargs)
I’m not sure if my __init__ in MLP_Regressor has been setup correctly (to avoid hidden bugs in the future).
Would greatly appreciate it! (I’ve searched, but couldn’t find a single example of KerasClassifier inheritance).

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 18, 2017 at 5:49 am
	                #
	                

				

		   		

				Thanks for the suggestion, I have not done this but perhaps in the future.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Rishi
	                November 21, 2017 at 12:54 pm
	                #
	                

				

		   		

				Jason, managed to get the inherited class working perfectly now:
class MLP_Classifier(KerasClassifier):
    def __init__(self, build_fn=None, **sk_params):
        self.sk_params = sk_params
        super().__init__(build_fn=None, **sk_params)
    def __call__(self, callbacks=None, layer_sizes=None,activations=None,input_dim=0,init=’normal’,optimizer=’adam’, metrics=’accuracy’, loss=’binary_crossentropy’, use_dropout_input=False, use_dropout_hidden=False):
        “””
        Constructs, compiles and return a Keras model
        Implements the “build_fn” function
        Returns a “Sequential” model
        “””
        # Code to build a model (that would typically go in “build_fn”) goes here.
        return model

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 22, 2017 at 10:47 am
	                #
	                

				

		   		

				Well done!

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Tmn
	                September 20, 2017 at 2:45 am
	                #
	                

				

		   		

				Hi Jason,
I can not thank you enough. I am sure that there are many people like me  who have learnt a lost from your tutorial on both “R” and “Python”. I have been following your tutorial for more than 3 year now. Before I was using R however, recently I moved to python for Deep learning. And I find your tutorial as usual, exceptional. I think Andrew Ng and  CS231n (andrej karpathy),  theoretical course and your programming course on deep learning is one of the best in the world. You rock! Thanks a lot.
I do have a question 🙂 as well.
The grid search parameter tuning works perfectly with CPU. I agree with your suggestion not to tune everything at once. Now I moved to GPU implementation. I was able to execute the code if I chose options n_job=1. However, if I do multi-threading n_job=-1. I am getting “CUDA_ERROR_OUT_OF_MEMORY”. I have GeForce GTX 1080. Did you happen to encounter  similar kind of error? I will post you the error log if needed.  
Once, again thank you.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 20, 2017 at 6:00 am
	                #
	                

				

		   		

				Thanks for all of your support!
Yes, I have the same and I would recommend using a “single thread” and let the GPU do its thing for a given single run.
In general, I’d recommend contrasting different approaches to grid searching (cpu/gpu) and use the approach that is overall faster for your specific tests.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Tmn
	                September 20, 2017 at 11:33 pm
	                #
	                

				

		   		

				Hi Jason,
Thank you for the response. The parameter search using CPU (n_job=-1) is (2.961489-4.977758) while using GPU (n_job=1) is (140.101048-142.151023) second.  
One more thing, after grid search I have value for parameters {batch_size, activation, neurons, learn_rate..} and accuracy around 90%. However, I wonder  why reusing these model parameter  does not provide the same results, now accuracy is 52%. Even though I executed it many times with same parameter the accuracy remains the same (52%). I could not achieve the accuracy as shown in grid search using best model parameter. I am doing 5-fold CV I do not expect the accuracy to be the same since it is stochastic process but it should be around SD±5%. What do you think? Did you also happen to encounter the same thing ?
Also the best parameter values changes in each executions with an accuracy SD±5%.
Thanks
P.S:
Below code is something I am doing to limit GPU memory usage and run multiple grid search. However, we should know the memory usage in advance (cs231n.github.io/convolutional-networks/#case). Let me know if it makes sense. 
Also, we can use n-job. I tried with n_job = 2 however the GPU memory is allocated based on fraction. I am searching how to allocated memory based on MB. I will do  more research on this “CUDA_ERROR_OUT_OF_MEMORY” and update you.
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.3
set_session(tf.Session(config=config))
Thanks!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 21, 2017 at 5:42 am
	                #
	                

				

		   		

				The results for the standalone model should fit into the distribution of the grid search results – if you repeated each grid search result many times, e.g. 10-30. See this post on evaluating model skill of neural networks:
https://machinelearningmastery.com/evaluate-skill-deep-learning-models/
Nice, sorry, I cannot give you good advice on grid searching with the GPU, it is not something I do generally. I am more likely to run instances serially or across AWS instances.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                TMN
	                October 6, 2017 at 2:12 am
	                #
	                

				

		   		

				Hi Jason,
Could you please help on how to do features normalization while doing the grid search and cross-validation. Is normalization is done automatically here, GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=15,cv=rkf)? If I normalize the features during training X = scaler.transform(X_train), this will introduce bias in cross-validation. Also, if possible, can you please provide me references on using scikit-learn wrapper with Keras for advance options, is their any limitation on wrapper ?
Thanks   
Without normalization:
Best: 0.535211 using {‘learn_rate’: 0.01, ‘dropout_rate’: 25, ‘batch_size’: 40, ‘neurons’: 200, ‘init_mode’: ‘lecun_uniform’, ‘optimizer’: ‘SGD’, ‘activation’: ‘relu’, ‘epochs’: 1000}
With normalization:
Best: 0.695775 using {‘optimizer’: ‘SGD’, ‘batch_size’: 132, ‘init_mode’: ‘lecun_uniform’, ‘epochs’: 1000, ‘learn_rate’: 0.01, ‘dropout_rate’: 25, ‘neurons’: 200, ‘activation’: ‘relu’}

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 6, 2017 at 5:37 am
	                #
	                

				

		   		

				Perhaps you can normalize your data prior to the grid search?

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                TMN
	                October 6, 2017 at 10:59 am
	                #
	                

				

		   		

				I normalize my data prior to grid search using X = scaler.transform(X_train) but dont you think it would introduce bias in the performance. Normally, I expect to normalize train set and use that normalization factor to normalize test or validation set before prediction. May be I did not understand you properly, how do you do normalization prior to grid search?
Thanks

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 6, 2017 at 11:07 am
	                #
	                

				

		   		

				Yes, it’s a struggle or trade-off.
Perhaps you can see if a Pipeline will work in the grid search, it may, but I expect it will error.
Perhaps the bias is minor and you can ignore it.
Perhaps you can implement your own grid search loop to only use training data to calculate data scaling coefficients.

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                TMN
	                October 6, 2017 at 6:44 pm
	                #
	                

				

		   		

				I started looking at the pipeline (http://scikit-learn.org/stable/modules/pipeline.html) on how they have been using it for SVM, lets see. I would expect the pipeline to work for Keras as well, as this is a classical problem in machine learning. Why do you expect error here?  I wanted to take the full advantage from automatic grid search. Well, the final option will be to implement my own grid search. 
The bias is really significant in 5-repeated 10-fold CV. Thanks
Without normalization:
Best: 0.535211 using {‘learn_rate’: 0.01, ‘dropout_rate’: 25, ‘batch_size’: 40, ‘neurons’: 200, ‘init_mode’: ‘lecun_uniform’, ‘optimizer’: ‘SGD’, ‘activation’: ‘relu’, ‘epochs’: 1000}
With normalization:
Best: 0.695775 using {‘optimizer’: ‘SGD’, ‘batch_size’: 132, ‘init_mode’: ‘lecun_uniform’, ‘epochs’: 1000, ‘learn_rate’: 0.01, ‘dropout_rate’: 25, ‘neurons’: 200, ‘activation’: ‘relu’}

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 7, 2017 at 5:51 am
	                #
	                

				

		   		

				If it works, that is great. I have seen cases where when grid search + keras gets fancy it causes errors.
I have a tutorial on Pipeline here that might help:
https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                HWU
	                September 22, 2017 at 6:52 am
	                #
	                

				

		   		

				This is such a great, thorough tutorial. Thanks for keeping your tutorials up to date! It’s so nice finding a resource with examples that you know will work because they’ve been tested on recent versions of required packages.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 23, 2017 at 5:32 am
	                #
	                

				

		   		

				Thanks!

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Marjan
	                September 29, 2017 at 1:08 pm
	                #
	                

				

		   		

				Thank you for your great tutorial.  I tried to use it for my model with multiple inputs. but It didn`t work. I found that the scikit-learn wrapper does not work for multiple inputs. it gives me an error for grid.fit([input1,input2],y)
Do you have any suggestion to handle it?
Thanks,

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                September 30, 2017 at 7:34 am
	                #
	                

				

		   		

				Sorry I do not. Perhaps run the grid search manually (e.g. your own for loop)?

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Buz Fifer
	                October 5, 2017 at 7:06 am
	                #
	                

				

		   		

				When I run your code to tune the dropout_rate, I get the following error:
ValueError: dropout_rate is not a legal parameter
In fact, I get this error for all labels except epochs and batch_size. Both of these were recognized and ran fine. I could not find a reference to valid labels anywhere, even in API docs. Any suggestions?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 5, 2017 at 5:16 pm
	                #
	                

				

		   		

				What do you mean by valid labels exactly?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Buz Fifer
	                October 6, 2017 at 3:02 am
	                #
	                

				

		   		

				Sorry, I should have included the code in the first place. I have added comments in the code to show exactly what I tried for each parameter. 
    #  ———— Define Keras Classifier Wrapper
    model1 = KerasClassifier(build_fn=kerasModel1, epochs=5, batch_size=10, verbose=0)
    # ———– define the grid search parameters
    mybatchs = [10, 20, 128]
    myepochs = [5, 10, 20, 50, 60, 80, 100]
    mylearn = [0.001, 0.002, 0.0025, 0.003]
    myopts = [‘Adam’, ‘Nadam’, ‘RMSprop’]
    myinits = [‘uniform’, ‘normal’, ‘lecun_uniform’, ‘lecun_normal’, ‘glorot_uniform’, ‘glorot_normal’]
    mydrop = [0.10, 0.20, 0.30, 0.35, 0.40, 0.50, 0.60, 0.70, 0.80]
# ————- Not Recognized
    #param_grid = dict(optimizer=myopts)
    #param_grid = dict(learn_rate=mylearn)
    #param_grid = dict(learning_rate=mylearn)
    #param_grid = dict(init=myinits)
    #param_grid = dict(init_mode=myinits)
    #param_grid = dict(dropout_rate=mydrop)
 # ———— Recognized
    #param_grid = dict(epochs=myepochs)            # —– OK
    #param_grid = dict(batch_size=mybatchs)        # —– OK
I removed comment # and ran each one separately. For example, running the first param_grid values resulted in: Error – optimizer is not a valid parameter. They all got the same rejection notice except for epochs and batch_size.
I hope that helps.

				
	                
	                    Reply	                

				

			

	



	      	

					                
	            
		      	

	                Buz Fifer
	                October 6, 2017 at 3:09 am
	                #
	                

				

		   		

				Just to be clearer, each parameter had it’s own name in the error message as follows:
Error – optimizer is not a valid parameter
Error – learn_rate is not a valid parameter
Error – learning_rate is not a valid parameter
Error – init is not a valid parameter
Error – init_mode is not a valid parameter
Error – dropout_rate is not a valid parameter

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 6, 2017 at 5:39 am
	                #
	                

				

		   		

				That is odd, I don’t have any good ideas, other than continue to debug and try different variations to see if you can expose the cause of the issue.
Double check all of your python libraries are up to date.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                ritika
	                October 6, 2017 at 11:49 pm
	                #
	                

				

		   		

				Hi Jason, Very nice tutorial..very well explained

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 7, 2017 at 5:55 am
	                #
	                

				

		   		

				Thanks.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                TC
	                October 17, 2017 at 10:27 am
	                #
	                

				

		   		

				Hi Jason thanks for the great post.  
Let’s say I’m using 5 fold CV on a relatively small dataset (not necessarily for a deep learning model).  In this case, the variance of the performance metric might be quite high, and just by chance, a point on the grid that is in reality far from optimal, might be selected as the “best”.  
So are there any approaches to smooth out the response surface of the grid search, to deal with “spikes” in performance due to variance?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 17, 2017 at 4:05 pm
	                #
	                

				

		   		

				Wonderful question.
Yes, we can approach this problem by increasing the number of repeats (not folds) of each param combination.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                TC
	                October 20, 2017 at 8:40 am
	                #
	                

				

		   		

				Hi Jason, by “number of repeats” do you mean to just repeat the process many times, with perhaps a different random seed?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 21, 2017 at 5:23 am
	                #
	                

				

		   		

				Exactly.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Lea
	                October 20, 2017 at 10:09 pm
	                #
	                

				

		   		

				Thank you for this great tutorial! I tried to adapt the code for a CNN, but I am running constantly in the same error. May anyone help? 
That is the code:
def create_model(nb_filters=3, nb_conv=2, pool=20):
    model = Sequential()
    model.add(Convolution1D(nb_filters, nb_conv, activation=’relu’,
                            input_shape=(X.shape[1], X.shape[2]), padding=”same”))
    model.add(MaxPooling1D(pool))
    model.add(Flatten())
    model.add(Dense(1, activation=’sigmoid’))
    model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])
    model.summary()
    return model
model = KerasClassifier(build_fn=create_model(), verbose=0)
nb_conv = [2, 4, 6, 8, 10]
pool= [10, 20, 30, 50]
param_grid = dict(nb_conv=nb_conv, epochs=pool)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
grid_result = grid.fit(X, y)
And the error I am getting is “nb_conv is not a legal parameter”. Unfortunately, I do not understand why.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 21, 2017 at 5:37 am
	                #
	                

				

		   		

				The API has changed:
https://keras.io/layers/convolutional/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                went
	                October 22, 2017 at 2:55 am
	                #
	                

				

		   		

				Hi Jason,
Great post and Thank you.
What do you think is the best sequence when tuning all those Hyperparameters? I think difference sequence will lead to difference final Hyperparameters..

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 22, 2017 at 5:32 am
	                #
	                

				

		   		

				This post has some ideas (at the end):
https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/
Also see the referenced paper.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Bgie
	                October 23, 2017 at 6:28 am
	                #
	                

				

		   		

				Hi Jason,
What a great blog, I very much appreciate you sharing some of your expertise!
I want to grid search the hyperparams from my CNN, but I’m using data augmentation with ImageDataGenerator. So I’m not calling model.fit but model.fit_generator for the actual training.
This does not seem to be supported through the grid search..
Am I forced to write my own KerasClassifier implementation?
Would you advise to just fall back to using (nested) for loops instead, or would I be missing some ‘magic’ from the existing scikit gridsearch?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 23, 2017 at 4:10 pm
	                #
	                

				

		   		

				I would recommend writing your own for loops to grid search instead.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Shubham Kumar
	                October 26, 2017 at 3:58 am
	                #
	                

				

		   		

				Hey Jason!
Needed help with model improvement!
Can you help me in understanding how to realize whether your model is suffering from
bad local minima, vanishing/exploding gradient problem?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 26, 2017 at 5:34 am
	                #
	                

				

		   		

				If you have exploding or vanishing gradients, then you will have NaN outputs.
This post will give you ideas on how to lift skill:
http://machinelearningmastery.com/improve-deep-learning-performance/
This post will give you advice on how to effectively evaluate your model:
https://machinelearningmastery.com/evaluate-skill-deep-learning-models/

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shubham Kumar
	                November 6, 2017 at 7:05 am
	                #
	                

				

		   		

				NaN outputs as in my predictions ?
Or the weights ?
If exploding gradient then weight will be very large (probably NaN) hence output would also be NaN.
But how will this logic be used for vanishing gradients. I  this case the weights basically stop changing r8?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Shubham Kumar
	                November 6, 2017 at 7:07 am
	                #
	                

				

		   		

				Should I use some kind of code that checks by how much the weights at each layer are changing…and if after a certain threshold they haven’t changed by a certain amount, I’ll declare vanishing gradient !

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 7, 2017 at 9:44 am
	                #
	                

				

		   		

				Try gradient clipping on the optimization algorithm.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Mustafa Murat ARAT
	                October 28, 2017 at 12:45 am
	                #
	                

				

		   		

				I have a question for you, Jason and for general audience. I tried to find optimal number of neurons for one of the hidden layers. i did loop over my function which contains my deep learning model. It is fast enough for the values I define and I get a result based on accuracy. However, when I use your code, it is extremely slow and never reached to an end. How long does it take on your computer?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 28, 2017 at 5:14 am
	                #
	                

				

		   		

				You could try to test fewer parameters or try to search on a smaller dataset?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Mustafa Murat ARAT
	                October 29, 2017 at 4:12 am
	                #
	                

				

		   		

				Hey Jason,
Thank you for your quick reply. I try grid search for number of neurons on Iris data set for the purpose of learning. I scale the data first and then transform and encode the dependent variable. However, first of all, even though I use small data set or fewer parameters, it is slow; second of all, when I get the results, it is all zero. This is very basic example and I am pretty much sure that my code is correct but I guess I am missing out something. 
Best: 0.000000 using {‘neurons’: 3}
0.000000 (0.000000) with: {‘neurons’: 3}
0.000000 (0.000000) with: {‘neurons’: 5}
THE CODE:
from pandas import read_csv
import numpy
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from keras.wrappers.scikit_learn import KerasClassifier
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from sklearn.model_selection import GridSearchCV
dataframe=read_csv(“iris.csv”, header=None)
dataset=dataframe.values
X=dataset[:,0:4].astype(float)
Y=dataset[:,4]
seed=7
numpy.random.seed(seed)
#encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
#one-hot encoding
dummy_y = np_utils.to_categorical(encoded_Y)
scaler = StandardScaler()
X = scaler.fit_transform(X)
def create_model(n_neurons):
    model = Sequential()
    model.add(Dense(n_neurons, input_dim=X.shape[1], activation=’relu’)) # hidden layer
    model.add(Dense(3, activation=’softmax’)) # output layer
    model.compile(loss=’categorical_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])
    return model
model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, initial_epoch=0, verbose=0)
# define the grid search parameters
neurons=[3, 5]
#this does 3-fold classification. One can change k.
param_grid = dict(n_neurons=neurons)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, dummy_y)
# summarize results
print(“Best: %f using %s” % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_[‘mean_test_score’]
stds = grid_result.cv_results_[‘std_test_score’]
params = grid_result.cv_results_[‘params’]
for mean, stdev, param in zip(means, stds, params):
    print(“%f (%f) with: %r” % (mean, stdev, param))

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 29, 2017 at 5:59 am
	                #
	                

				

		   		

				Sorry, I cannot debug your code/problem for you.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Mustafa Murat ARAT
	                October 30, 2017 at 8:29 am
	                #
	                

				

		   		

				I totally understand you. Thank you so much, though. I figured out my mistake. Iris dataset is very well balanced so I need to shuffle the data because GridSearchCV is using 3-Fold Cross Validation.

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                October 30, 2017 at 3:49 pm
	                #
	                

				

		   		

				Glad to hear it.

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                jenny
	                November 8, 2017 at 4:12 am
	                #
	                

				

		   		

				Thanks for sharing such a wonderful tutorial. Learnt many new things.
How can i save all the models that the grid search is generation with identifiers for each model?
I am an R user. This how I do it in R to save models with passing parameter values to its names.
xgb.object <- paste0('/path/xgb_disc20_new_',
 sample.sizes[i], '_', s,'_',nrounds[j],'_',max.depth[k],'_',eta[l], '.RData')
write.table(cbind(sample.sizes[i], s,nrounds[j],max.depth[k],eta[l],tpr, tnr, acc, roc.area, 
concordance),   paste0('/path/xgb_disc20_new_', min.sample.size,'_', max.sample.size, 
'.csv'),   append=TRUE, sep=",",row.names=FALSE,col.names=FALSE)
How can this be achieved in python for keras(neural network) and other models in other libraries?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 8, 2017 at 9:29 am
	                #
	                

				

		   		

				I would recommend using grid search to find the parameters for a well performing model then train a new standalone model with those parameters that you can then save.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                jenny
	                November 8, 2017 at 9:34 pm
	                #
	                

				

		   		

				thank you jason for your quick reply . I will try that way.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Wassim
	                November 16, 2017 at 11:26 pm
	                #
	                

				

		   		

				Hi Jason,
Thank you for the great tutorial. I just have an issue when using exactly your code: when I try to parallelize the grid search with n_jobs=-1, I end up with the error “AttributeError: Can’t get attribute ‘create_model’ on ” while it works well without parallelization. Any idea where the issue comes from?
Thank you,
Wassim

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 17, 2017 at 9:25 am
	                #
	                

				

		   		

				I’m not sure, perhaps you cannot parallelize the grid search with Keras models.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Sangwon Chae
	                November 28, 2017 at 9:51 pm
	                #
	                

				

		   		

				Hi Jason,
The example code calculates the best score for accuracy to obtain the hyperparameter.
In my problem, I want to find RMSE rather than accuracy because it is regression problem (numerical prediction).
However, ‘grid_result.cv_resluts_’ only provides ‘fit_time’ and ‘score’, so it can not calculate RMSE.
What should I do?
Thank you.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                November 29, 2017 at 8:23 am
	                #
	                

				

		   		

				You can change the configuration to calculate MSE (e.g. scoring=’neg_mean_squared_error’) and then take the square root.
Learn more here:
http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Estelle
	                December 5, 2017 at 7:37 am
	                #
	                

				

		   		

				Hi Jason,
Thank you for this post.
Is there anything that prevents me to use Grid Search with train_on_batch() instead of fit()?
Thank you for letting me know.
All the best,
Estelle

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 5, 2017 at 10:26 am
	                #
	                

				

		   		

				I think the wrapper is quite limited and does not offer this facility via sklearn.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Estelle
	                December 6, 2017 at 8:15 am
	                #
	                

				

		   		

				Thanks for your quick answer.
All the best,
Estelle

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 6, 2017 at 9:08 am
	                #
	                

				

		   		

				No problem.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Peter
	                December 8, 2017 at 1:56 pm
	                #
	                

				

		   		

				Thanks very much for the tutorial. It is extremely helpful for my work. I came across a problem with grid search with Keras (tensorflow backend). I want to run the same grid search on different datasets. Everything works fine on the first dataset. But when I fit the grid search to the second dataset, the program got stuck there. I run the grid search with n_jobs=-1 and put keras.backend.clear_session() between two fits. You can replicate this issue by fit to the data twice in your examples. Could you please kindly help me with this issue?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 8, 2017 at 2:30 pm
	                #
	                

				

		   		

				I’m sorry to hear that, perhaps change n_jobs to 1?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Peter
	                December 8, 2017 at 2:40 pm
	                #
	                

				

		   		

				Thanks for the quick reply. It works when n_jobs=1, but I do need parallel threads for speed.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 9, 2017 at 5:34 am
	                #
	                

				

		   		

				The neural network will be using all the cores, so running multiple threads may not offer any benefit.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Peter
	                December 11, 2017 at 9:53 am
	                #
	                

				

		   		

				I got it to work by just fitting one dataset in the python script and looping the python script over multiple datasets in a bash script. I am still not clear why second fitting fails in python, but this is a not-so-beautiful workaround.

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 11, 2017 at 4:52 pm
	                #
	                

				

		   		

				Glad to hear that you made some progress.

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                Daniel Pamplona
	                December 13, 2017 at 1:37 am
	                #
	                

				

		   		

				Hi Jason
Thank you so much for sharing your knowledge.
I am trying to optimize the number of hidden layers.
I can´t figure it out how to do it with keras (actually I am wondering how to set up the function create_model in order to maximize the number of hidden layers)
Could you please help me?
Thank you

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 13, 2017 at 5:42 am
	                #
	                

				

		   		

				Perhaps the number of layers could be a parameter to your function.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Sean
	                December 15, 2017 at 1:43 am
	                #
	                

				

		   		

				Hi Jason,
Thanks for this insightful and useful tutorial as always
No doubt your blog posts are arguably the best in the field of data sciences
Best wishes

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 15, 2017 at 5:36 am
	                #
	                

				

		   		

				Thanks Sean.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Sean
	                December 16, 2017 at 12:06 am
	                #
	                

				

		   		

				Hello Jason,
I decided to try the code on a textual data of about 3000 tweets having binary classification (Y) and the text corpus as (X). Started off with tuning the batch size and number of epochs
but got the following error:

		
		
			
			
			
			
Name: 0, Length: 2185, dtype: object, names=['dense_1_input'], shapes=[(None, 8)], check_batch_axis=False, exception_prefix='input')
    148                         raise ValueError(
    149                             'Error when checking ' + exception_prefix +
    150                             ': expected ' + names[i] +
    151                             ' to have shape ' + str(shapes[i]) +
    152                             ' but got array with shape ' +
--> 153                             str(array.shape))
        array.shape = (2185, 1)
    154     return arrays
    155 
    156 
    157 def _standardize_sample_or_class_weights(x_weight, output_names, weight_type):

ValueError: Error when checking input: expected dense_1_input to have shape (None, 8) but got array with shape (2185, 1)
			
				
					1234567891011121314
				Name: 0, Length: 2185, dtype: object, names=['dense_1_input'], shapes=[(None, 8)], check_batch_axis=False, exception_prefix='input')    148                         raise ValueError(    149                             'Error when checking ' + exception_prefix +    150                             ': expected ' + names[i] +    151                             ' to have shape ' + str(shapes[i]) +    152                             ' but got array with shape ' +--> 153                             str(array.shape))        array.shape = (2185, 1)    154     return arrays    155     156     157 def _standardize_sample_or_class_weights(x_weight, output_names, weight_type): ValueError: Error when checking input: expected dense_1_input to have shape (None, 8) but got array with shape (2185, 1)
			
		


Here’s the modified code below:


		
		
			
			
			
			
# Use scikit-learn to grid search the batch size and epochs
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
# Function to create model, required for KerasClassifier
def create_model():
	# create model
	model = Sequential()
	model.add(Dense(12, input_dim=8, activation='relu'))
	model.add(Dense(1, activation='sigmoid'))
	# Compile model
	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
# fix random seed for reproducibility
seed = 7
np.random.seed(seed)
# load dataset
dataset = pd.read_csv('training.txt', delimiter = '\t', quoting = 3, header = None)
# split into input (X) and output (Y) variables
X = dataset.iloc[:,0]
Y = dataset.iloc[:,1]
# create model
model = KerasClassifier(build_fn=create_model, verbose=0)
# define the grid search parameters
batch_size = [10, 20, 40, 60, 80, 100]
epochs = [10, 50, 100]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(X, Y)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
			
				
					123456789101112131415161718192021222324252627282930313233343536373839
				# Use scikit-learn to grid search the batch size and epochsimport pandas as pdimport numpy as npfrom sklearn.model_selection import GridSearchCVfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifier# Function to create model, required for KerasClassifierdef create_model():	# create model	model = Sequential()	model.add(Dense(12, input_dim=8, activation='relu'))	model.add(Dense(1, activation='sigmoid'))	# Compile model	model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])	return model# fix random seed for reproducibilityseed = 7np.random.seed(seed)# load datasetdataset = pd.read_csv('training.txt', delimiter = '\t', quoting = 3, header = None)# split into input (X) and output (Y) variablesX = dataset.iloc[:,0]Y = dataset.iloc[:,1]# create modelmodel = KerasClassifier(build_fn=create_model, verbose=0)# define the grid search parametersbatch_size = [10, 20, 40, 60, 80, 100]epochs = [10, 50, 100]param_grid = dict(batch_size=batch_size, epochs=epochs)grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)grid_result = grid.fit(X, Y)# summarize resultsprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))means = grid_result.cv_results_['mean_test_score']stds = grid_result.cv_results_['std_test_score']params = grid_result.cv_results_['params']for mean, stdev, param in zip(means, stds, params):    print("%f (%f) with: %r" % (mean, stdev, param))
			
		

Thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 16, 2017 at 5:29 am
	                #
	                

				

		   		

				Sorry to hear that, it’s not clear to me. Perhaps post to stackoverflow to get help debugging your code?

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Olivier Blais
	                December 16, 2017 at 3:24 am
	                #
	                

				

		   		

				Hi Jason, first thanks for your articles! Super useful!
I tried to execute the gripsearch but cam up with parallelism issues. I have a Windows OS and I get this error when I try to run the script on multiple cpus:
ImportError: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using “if __name__ == ‘__main__'”. Please see the joblib documentation on Parallel for more information.
Do you know how I should address that?
Thanks in advance

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 16, 2017 at 5:35 am
	                #
	                

				

		   		

				Perhaps try setting the number of jobs to 1?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Olivier Blais
	                December 28, 2017 at 5:30 am
	                #
	                

				

		   		

				Hi Jason! Yes this works but it is very slow as this is not parallel. Do you understand why it cannot run in parallel and how to fix that?
Thanks again !
Olivier

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 28, 2017 at 2:10 pm
	                #
	                

				

		   		

				The backend is parallelized and the two levels of parallelization are in conflict.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Shabnam
	                December 18, 2017 at 2:21 pm
	                #
	                

				

		   		

				Thanks a lot for such a wonderful post. Overall, there are a lot of parameters that need to be tuned. I was thinking to use RandomizedSearchCV instead of GridSearchCV. Still, it will be time consuming for a lot of simulations. Do you have any suggestion for fast parameter tuning? For example, can we say that specific parameters have more effect on scores, so lets try to Grid/RandomizedSearchCV them first?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 18, 2017 at 3:32 pm
	                #
	                

				

		   		

				Yes, there are some great tips at the end of this post:
https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Henry
	                December 20, 2017 at 10:51 pm
	                #
	                

				

		   		

				Dear Jason,
Fantastic post, thank you for this wonderful tutorial.
I was wondering if it would be more appropriate to tune all the hyperparameters at one go instead of breaking it up into various parts as shown above – you may be doing it for the sake of visibility of how each component is tuned but would it be better to tune everything together since there might be “interactions between the hyperparameters” which would not be captured if they were tuned separately?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                December 21, 2017 at 5:25 am
	                #
	                

				

		   		

				If you have the resources, then sure.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Hao
	                January 3, 2018 at 2:26 am
	                #
	                

				

		   		

				Hi Jason,
Many thanks for a series of excellent posts!
I have an extremely imbalanced data set to study, of which #negative : #positive is about 100:1. When I built the first model, I performed 10-fold validation and in each validation round, I use oversampling to add positive samples on training data, but not on testing data. Now I question is: if I want to perform hyperparameter search, how do I tell GridSearchCV() to do  oversampling for each round of cross-validation?
Many thanks

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 3, 2018 at 5:40 am
	                #
	                

				

		   		

				Good question, you might need to use a Pipeline and have data prep happen within it.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Justin Solms
	                January 7, 2018 at 11:24 pm
	                #
	                

				

		   		

				Hello Jason
A good 2018 to you. I have a question about how Keras early stopping callbacks might be able to use the GridSearchCV k-fold generated validation data set as their val_loss or val_acc. The question I posted on StackOverflow but I wished to call your attention to it – should you so wish. 
https://stackoverflow.com/questions/48127550/how-do-i-implement-early-stopping-with-keras-and-the-sklearn-gridsearchcv-cross  
Kind regards,
Justin

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 8, 2018 at 5:43 am
	                #
	                

				

		   		

				I would suggest not combing CV and early stopping.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                James
	                March 11, 2018 at 6:16 am
	                #
	                

				

		   		

				Could early stopping be used as a substitute for grid searching epoch size?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 11, 2018 at 6:30 am
	                #
	                

				

		   		

				Yes, but you might need to code it up yourself. sklearn might blow up.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                shwetabh shekhar
	                January 19, 2018 at 12:03 am
	                #
	                

				

		   		

				Hello sir
if i have large dataset the also we can do this hyperparameter tunning .
If i have 70 to 80 feature column and about 50000 rows.
can we apply this tunnig

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 19, 2018 at 6:31 am
	                #
	                

				

		   		

				Sure, you might need a large computer or to split the work up across many computers.
Perhaps you can work with a sample of your data.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                shwetabh shekhar
	                January 19, 2018 at 1:13 am
	                #
	                

				

		   		

				how to select the hidden layer if i have largedataset mentioned as above

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 19, 2018 at 6:33 am
	                #
	                

				

		   		

				I don’t follow, what do you mean exactly?

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Kafeel Basha
	                January 29, 2018 at 5:48 pm
	                #
	                

				

		   		

				Very good post.
Hyper Parameter Tuning: How can I do grid search on number of neuron/epochs or batch size using Keras interface in R.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                January 30, 2018 at 9:48 am
	                #
	                

				

		   		

				Sorry, I don’t have an example in R.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                neha
	                February 2, 2018 at 6:34 am
	                #
	                

				

		   		

				Hi,I am facing a basic query where i have training and test set.i built lstm on training and using history = model.fit(trainX, trainY, epochs=100, batch_size=50,
                    validation_data=(testX, testY), verbose=0, shuffle=False) to fit my model.
After this i tried to model.predict(testX) to get predicted Y values.Now that was basic code.i am now trying to apply gridsearch.what variation in the history statement code i have to make to apply grid =
GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
grid_result = grid.fit(testX, testY, verbose=0, shuffle=False)

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                neha
	                February 2, 2018 at 6:50 am
	                #
	                

				

		   		

				can gridsearchcv work for time series as well?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 2, 2018 at 8:25 am
	                #
	                

				

		   		

				Not really. You will have to write your own for loops and perform walk forward validation.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Jack
	                February 2, 2018 at 8:01 pm
	                #
	                

				

		   		

				Hi Jason, thank you for your great tutorial! My question here is about ‘grid_result.best_score’. In this article the best score seems to be the best mean score, but in a regression problem, the mean score is irrelevant, so I have to look for the best std score. Is that correct?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 3, 2018 at 8:35 am
	                #
	                

				

		   		

				Mean score in regression will be mean error. Not irrelevant.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jack
	                February 3, 2018 at 8:15 pm
	                #
	                

				

		   		

				I see. But when I run the code, the ‘grid_result.best_score’ printed out the biggest score. I don’t think that’s right, cause in a regression problem I should look for the smallest mean error. Am I understanding this right?
Below are the results:
Best: 0.062234 using {‘optimizer’: ‘Nadam’}
0.059561 (0.017101) with: {‘optimizer’: ‘SGD’}
0.056818 (0.013662) with: {‘optimizer’: ‘RMSprop’}
0.059617 (0.014734) with: {‘optimizer’: ‘Adagrad’}
0.061506 (0.014503) with: {‘optimizer’: ‘Adadelta’}
0.059331 (0.014835) with: {‘optimizer’: ‘Adam’}
0.057696 (0.014828) with: {‘optimizer’: ‘Adamax’}
0.062234 (0.010834) with: {‘optimizer’: ‘Nadam’}

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 4, 2018 at 5:09 am
	                #
	                

				

		   		

				Yes, for regression it should be the smallest error. Are you using negative mse as the score function?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jack
	                February 5, 2018 at 6:52 pm
	                #
	                

				

		   		

				I’m not sure. I just copied the codes from this tutorial and changed ‘KerasClassifier’ to ‘KerasRegressor’.I didn’t make any change other than that.  I don’t understand how score function works and I’m not familiar with the concept of negative mse. Would you please elaborate?

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 6, 2018 at 9:12 am
	                #
	                

				

		   		

				You must specify a scoring function in sklearn, learn more about the API here:
http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
Here are examples:
http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                Mohamed Abd-Allah
	                February 4, 2018 at 9:09 am
	                #
	                

				

		   		

				very good tutorial, But I have a small question. can I tune all these hyperparameters together or I should take a part of the dataset and tune them separately like the examples you mentioned.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 5, 2018 at 7:43 am
	                #
	                

				

		   		

				Ideally, you would tune them all together, but this is often to computationally expensive.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Vidar
	                February 8, 2018 at 7:40 am
	                #
	                

				

		   		

				Is there a way to do similar things in R using the Caret package? Or other package that can help you with hyperparameter grid search when using Keras in R?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 8, 2018 at 8:33 am
	                #
	                

				

		   		

				I don’t know if Keras and caret are compatible, sorry.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                joseph
	                February 8, 2018 at 3:17 pm
	                #
	                

				

		   		

				hi Jason,
do i need to split the training data for cross validation, or only perform splitting on the input data.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 9, 2018 at 8:59 am
	                #
	                

				

		   		

				Why do you want to split exactly? You goals will help me answer your question.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                joseph
	                February 9, 2018 at 10:36 am
	                #
	                

				

		   		

				Thanks Jason for the quick reply…i will figure that out.. Just another minor question, is there any way to perform data preprocessing on 3d input (due to the input shape for lstm)

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 10, 2018 at 8:49 am
	                #
	                

				

		   		

				Sure, but it might be easier (or make more sense) to perform data prep prior to shaping data for the LSTM.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                joseph
	                February 10, 2018 at 12:12 pm
	                #
	                

				

		   		

				Thanks Jason..i will try that out.. Is it a good idea to tune the hyperparameter using the keras wrapper, then apply those tuned parameters on lstm model? Hope to get some comments on it. Thank you.

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 11, 2018 at 7:51 am
	                #
	                

				

		   		

				You can. Or you can write your own for loop and tune the model directly.

				
	                
	                    	                

				

			

	

	      	

					                
	            
		      	

	                joseph
	                February 12, 2018 at 1:10 pm
	                #
	                

				

		   		

				Thanks a lot Jason.. i will definitely try that one out..

				
	                
	                    	                

				

			

	





	      	

					                
	            
		      	

	                Boris Branson
	                February 15, 2018 at 7:27 am
	                #
	                

				

		   		

				Hi Jason, wonderful post. I love your books – amazing.
I wish to include callbacks in the Grid Search (one for TensorBoard and one for logging losses on every combination over the params).
I have something like:
loggerCB = keras.callbacks.TensorBoard(log_dir=’logs’, histogram_freq=0, write_graph=True)
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get(‘loss’))
historyCB = LossHistory()
grid_search = GridSearchCV(estimator=model,
                           param_grid=fit_params,
                           scoring=’accuracy’,
                           cv=10)
grid_search = grid_search.fit(X_train, y_train, fit_params={‘callbacks’: [loggerCB, historyCB]})
BUT I got this error:
TypeError: Unrecognized keyword arguments: {‘fit_params’: {‘callbacks’: [, ]}}
How can I pass callbacks using Grid Search?
Thanks,
Boris Branson

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 15, 2018 at 8:52 am
	                #
	                

				

		   		

				ry, I have not used callbacks with a grid search. You might need to write your own for-loops for the search.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Alessandro
	                February 17, 2018 at 4:02 am
	                #
	                

				

		   		

				Hello Jason,
let me congratulate for the good post.
I am curious about the use of CV . Each time you call
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)
you are compiling a new keras model with the new set of parameters. 
Are these different models of keras, compiled one after another, accumulating in the memory? Would this imply a memory usage problem in the case of an extensive grid search with bigger models? Any tips?
Best,
Alessandro

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 17, 2018 at 8:50 am
	                #
	                

				

		   		

				Yes, each model is evaluated and discarded.
For larger models, you could run each fold on a different machine (e.g. run the eval manually).

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Boris Branson
	                February 23, 2018 at 9:26 pm
	                #
	                

				

		   		

				Hello Jason,
I see you have used only SGD in the example of learning rate parameterization. Is it possible to combine different values for lr with different optimizers (not only SGD) in one grid search or i’d need a for loop?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 24, 2018 at 9:11 am
	                #
	                

				

		   		

				Yes, but the more parameters you grid search at once, the slower the search.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Priyansh
	                February 25, 2018 at 7:39 pm
	                #
	                

				

		   		

				I Jason your article is super useful, but I am having problem using it for MNIST data set which is a three dimensional data set , When I try so ‘fit’ this one gives me error, Dimension error. Can you do one for MNIST data set. Thanks a lot

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 26, 2018 at 6:05 am
	                #
	                

				

		   		

				Perhaps try this tutorial:
https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                TonyWang
	                February 27, 2018 at 10:20 pm
	                #
	                

				

		   		

				Hi Jason, Great tutorial, always learn a lot from your post. I have question, is it possible to combine all the parameters and with gridsearch? Seems more than thousands of combinations. For some models it will cost few days or weeks. Is there any better solution for this? randomgridsearch or something else? Thanks again!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                February 28, 2018 at 6:04 am
	                #
	                

				

		   		

				Yes, but as you say, you will need a lot of time or a lot of parallel compute resources to get a result.
Random search is often preferred because you can uniformly sample the domain and get good enough results quickly.

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                TonyWang
	                March 1, 2018 at 3:18 am
	                #
	                

				

		   		

				Thanks for your reply. Googled a lot but didn’t find any method to search optimizers and their params, say different optimizer, adam and it’s learning rates. Is there any suggestions? Thanks!

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 1, 2018 at 6:16 am
	                #
	                

				

		   		

				Yes, just start searching for viable params on your model/data. No need to find confirmation.

				
	                
	                    Reply	                

				

			

	




	      	

					                
	            
		      	

	                Johnson Muthii
	                March 2, 2018 at 8:56 am
	                #
	                

				

		   		

				Hello Jason,
Thanks for this awesome tutorial. Am very fresh in machine learning and your tutorials are so simplified and easy to follow.
Am encountering an error when i run the  epochs and batch size tuning code. Kindly help
This the code part bringing the error…
# create model
model = KerasClassifier(build_fn=create_model, verbose=0)
# define the grid search parameters
batch_size = [10, 20, 40, 60, 80, 100]
epochs = [10, 50, 100]
param_grid = dict(batch_size=batch_size, epochs=epochs)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs= 1)
grid_result = grid.fit(X_train, y_train)
TypeError: __call__() missing 1 required positional argument: ‘inputs’

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 2, 2018 at 3:20 pm
	                #
	                

				

		   		

				Sorry, I have not seen this error. Are you able to confirm that you have copied all of the code and that your development environment is up to date?

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                sonia
	                March 7, 2018 at 2:05 am
	                #
	                

				

		   		

				dear jason
how much time this program run while tunning ?like tuning epoch and batch size?

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 7, 2018 at 6:16 am
	                #
	                

				

		   		

				It depends on the size of the dataset, the size of the model and the speed of your system.

				
	                
	                    Reply	                

				

			

	


	      	

					                
	            
		      	

	                Yumlembam Rahul
	                March 12, 2018 at 1:59 pm
	                #
	                

				

		   		

				Hi,
As you mention in your blog “As we proceed through the examples in this post, we will aggregate the best parameters. This is not the best way to grid search because parameters can interact, but it is good for demonstration purposes.” does this mean we should so the hyper parameter search in one grid instead of dividing.
regrads,
Yumlembam Rahul

				
	                
	                    Reply	                

				

			

	

	      	

					                
	            
		      	

	                Jason Brownlee
	                March 12, 2018 at 2:28 pm
	                #
	                

				

		   		

				Ideally, if you have the time and resources.

				
	                
	                    Reply	                

				

			

	

		 		
		Leave a Reply Click here to cancel reply.			
				Comment Name (required) 
Email (will not be published) (required) 
Website
 

			
			
	     
            
                
            
Welcome to Machine Learning Mastery
Hi, I'm Jason Brownlee, Ph.D.

My goal is to make practitioners like YOU awesome at applied machine learning.
Read More

			
Finally Get Started With Deep Learning
Sick of the fancy math and need for super computers?
Looking for step-by-step tutorials?
Want end-to-end projects?
Get Started With Deep Learning in Python Today!



		
		 		

            Popular

            

            

	            
                                
				Your First Machine Learning Project in Python Step-By-Step
		June 10, 2016
		
	
				Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras
		July 21, 2016
		
	
				Multivariate Time Series Forecasting with LSTMs in Keras
		August 14, 2017
		
	
				How to Setup a Python Environment for Machine Learning and Deep Learning with Anaconda
		March 13, 2017
		
	
				Develop Your First Neural Network in Python With Keras Step-By-Step
		May 24, 2016
		
	
				Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras
		July 26, 2016
		
	
				Time Series Forecasting with the Long Short-Term Memory Network in Python
		April 7, 2017
		
	
				Regression Tutorial with the Keras Deep Learning Library in Python
		June 9, 2016
		
	
				Multi-Class Classification Tutorial with the Keras Deep Learning Library
		June 2, 2016
		
	
				How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras
		August 9, 2016
		
	
                                                                
            

        

                 

		         

		
    
	
	

		
		
			© 2018 Machine Learning Mastery. All Rights Reserved. 		

		
			
Privacy | 
Contact |
About