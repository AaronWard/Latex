\chapter{Literature Review}
This chapter will review five papers in relation to the topics of facial detection and analysis use cases, the use cases of artificial intelligence in marketing and the process of building neural networks. A critical analysis shall be given on the strengths and downfalls of each paper and the insight shall be given into the relevancy of the findings in each paper to the proposed project.

\section{Analyzing and Detecting Employee's Emotion for Amelioration of Organizations}

\citeauthor{SUBHASHINI} make the opening statement that emotions usually do not take any place in a work environment in current society. Although the expression of feelings is suppressed in places of work, they suggest that emotions can affect five major areas in competitive advantage. The five given aspects of competitive advantage are as follows: Intellectual Capital, Customer Service, Organizational Reactivity, Production, Employee appeal and retentivity \citep{SUBHASHINI}. In order to counter this apprehensiveness to expression of emotions in the work place, \citeauthor{SUBHASHINI} suggest the concept of a facial emotion tracking system that will map the facial expressions of an employee's face as they enter the company building. 

The system architecture given by \citeauthor{SUBHASHINI} briefly describes logistics of the program. Most employees entering a building to an organization must swipe a card to clock into the work hours. They suggest that they have designed a new system that removes the need for card swiping, by performing facial recognition. Not only this, but the system also implements emotion detection. The employee looks into a camera that will prove their presence in the building but will also perform some emotion detection. The system was implemented in the C Sharp programming language and uses skin tone segmentation to detect facial features. The binary image is then converted to an RGB image and an inspection of every individual pixel if performed. If the RGB value is greater than 110, then the pixel colour is refactored to be a white pixel, otherwise it becomes a black pixel. This is done to make it easy to detect facial features in the video stream \citep{SUBHASHINI}. Once detected, the image around the face is cropped. They then apply a Bezier Curve to the regions around the lips and eyes of the person being analysed. The results of the person's identity and emotional status are then stored within a database. They conclude their paper by explaining that this system can be used by management to gain an understanding of their employee's sentimental state. 
\\

\section{Deep Learning for Video Classification and Captioning}
\citeauthor{Wu}'s paper provides an in-depth analysis on the methods for video classification and video captioning in terms of deep learning. They claim that because of the exponential growth in internet bandwidth and computing power, video communications are becoming more and more prevalent, therefore paving the way for new video understanding applications \citep{Wu}. They refer to current implementations to prove the growth of interest in the field of computer vision and video analysis, notably the ImageNet challenge. 

Move over, they go on to give brief description of the two "deep learning modules" that have been used for visual analysis: Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). It is explained that LeCun et al. that developed LeNet-5 made a break through when they developed a CNN using the Back-Propagation Algorithm. But its noted that this is limited in performance when the complexity of the tasks is increased. Deep belief networks were developed to train networks in a unsupervised manner in order to counter this problem \citep{Wu}. AlexNet, a CNN proposed by Krizhevsky et al. in 2012, introduced two way to increase the performance of CNN's using the Rectified Linear Units(ReLU) activation function and dropout to decrease overfitting \citep{Wu}. Secondly, RNN's are brought forward. \citeauthor{Wu} explain the difference between CNN's and RNN's, stating that CNN's are all feed forward networks that do not use cycling, which can prove be disadvantageous when working with sequence labeling. Two issues can occur with RNN's: Vanishing Gradient and Exploding Gradients as short-term memory is used when cycling through the network. The solution given to this is an RNN variant called Long Short-Term Memory (LSTM). \\
\subsection{Image-Based Video Classification using CNN's and LSTM's}
They state that  Karparthy et al. researched the common architectures for learning spatial-temporal clues in large video datasets. It appeared that models using single frames as input achieve similar results as models using stacks of frames. From this, Simonyan and Zisserman proposed the idea of the Two-Stream Approach, because of the cost effectiveness and time consumption that come with training 3D CNN's. This Two-Stream approach involves training the CNN on single and stack frames concurrently. Both outputs are put through a score fusion. The result is the weighted sum of both scores \citep{Wu}. 

Although Two-Stream is a good approach, it is not sufficient as it is not capable of dealing with long video clips. Therefore, LSTM's are utilized as they do not suffer from the problem of vanishing gradients. It has been found that CNN's and LSTM's complement each other when working in conjunction with each other \citep{Wu}. They conclude their paper with a summary of the written topics about, regarding the growth for the need for video understanding applications, the used of CNN's and RNN's, in addition to the variants of these deep learning modules. 

\section{Subject independent facial expression recognition with robust face detection using a convolutional neural network}
As stated by \citeauthor{MATSUGU}, difficulties may arise with facial recognition. In terms of a face being in a smiling-like state, could have different implications. As well as this, a facial recognition system should be able to work with a wide range of variability of faces. They then give some examples of past implementations such as facial recognition with rigid head movement by Black and Yackoob in 1995 and speak about how this does not meet the requirements of dealing with wide variance. A rule based system is proposed \citep{MATSUGU}. With their model, layer trains on a module-by-module (module being the nose, eyes, mouth etc) basis. Meaning each layer trains on a certain facial feature. Each of the neurons perform an averaging of some local receptive fields then they use a skin tone detector to detect each module on the face \citep{MATSUGU}. For training, Layer one and layer two are trained for 8 modules using back propagation. Layer three and four train on more complex feature detectors such as the mouth and eyes. The output is then sent to the rule based algorithm for handling variability and robustness. \\
The rule based algorithm takes the output of the CNN and measures the distance between the features. From these calculation, the rules are applied to determine if the person is in a laughing or smiling state. The rules are summarised as follows \citep{MATSUGU}:
\begin{itemize}
	\item The distance between eyes and lip get shorter.
	\item The horizontal length of lip gets longer.
	\item The eyes wrinkle.
	\item The gradient of lip from the end point to the end point increases.
	\item Detection of teeth increases.
	\item The edges (wrinkles) of cheek increase.
\end{itemize}
In conclusion, they received a 97.6 percent accuracy for 10 test subjects with 5600 images. They assert that their model is significantly more efficient as they only require one CNN due to their rule based algorithm, in contrast to Fasels implementation that uses two CNN's working in synergy with one another.

\section{Neuromarketing - The Art and Science of Marketing and Neurosciences Enabled by IoT Technologies}
A paper by \citeauthor{arthmann} describes the growing field of Neuromarketing with an opening statement: Advertisers recognise that there is a relationship between stimulating the emotions of a customer and influencing their actions. Online shopping has drastically affected the store sales, and it is also explained that more than 3500 stores have shut down due to bankruptcy in 2017 \citep{arthmann}. Their answer to this change in buying is Neuro Linguistic Programming(NLP). Neuro Linguistic Programming, not to be confused with natural language processing, is a method of observing the verbal and non-verbal communication of humans. Eye accessing cues (eye movements) are said to be linked to certain emotions or thoughts. Neuromarketing incorporates NLP and IoT devices to understand the consumer sentiment more extensively. Neuromarketing aims to remove marketing biases by utilizing the consumers subconscious. One example given by \citeauthor{arthmann} is the notion of facial coding and motion tracking, which is put in place to determine why consumer make certain decisions. 
Furthermore, they make their belief clear of retailers benefiting from this when it is put forward that artificial intelligence and machine learning will evolve, giving better results of consumer preference shifts. Additionally, these neuromarketing systems can replace thermoimaging people counting devices that clock people walking into stores that may not be eligible customers (children). Further examples are provided for these technologies. They go on to describe a scenario when a customer is given an image of a product and a price in front of a webcam, and by performing facial coding and sentiment analysis, we may get a better sense of what the consumer is feeling.
\citeauthor{arthmann} conclude their paper by declaring the future potential of these systems using when integrated with "always on" IoT technologies. 

\section{Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order}
\citeauthor{LOPES} open their paper by explaining the definition of facial expression. They describe it as facial changes that occur with someone due to emotional state or social communication. In terms of facial expression recognition software, a lot of systems give misleading accuracy results due to the overlapping of training and test data. They explain that some problems may occur. For example: dealing with ethnicity and variability of faces. Their response to this is training with one data set and testing with another to provide more accurate results \citep{LOPES}. They make reference to Liu et al's work on facial recognition by describing the three stages of training: Feature learning, which is responsible for extraction of all facial features. Secondly, Feature Selection, that selects the best features to learn. And lastly, the classifier, that each expression has one specifically allocated to. \citeauthor{LOPES} then move on to explain convolutional neural networks (CNN) on a high level. Firstly, the CNN is comprised of a convolution layer, that if given a kernel size. This kernel shifts over the image given to generate a map. This is followed by sub-sampling. Sub-sampling is used to reduce the map size to increase the accuracy in variance. Lastly, the fully-connected layer is introduced. This is a neural network that has fully-connected neurons to it's previous layer \citep{LOPES}. They explain that of all the facial recognition methods, CNN's prove to be  the most advantageous because they can use raw image data input for an accurate prediction. 

A related work section is then brought forward to explain similar models that have been developed. Much progress has been made in the field of neural networks have come about in recent years. This is due to advances in GPU technologies and computing power \cite{LOPES}. One example of relevant work they provide is the work done by Song et al., in which a CNN was developed for a mobile phone application for facial expression recognition. This CNN used image augmentation techniques, due to the lack of public data for their network, to prevent over-fitting. This increases the amount of data available for training the network \citep{LOPES}. Song et al. received and accuracy of 99.2 percent using the CK+ dataset. A CNN with 15 layers was developed by Burkert et al that achieved similar result, of 99.6 percent. They point out that although this network achieved high results, they may prove to be misleading as is it not made clear that their training and test datasets were different \citep{LOPES}. They then go on to compare the related works by expressing the flaws of using over-lapping data and the lack of emotion expressions classified.  

\subsection{Facial Expression Recognition System}
Following the introduction and related work, \citeauthor{LOPES} provide a prerequisite understanding for their model. The implementation of their model has two stages: the training stage and the testing stage. The training stage consists of a few preliminary steps. Firstly, new images are made from existing ones in the dataset to increase the trainable data. This is done using a method proposed by Simard et al. called "synthetic sample generation", which involves rotating and skewing the existing images. For every photograph that exist, and additional 70 are made, adding noise to the data. This synthetic data is only used in the training stage and advantageous as it allows the model to handle variance in an image \citep{LOPES}. Secondly, to address the problem in alignment with of facial features, the notion of rotation correction is introduced that aligned the images using the eyes as the horizontal axis. Image cropping is then used to reduce the amount of background noise as it is said to decrease the accuracy and overall performance of the CNN. This is done by detecting only features that are valid of expression classification and cropping the image around them, excluding the neck, ears and background from the image.
Down sampling is then applied for reducing the size of the image, making it 32 x 32 pixels in size. Brightness and contrast can cause problems with images, therefore intensity normalisation is applied to lower these aspects of the image \citep{LOPES}. \\
As for the testing stage, the same methods are used as the training stage. The CNN outputs the predicted emotional expression with the following number ID's:\\
\textbf{0} - Angry\\
\textbf{1} - Disgust\\
\textbf{2} - Fear\\
\textbf{3} - Happy\\
\textbf{4} - Sad\\
\textbf{5} - Surprise\\

Their model is comprised of 2 convolution layers, 2 sub-sampling layers and 1 fully-connected layer. The sub-sampling layers use max pooling with a 2x2 kernel. This halves the size of the image. The fully-connected layer consists of 256 neurons and provides 6 prediction outputs. Stochastic gradient descent is applied for back propagation and the Loss is calculated using soft-max. They also use the rectified linear unit (ReLU) for their activation function.

\subsection{Experiments}
\citeauthor{LOPES}'s experiments include a number of different test cases. These test cases utilize the the CK+ database that has images of a hundred students ages from 18 to 30. An average accuracy is given for each of the test cases. See table \ref{table:testcases} on page \pageref{table:testcases} for results.

\begin{table}
\begin{tabular}{ |p{10cm}||p{3cm}|}
	\hline
	\multicolumn{2}{|c|}{Test Case Averages} \\
	\hline
 	\textbf{Test Case}& \textbf{Average}\\
	\hline
		no pre-processing  & 53.57\% \\
	\hline
		just cropping  & 71.67\%\\
	\hline
		just rotation correction  & 61.55\%\\
	\hline
		cropping and rotation  & 87.86\%\\
	\hline	
		intensity normalisation  & 57\%\\
	\hline
		both normalisations & 86.67\%\\
	\hline
		spatial normalisation and synthetic data & 87.1\%\\
	\hline
		both normalisations and synthetic data & 89.76\%\\
	\hline
\end{tabular}
\caption{Test cases by \citeauthor{LOPES}}
\label{table:testcases}
\end{table}


\newpage
In summary, it can be seen from the table above that by applying the synthetic and normalized data to the training dataset, a higher accuracy can be achieved when testing.

\section{Relevant Work and Critical Analysis}

\citealp{SUBHASHINI}'s work on analysing and detecting employee's emotions for organizations ties in very well with the proposed project as it involves using sentiment analysis on subject to gain an underlining understanding of their emotions that may or may not be expressed verbally. The main research goal was to achieve employee identification in conjunction with facial emotional analysis and they were successful in execution. They used a Bezier Curve on the subjects lips and eye to detect the emotion expressed by analysing the gradient of the curve. However, although this paper proved that the project was a success, there are some inconsistencies. For example: in the related work, they do not explain how the work is related and only give titles. Secondly, the results show no code snippets or pseudocode to explain the implementation of the system. Only screen shots of the user interface are provided. Furthermore, there are more absences of proof. The paper is lacking statistics and graphs to display the accuracy metrics or progress of the systems performance. There are some bold statements used that are not back up by citatins like when it is said that "emotions were considered a forbidden topic in the working place". Despite these weak areas in the paper, a good aspect of this system is the use of real life subjects used in testing. 

The review of Deep Learning for Video Classification and Captioning by \citeauthor{Wu} provides an in depth look into the aspects of different neural networks and what are they strong and weak points. The motivation for their research is driven by their claim that video communications is growing and that their needs to be better applications for video understanding. The relevancy of this paper provides the concept of the "Two-stream" architecture. Although it is not planned to develop two convolutional neural networks (CNN), iit may be sought after to develop a score fusion algorithm, similar to the one mentioned in this paper. This paper is very in depth and draws good comparisons between the different techniques that can be used for video classification. Despite the quality of this paper, some aspects need improvement. There is heavy usage of words like "we" used. Also, some statements are made by \citep{Wu} that are not cited to supported their claim. This is evident when its said "As deep learning for video analysis is an emerging and vibrant field..." (According to whom?).

Subject independent facial expression recognition with robust face detection using a convolutional neural network by \citeauthor{MATSUGU} illustrates the difficulties that may arise when performing facial recognition. They highlight the problems that may occur in terms of being able to handle variability of subject faces, and certain angles. Their approach to this problem is addressed by implementing a rule based algorithm that analysis the results given from the CNN. Furthermore, their model is designed to be segment and be trained on specific facial features instead the face as a whole. Their model proves to be a success as they score an accuracy of 97.6 percent, also they do not require a second CNN working concurrently to achieve similar results as other models have done so previous, which can be cost effective. Some similarities arise between this paper and the proposed project, they both use sentiment analysis and require the ability to handle a wide range of variability. This proves beneficial to the proposed project as it provides inspiration to use a rule based algorithm for determining emotions. Even though this paper is well written, there are some issues. In certain parts there are abbreviations to words given without the full word be given prior which can cause confusion to the reader. For example: "FP neurons". In addition to this, their model is specific for smiling faces and doesn't accommodate for other emotions, which should be at least provided in a further work section. 

\citeauthor{arthmann}'s paper titled Neuromarketing â€“ The Art and Science of Marketing and Neurosciences Enabled by IoT Technologies is a promising insight to the field of neuromarketing. They recognize the association of online shopping and loss of sales for retail stores, and give example of how neuro-linguistic programming and IoT technologies can be used as a combination to understand their customers un-explicitly expressed emotions. Furthermore, it's heavily argued that the integration of AI and machine learning will evolve this concept to understand the thoughts of consumers and further tackle loss in productivity. This proves relevant to the proposed project as it is a very similar use case. They both aim to gain a deeper understanding of human sentiment that may not be express verbally. Although this is possibly the most interesting paper of this review, it is severally lacking citations. Also, there are a lot of assumption brought forward with no clear indication as to how this knowledge is known. Additionally, it is also assumed that people will adopt these "always on" IoT devices and agree for their physical aspects to be used for consumer targeted marketing.
 
\citeauthor{LOPES} go into great detail of the past implementations of facial recognition systems. They review a number of methodologies to reinforce the idea of data augmentation with their own work. It is stated that due to the lack of datasets that are publicly available and overlapping of data, much of the accuracy results that are given by past works may be inaccurate \citep{LOPES}. Furthermore, they enable the reader to grasp a deep understanding to not only how convolutional neural networks operate, but how the common methods of implementation should be done. This is evident when explaining their system's implementation which was broken into two segments: The training phase and testing phase. Additionally, they give a clear and comprehensible description of the methods chosen and why they were taken. For example: using synthetic sample generation, rotation, cropping, down sampling, and normalization. This work proves relevant to the proposed project as it gives an explanation of how a segment of the project should be made. Also, \citeauthor{LOPES} make it very clear that the CK+ dataset is the most suited dataset for facial and sentimental expression recognition. 

\section{Concluding Remarks of Literature Review}
In conclusion, the five papers reviewed topics in relation to convolutional neural networks, facial sentiment analysis, emotion detection and current applications in the real world. Relevant aspects include detecting employees emotions in a work environment, implementing a score fusion algorithm for achieve a summation of two sentiment detecting technologies, the idea of segmenting facial features while training to accommodate for variance and the notion using human emotion understanding for a business solution. Lastly, an indication to what data that should be utilized is obtained throughout the range of papers that have been reviewed.