\prefacesection{Literature }

\section*{Analyzing and Detecting Employee's Emotion for Amelioration of Organizations}

\citeauthor{SUBHASHINI} make the opening statement that emotions usually do not any place in a work environment in current society. Although the expression of feelings is suppressed in places of work, they suggest that emotions can affect five major areas in competitive advantage. The five given aspects of competitive advantage are as follows: Intellectual Capital, Customer Service, Organizational Reactivity, Production, Employee appeal and retentivity \citep{SUBHASHINI}. In order to counter this apprehensiveness to expression of emotions in the work place, \citeauthor{SUBHASHINI} suggest the concept of a facial emotion tracking system that will map the facial expressions of an employees face as they enter the organization. Two related works are also given; Emotion Detections Based on Text and Emotion Recognition Based on Brain-Computer Interface Systems. 

The system architecture given by \citeauthor{SUBHASHINI} briefly describes the program. Most employees entering a building to an organization must swipe a card to clock into the work hours. They suggest that they have designed a new system that removes the need for card swiping but also implements emotion detection. The employee must look into a camera that will prove their presence in the vicinity but will also perform some emotion detection. The system was implemented in the C Sharp programming language and uses skin tone segmentation to detect. The binary image is then converted to an RGB image and an inspection of every individual pixel if performed. If the RGB value is greater than 110, then the pixel colour is refactored to be a white pixel, other wise it becomes a black pixel. This is done to make it easy to detect facial features in the video stream \citep{SUBHASHINI}. Once detected, the image around the face is cropped. They then apply a Bezier Curve to the regions around the lips and eyes of the person being analyzed. The results of the persons identity and emotional status are then stored within a database. They conclude their paper by explaining that this system can be used by management to gain an understanding of their employees sentimental state. 
\\
\section*{Deep Learning for Video Classiﬁcation and Captioning}
\citeauthor{Wu}'s paper provides an in depth analysis on the methods for video classification and video captioning in terms of deep learning. They claim that because of the exponential growth in internet bandwidth and computing power, video communications are becoming more and more prevalent, therefore paving the way for new video understanding applications \citep{Wu}. They make reference to current implementations to prove the growth of interest in the field of computer vision and video analysis, notably the ImageNet challenge. 

Move over, they go on to give brief description of the two "deep learning modules" that have been used for visual analysis: Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). It is explained that LeCun et al. that developed LeNet-5 made a break through when they developed a CNN using the Back-Propogation Algorithm. But its noted that this is limited in performance when the complexity of the tasks is increased. Deep belief networks were developed to train networks in a unsupervised manner in order to counter this problem \citep{Wu}. AlexNet, a CNN proposed by Krizhevsky et al. in 2012, introduced two way to increase the performance of CNN's using ReLU (Rectified Linear Units) and Dropout to descrease overfitting \citep{Wu}. Seondly, RNN's are brought forward. \citeauthor{Wu} explain the difference between CNN's and RNN's, stating that CNN's are all feed forward networks that do not use cycling, which can prove be disadvantageous when working with sequence labeling. Two issues can occur with RNN's: Vanishing Gradient and Exploding Gradients as short term memory is used when cycling through the network. The solution given to this is an RNN variant called Long Short-Term Memory (LSTM). \\
\subsection*{Image-Based Video Classiﬁcation using CNN's and LSTM's}
They state that  Karparthy et al. rsearched the common architectures for learning spatial-temporal clues in large video datasets. It appeared that models using single frames as input achieve similar results as models using stacks of frames. From this, Simonyan and Zisserman proposed the idea of the Two-Stream Approach, because of the cost effectiveness and time consumption that come with training 3D CNN's. This Two-Stream approach involves training the CNN on single and stack frames concurrently. Both outputs are put through a score fusion. The result is the weighted sum of both scores \citep{Wu}. 

Although Two-Stream is a good approach, it is not sufficient as it is not capable of dealing with long video clips. Therefore, LSTM's are utilized as they do not suffer from the problem of vanishing gradients. It has been found that CNN's and LSTM's compliment eachother when working in conjunction with eachother \citep{Wu}. They conclude their paper with a summary of the written topics about, regarding the growth for the need for video understanding applications, the used of CNN's and RNN's, in addition to the variants of these deep learning modules. 

\section*{Subject independent facial expression recognition with robust face detection using a convolutional neural network}
As stated by \citeauthor{MATSUGU}, difficulties may arise with facial recognition. In terms of a face being in a smiling-like state, could have different implications. As well as this, a facial recognition system should be able to work with a wide range of variability of faces. They then give some examples of past implementations such as facial recognition with rigid head movement by Black and Yackoob in 1995, and speak about how this does not meet the requirements of dealing with wide variance. A rule based system is proposed \citep{MATSUGU}. With their model, layer trains on a module-by-module (module being the nose, eyes, mouth etc) basis. Meaning each layer trains on a certain facial feature. Each of the neurons perform an averaging of some local receptive fields then they use a skin tone detector to detect each module on the face \citep{MATSUGU}. For training, Layer one and layer two are trained for 8 modules using back propagation. Layer three and four train on more complex feature detectors such as the mouth and eyes. The output is then sent to the rule based algorithm for handling variability and robustness. \\
The rule based algorithm takes the output of the CNN and measures the distance between the features. From these calculation, the rules are applied to determine if the person is in a laughing or smiling state. The rules are summarised as follows \citep{MATSUGU}:
\begin{itemize}
	\item The distance between eyes and lip get shorter.
	\item The horizontal length of lip gets longer.
	\item The eyes wrinkle.
	\item The gradient of lip from the end point to the end point increases.
	\item Detection of teeth increases.
	\item The edges (wrinkles) of cheek increase.
\end{itemize}
In conclusion, they received a 97.6 percent accuracy for 10 test subjects with 5600 images. They assert that their model is significantly more efficient as they only require one CNN due to their rule based algorithm, in contrast to Fasels implementation that uses two CNN's working in synergy with one another.

\section*{Neuromarketing - The Art and Science of Marketing and Neurosciences Enabled by IoT Technologies}
A paper by \citeauthor{arthmann} describes the growing field of Neuromarketing with an opening statement: Advertisers recognise that there is a relationship between stimulating the emotions of a customer and influencing their actions. Online shopping has drastically affected the store sales, and it is also explained that more than 3500 stores have shut down due to bankruptcy in 2017 \citep{arthmann}. Their answer to this change in buying is Neuro Linguistic Programming. Neuro Linguistic Programming (NLP), not to be confused with natural language processing, is a form of observing the verbal and non verbal communication of humans. Eye accessing cues (eye movements) are said to be linked to certain emotions or thoughts. Neuromarketing incorporates NLP and IoT devices to understand the consumer sentiment more extensively. Neuromarketing aims to remove marketing biases by utilizing the consumers subconscious. One example given by \citeauthor{arthmann} is the notion of facial coding and motion tracking, which is put in place to determine why consumer make certain decisions. 
Furthermore, they make their belief clear of retailers benefiting from this when it is put forward that artificial intelligence and machine learning will evolve, giving better results of consumer preference shifts. Additionally, These neuromarketing systems can replace thermoimaging people counting devices that clock people walking into stores that may not be eligible customers (children). Further examples are provided for these technologies. The describe a scenario when a customer is given an image of a product and a price in front of a webcam, and by performing facial coding and sentiment analysis, we may get a better sense of what the consumer is feeling.
\citeauthor{arthmann} conclude their paper by declaring the future potential of these systems using when integrated with "always on" IoT technologies. 

